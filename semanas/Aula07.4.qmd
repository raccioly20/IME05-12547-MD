---
title: "Regularização de Modelos"
author: "Ricardo Accioly"
date: "`r Sys.Date()`"
format:
 html:
    code-link: true
    fig-width: 9
    fig-height: 7
    fig-dpi: 300
knitr:
  opts_chunk: 
    out.width: 90%
    comment: "#>"
---

## Regularização de modelos

## Carregando Bibliotecas

```{r bibliotecas, warning=FALSE, message=FALSE}
library(tidyverse)
library(glmnet)
```

## Carregando os dados

Vendas de casas em Seattle entre 2015 e 2016

```{r Dados}
vendas_casa <- readRDS("home_sales.rds")
head(vendas_casa)
vendas_casa <- vendas_casa %>% rename(preco=selling_price,
                                      idade=home_age,
                                      quartos=bedrooms,
                                      banheiros= bathrooms,
                                      m2_princ=sqft_living,
                                      m2_tot=sqft_lot,
                                      m2_porao=sqft_basement,
                                      andares=floors
                                      )
summary(vendas_casa)
vendas_casa <- vendas_casa %>% mutate(preco_m=preco/1000) %>% select(-preco)
summary(vendas_casa)
```

## Conjunto de treino e de teste

```{r treino_teste}
library(caret)
set.seed(21)
nrow(vendas_casa)
y <- vendas_casa$preco_m
indice_teste <- createDataPartition(y, times = 1, p = 0.2, list = FALSE)

conj_treino <- vendas_casa[-indice_teste,]
conj_teste <- vendas_casa[indice_teste,]

str(conj_treino)
str(conj_teste)
gt::gt(head(conj_treino, 10))
```

## Métodos de Regularização

O pacote glmnet não usa a linguagem de formula, em particular nós devemos passar $x$ como uma matriz e $y$ como um vetor, pois não se usa a sintaxe $y \sim x$. Com isso será necessário ajustar x e y. A função model.matrix() é particularmente útil para criar x; não só produz uma matriz correspondente as variáveis explicativas, **mas também transforma automaticamente quaisquer variáveis qualitativas em variáveis dummy. Esta última propriedade é importante porque o glmnet() só pode tomar insumos numéricos e quantitativos.**

**O pacote glmnet também por default padroniza as variáveis, o que é importante para a regressão Ridge e também para o LASSO. Ele posteriormente retorna os coeficientes para a escala inicial.**

```{r preparando_dados}
x_treino <- model.matrix(preco_m ~ . , data = conj_treino)[, -1]
y_treino <- conj_treino$preco_m

x_teste <- model.matrix(preco_m ~ . , data = conj_teste)[, -1]
y_teste = conj_teste$preco_m
```

## Regressão Ridge

Primeiro vamos ajustar um modelo de regressão Ridge. Isso é conseguido chamando `glmnet()` com `alpha=0`, se `alpha=1` então `glmnet()` ajusta um lasso.(veja o arquivo de ajuda).

```{r Ridge}
## Estabelecendo um grid de valores para lambda
grid <- 10^seq(-2, 10, length = 100)
ajusreg.ridge <- glmnet(x_treino, y_treino, alpha=0, lambda = grid)

```

Por padrão, a função `glmnet()` executa a regressão ridge automaticamente selecionando a faixa de valores de $\lambda$. No entanto, aqui nós escolhemos implementar usando uma grade de valores que variam de $\lambda = 10^{-2}$ a $\lambda = 10^{10}$, cobrindo toda a gama de cenários do modelo nulo contendo apenas o coeficiente linear até o ajuste dos mínimos quadrados.

Também podemos calcular o modelo para um valor particular de $\lambda$ que não é um dos valores de grade. Observe que, por padrão, a função `glmnet()` padroniza as variáveis para que elas estejam na mesma escala. **Esta padronização é muito importante no caso da regressão Ridge, pois ela é afetada pela mudança de escala das variáveis explicativas.**

Associado a cada valor de $\lambda$ existe um vetor de coeficientes de regressão de ridge, que é armazenado em uma matriz que pode ser acessada por 'coef()'. Neste caso, é uma matriz $13 \times 100$, com 13 linhas (uma para cada preditor, mais uma para o coeficiente linear) e 100 colunas (uma para cada valor de $\lambda$).

```{r r1}
dim(coef(ajusreg.ridge))
plot(ajusreg.ridge, xvar="lambda", label=TRUE) # Representando os coeficientes

```

Quando $\lambda$ é grande o esperado é que os coeficentes sejam pequenos e quando $\lambda$ é pequeno os coeficientes assumem valores maiores.

```{r r2}
ajusreg.ridge$lambda[1] # Mostra primeiro valor de lambda
coef(ajusreg.ridge)[,1] # Mostra os coeficientes associados com o primeiro valor
ajusreg.ridge$lambda[100] # Mostra centésimo valor de lambda
coef(ajusreg.ridge)[,100] # Mostra os coeficientes associados com o centésimo valor
```

```{r r3}
library(plotmo)
plot_glmnet(ajusreg.ridge)
```

## Cross-Validation no Ridge

Nós podemos usar o k-fold cross validation para identificar o melhor valor de $\lambda$

A biblioteca glmnet já tem internamente uma função para uso do crosss validation. O default são 10 envelopes de dados `nfold=10`.

```{r r4}
set.seed(21)
ridge_cv <- cv.glmnet(x_treino,y_treino, alpha=0) ## por padrão k=10
plot(ridge_cv)
m_lamb <- ridge_cv$lambda.min  # Seleciona o lambda que minimiza o MSE (EQM) de treino
m_lamb
log(m_lamb)
coef(ridge_cv, s=m_lamb)
```

## Avaliando com conjunto de teste

Em seguida avaliamos seu MSE no conjunto de teste, usando $\lambda$ = m_lamb. Observe o uso da função 'predict()': desta vez temos previsões para um conjunto de teste, com o argumento `newx`.

```{r avaliando}
ajusreg.ridge2 <- glmnet(x_treino, y_treino, alpha=0, lambda = m_lamb)
y_prev <- predict(ajusreg.ridge2, s = m_lamb, newx = x_teste)
# Metricas de desempenho
sqrt(mean((y_prev - y_teste)^2))
```

## Comparando real vs previsão no conjunto de teste

```{r}
v_teste <- data.frame(y_teste, y_prev)
ggplot(v_teste, aes(x=y_prev, y=y_teste)) +geom_point() +
  geom_abline(color = "darkblue") +
  ggtitle("Preço da Casa vs. Previsões do modelo Ridge")
```

## LASSO

Primeiro ajustamos com todos os dados como no caso do Ridge

```{r LASSO}
ajusreg.lasso <- glmnet(x_treino,y_treino, alpha = 1)
plot(ajusreg.lasso, xvar="lambda", label=TRUE) # Representando os coeficientes
plot_glmnet(ajusreg.lasso)
```

## Validação Cruzada no LASSO

```{r Lasso2}
lasso_cv <- cv.glmnet(x_treino,y_treino, alpha = 1)
plot(lasso_cv)
m_lamb1 <- lasso_cv$lambda.min  # Seleciona o lambda que minimiza o MSE de treino
m_lamb1
log(m_lamb1)
coef(lasso_cv, s=m_lamb1)
```

## Avaliando com conjunto de teste

```{r lasso2}
ajusreg.lasso2 <- glmnet(x_treino, y_treino, alpha=1, lambda = m_lamb1)
y_prev <- predict(ajusreg.lasso2, s = m_lamb1, newx = x_teste)
# Metricas de desempenho
sqrt(mean((y_prev - y_teste)^2))
```

## Comparando real vs previsão no conjunto de teste

```{r}
v_teste <- data.frame(y_teste, y_prev)
ggplot(v_teste, aes(x=y_prev, y=y_teste)) +geom_point() +
  geom_abline(color = "darkblue") +
  ggtitle("Preço da Casa vs. Previsões do modelo Ridge")
```
