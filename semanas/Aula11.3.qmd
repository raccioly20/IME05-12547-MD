---
title: 'Arvores de Regressão - XGBoost'
author: "Ricardo Accioly"
date: "`r Sys.Date()`"
execute: 
  echo: true
  warning: false
  message: false
  freeze: auto
format:
 html:
    code-link: true
    fig-height: 10
    fig-width: 10
    fig-align: center
    fig-dpi: 300
knitr: 
  opts_chunk: 
    out.width: 90%
    fig.showtext: true
    collapese: true
---

## Bibliotecas

```{r bibliotecas , message=FALSE}
library(MASS)
library(tidyverse)
```

## Avaliando, selecionando dados

```{r dados}
data("Boston")
names(Boston)
dados <- Boston 
```

## Treino e Teste com todas as variáveis

```{r conjuntos-treino-teste}
## Vamos criar os conjuntos de treino teste e desenvolver a arvore 
## com todas as variáveis.
library(caret)
set.seed(21)
indice <- createDataPartition(dados$medv, times=1, p=0.75, list=FALSE)
conj_treino <- dados[indice,]
conj_teste <- dados[-indice,] 
str(conj_treino)
str(conj_teste)
```

## Preparando os dados

```{r}
x_treino <- model.matrix(medv ~ . , data = conj_treino)[, -1]
y_treino <- conj_treino$medv

x_teste <- model.matrix(medv ~ . , data = conj_teste)[, -1]
y_teste = conj_teste$medv
```

## 1a tentativa Xgboost

```{r xgboots}
library(xgboost)
set.seed(21)
cv <- xgb.cv(data = as.matrix(x_treino), label = as.matrix(y_treino),
             objective = "reg:squarederror", nrounds = 100, nfold = 5, eta = 0.3, max_depth = 6,
             verbose = FALSE)
# cv
elog <- as.data.frame(cv$evaluation_log)
elog %>% 
   summarize(ntrees.train = which.min(train_rmse_mean),   # find the index of min(train_rmse_mean)
             ntrees.test  = which.min(test_rmse_mean))   # find the index of min(test_rmse_mean)
(nrounds <- which.min(elog$test_rmse_mean))
```

## Modelo Final

```{r final}
 modelo_xgb <- xgboost(data = as.matrix(x_treino), label = as.matrix(y_treino),
             objective = "reg:squarederror", nrounds = nrounds, eta = 0.3, max_depth = 6,
             verbose = FALSE)
```

## Previsões

```{r}
conj_teste$prev <- predict(modelo_xgb, as.matrix(x_teste))


ggplot(conj_teste, aes(x = prev, y = medv)) + 
  geom_point() + 
  geom_abline()
```

## Calculando o RMSE

```{r}
conj_teste %>%
  mutate(residuos = medv - prev) %>%
  summarize(rmse = sqrt(mean(residuos^2)))
caret::postResample(conj_teste$prev, conj_teste$medv)
```

## Comparação com outro modelo (Regressão Linear)

```{r}
ctrl <- trainControl(method = "cv", number = 5)

model_lm <- train(
  medv ~ ., data = conj_treino,
  method = "lm",
  trControl = ctrl
)

pred_lm <- predict(model_lm, newdata = conj_teste)
postResample(pred_lm, conj_teste$medv)
```
