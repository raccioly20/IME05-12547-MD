---
title: "Arvores de Classificação - Única e GBM"
author: "Ricardo Accioly"
date: "`r Sys.Date()`"
format:
 html:
    code-link: true
---

## Carregando Bibliotecas

```{r bibliotecas, message=FALSE}
library(tidyverse)
library(ISLR)
data(Default)
summary(Default)
str(Default)
head(Default)
```

## Manipulando os dados

```{r inadimplente}
credito <- tibble(Default)
summary(credito)
# renomeando colunas
credito <- credito %>% 
                rename( inadimplente = default, estudante = student, balanco = balance,
                receita = income)
credito <- credito %>% mutate( inadimplente =  case_when(
                           inadimplente == "No"  ~ "Nao",
                           inadimplente == "Yes" ~ "Sim"
                          )) %>% mutate(inadimplente = factor(inadimplente))
credito <- credito %>% mutate( estudante =  case_when(
                           estudante == "No"  ~ 0,
                           estudante == "Yes" ~ 1
                          )) 

str(credito)
summary(credito)

```

## Treino e Teste

```{r conjuntos-treino-teste, message=FALSE}
library(caret)
set.seed(21)
y <- credito$inadimplente
indice_teste <- createDataPartition(y, times = 1, p = 0.2, list = FALSE)

conj_treino <- credito %>% slice(-indice_teste)
conj_teste <- credito %>% slice(indice_teste)

summary(conj_treino)
summary(conj_teste)

```

## Arvore de Classificação

Na biblioteca rpart as arvores de classificação são obtidas usando o método class. Existem alguns controles que podem ser feitos nos parametros da arvore.

Neste exemplo só definimos o menor conjunto de dados numa partição (minsplit) e o parametro de complexidade cp. Posteriormente vamos ampliar este controle. Um valor de cp muito pequeno ocasiona overfitting e um valor muito grande resulta numa arvore muito pequena (underfitting). Nos dois casos se diminui o desempenho do modelo.

```{r}
##Usando rpart para desenvolver a arvore  
library(rpart)
arvcl <- rpart(inadimplente ~ ., 
                data=conj_treino,
                method="class", #para arvore de classificação
                control=rpart.control(minsplit=30,cp=0.02))
plot(arvcl)
text(arvcl,pretty=0)
```

## Regras

```{r}
# Regras de Decisão
arvcl
```

## Desenhando a Árvore de uma forma mais clara

```{r}
library(rattle)
library(rpart.plot)
library(RColorBrewer)
fancyRpartPlot(arvcl, caption = NULL)

```

## Previsões

```{r}

# Fazendo Previsões
y_chapeu <- predict(arvcl, newdata = conj_teste, type="class")

confusionMatrix(y_chapeu, conj_teste$inadimplente, positive="Sim") 
```

## Arvore de Classificação no caret

```{r}
##Usando rpart para desenvolver a arvore  
library(rpart)
set.seed(21)
## Otimizamos o valor de cp usando um 10-fold cv
# O parametro tuneLength diz para o algoritmo escolher diferentes valores para cp
# O parametro tuneGrid permite decidir que valores cp deve assumir enquanto que o
# tuneLength somente limita o número default de parametros que se usa.
tgrid <- expand.grid(cp = seq(0.01,0.10,0.001))
ctrl <- trainControl(method = "cv", classProbs=TRUE)
arvclass <- train(inadimplente ~ . , data = conj_treino, method = "rpart",
                 trControl = ctrl,
                 tuneGrid = tgrid
                 )
# Mostra a acurácia vs cp (parametro de complexidade)
plot(arvclass)
## Indica o melhor valor de cp
arvclass$bestTune
```

## Uma forma melhor de ver a Árvore

```{r}
## melhorando apresentação da árvore
library(rattle)
library(rpart.plot)
library(RColorBrewer)
fancyRpartPlot(arvclass$finalModel, caption = NULL)
```

## Previsões

```{r}

# Fazendo Previsões
y_chapeu <- arvclass %>% predict(conj_teste) %>% 
                   factor(levels = levels(conj_teste$inadimplente))
head(y_chapeu)
confusionMatrix(y_chapeu, conj_teste$inadimplente, positive="Sim") 
```

## Verificando a consistencia dos resultados

```{r}
set.seed(121)
y <- credito$inadimplente
indice_teste <- createDataPartition(y, times = 1, p = 0.2, list = FALSE)

conj_treino <- credito %>% slice(-indice_teste)
conj_teste <- credito %>% slice(indice_teste)

str(conj_treino)
prop.table(table(conj_treino$inadimplente))
str(conj_teste)
prop.table(table(conj_teste$inadimplente))
```

## Obtendo a arvore

```{r}
##Usando rpart para desenvolver a arvore  
library(rpart)
arvcl <- rpart(inadimplente ~ ., 
                data=conj_treino,
                method="class", #para arvore de classificação
                control=rpart.control(minsplit=30,cp=0.02))

```

## Desenhando a Árvore de uma forma mais clara

```{r}
library(rattle)
library(rpart.plot)
library(RColorBrewer)
fancyRpartPlot(arvcl, caption = NULL)

```

## GBM

### Criando um grid para avaliar os parametros

```{r}
hiper_grid <- expand.grid(
  shrinkage = c(.001, .01, .1),
  interaction.depth = c(1, 3, 5),
  n.minobsinnode = c(5, 10, 15),
  bag.fraction = c(.65, 1),
  optimal_trees = 0, # um lugar para guardar resultados
  min_erro = 0   # um lugar para guardar resultados
  )

# número total de combinações
nrow(hiper_grid)
```

## Avaliando o grid de parametros

```{r}
library(gbm)
conj_treino$inadimplente <- as.numeric(conj_treino$inadimplente)
conj_treino <- transform(conj_treino, inadimplente=inadimplente - 1)

#Busca no grid
for(i in 1:nrow(hiper_grid)) {

  #
  set.seed(21)

  # treina o modelo
  gbm.tune <- gbm(
    formula = inadimplente ~ .,
    distribution = "bernoulli",
    data = conj_treino,
    n.trees = 2000,
    interaction.depth = hiper_grid$interaction.depth[i],
    shrinkage = hiper_grid$shrinkage[i],
    n.minobsinnode = hiper_grid$n.minobsinnode[i],
    bag.fraction = hiper_grid$bag.fraction[i],
    train.fraction = .75,
    n.cores = NULL,
    verbose = FALSE
  )

  # adiciona os erros de treino e arvores ao grid
  hiper_grid$optimal_trees[i] <- which.min(gbm.tune$valid.error)
  hiper_grid$min_erro[i] <- min(gbm.tune$valid.error)
}

hiper_grid %>% dplyr::arrange(min_erro) %>% head(10)
```

## Modelo final

```{r}
# 
set.seed(21)

# treina o modelo GBM
gbm.fit.final <- gbm(
  formula = inadimplente ~ .,
  distribution = "bernoulli",
  data = conj_treino,
  n.trees = 193,
  interaction.depth = 1,
  shrinkage = 0.10,
  n.minobsinnode = 5,
  bag.fraction = 1.00, 
  train.fraction = 1,
  n.cores = NULL, 
  verbose = FALSE
  )  

```

## Importância das Variáveis

```{r}
par(mai = c(1, 2, 1, 2))
summary(
  gbm.fit.final, 
  cBars = 10,
  method = relative.influence, # também pode ser usado permutation.test.gbm
  las = 2
  )

```

## Previsão

```{r}
conj_teste$inadimplente <- as.numeric(conj_teste$inadimplente)
conj_teste <- transform(conj_teste, inadimplente=inadimplente - 1)

# Fazendo Previsões
previsao1 <- predict(gbm.fit.final, 
                     newdata = conj_teste,
                     n.trees=gbm.fit.final$n.trees,
                     type = "response")
head(previsao1)

gbm.ychapeu <- as.factor(ifelse(previsao1 < 0.5,0,1))
 
confusionMatrix(gbm.ychapeu,as.factor(conj_teste$inadimplente), positive="1")

```

## Curva ROC

```{r}
library(pROC)
p_chapeu_gbm <- previsao1
roc_gbm <- roc(conj_teste$inadimplente ~ p_chapeu_gbm, plot = TRUE, print.auc=FALSE, col="black", legacy.axes=TRUE)
as.numeric(roc_gbm$auc)
```
