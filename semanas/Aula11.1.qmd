---
title: "Arvores de Regressão"
author: "Ricardo Accioly"
date: "`r Sys.Date()`"
execute: 
  echo: true
  warning: false
  message: false
  freeze: auto
format:
 html:
    code-link: true
    fig-height: 10
    fig-width: 10
    fig-align: center
    fig-dpi: 300
knitr: 
  opts_chunk: 
    out.width: 90%
    comment: "#>"
---

## Bibliotecas

```{r bibliotecas , message=FALSE}
library(MASS)
library(tidyverse)
library(rpart)
library(rpart.plot)
library(psych)
library(caret)
```

## Carregando os dados

```{r dados}
data("Boston")
names(Boston)
describe(Boston)
```

## Conhecendo as variáveis

-   crim: taxa de criminalidade per capita por cidade.

-   zn: proporção de terrenos residenciais zoneados para lotes acima de 25.000 pés quadrados.

-   indus: proporção de acres de negócios não varejistas por cidade.

-   chas: variável fictícia do rio Charles (= 1 se o trato limita o rio; 0 caso contrário). nox concentração de óxidos de nitrogênio (partes por 10 milhões).

-   rm: número médio de cômodos por habitação.

-   age: proporção de unidades ocupadas pelo proprietário construídas antes de 1940.

-   dis: média ponderada das distâncias até cinco centros de emprego de Boston.

-   rad: índice de acessibilidade a rodovias radiais. tax taxa de imposto sobre a propriedade do valor total por \$ 10.000.

-   ptratio: proporção aluno-professor por cidade.

-   black: 1000(Bk−0,63)21000(Bk−0,63)2 onde BkBk é a proporção de negros por cidade.

-   lstat: status inferior da população (por cento).

-   medv: valor médio das casas ocupadas pelo proprietário em \$ 1000s.

## Selecionando os dados

```{r}
dados <- Boston 
extrato <- dados %>% select(medv, nox, rm)  
summary(extrato)
boxplot(extrato$medv)
```

## Visualizando os dados

```{r ggplot, fig.height=6.5, fig.width=7.}
## Distribuição de dados na maior parte simétrica com valores na cauda direta 
## maior do que o esperado para uam distribuição simétrica
ggplot(extrato, aes(x=medv)) +
  geom_histogram(bins = round(1+3.322*log10(nrow(extrato)),0))

## Grafico de dispersão nox vs rm 
ggplot(extrato, aes(x=rm, y=nox)) + 
  geom_point()
```

## Arvore de Regressão

Na biblioteca rpart as arvores de regressão são obtidas usando o método anova. Existem alguns controles que podem ser feitos nos parametros da arvore.

Neste exemplo só definimos o menor conjunto de dados numa partição (minsplit) e o parametro de complexidade cp. Qualquer partição/divisão que não melhore o ajuste por um fator de cp não é tentada. Por exemplo, com a partição pela anova, isso significa que o R-quadrado geral deve aumentar pelo valor de cp a cada etapa. O principal papel deste parâmetro é economizar tempo de computação podando divisões que obviamente não valem a pena. Essencialmente, o usuário informa ao programa que qualquer divisão que não melhore o ajuste pelo cp, provavelmente será podada por validação cruzada, e que, portanto, não é necessário persegui-lo.

```{r arvore}
##Usando rpart para desenvolver a arvore  
arvreg <- rpart(medv ~ ., 
                data=extrato,
                method="anova", #para arvore de regressão
                control=rpart.control(minsplit=30,cp=0.06))
arvreg
rpart.plot(arvreg)
```

## Segmentos

A partir da árvore obtida no item anterior podemos fazer uma representação gráfica das partições obtidas.

```{r segmentos}
ggplot(extrato, aes(x=rm, y=nox)) + 
  geom_point() +
  geom_segment(aes(x = 0, y = 0.6695, xend = 6.941, yend = 0.6695), 
               linetype="dashed", color="red", size=1) +
  geom_vline(xintercept = 6.941, linetype="dashed", color="red", size=1) +
  geom_vline(xintercept = 7.437, linetype="dashed", color="red", size=1) 
  # scale_y_continuous(limits = c(0.3, 1)) +
```

## Treino e Teste com todas as variáveis

Agora vamos trabalhar com o conjunto completo criando um conjunto de treino e teste.

```{r conjuntos-treino-teste}
## Vamos criar os conjuntos de treino teste e desenvolver a arvore 
## com todas as variáveis.
set.seed(2025)
indice <- createDataPartition(dados$medv, times=1, p=0.75, list=FALSE)
conj_treino <- dados[indice,]
conj_teste <- dados[-indice,]
head(conj_treino)
head(conj_teste)
```

## Arvore de Regressão com caret

Aqui vamos usar a biblioteca caret que tem umas facilidades para otimização do cp e apresentação dos resultados

```{r arvore3}
set.seed(2025)
## Otimizamos o valor de cp usando um 10-fold cv
# O parametro tuneLength diz para o algoritmo escolher diferentes valores para cp
# O parametro tuneGrid permite decidir que valores cp deve assumir enquanto que o
# tuneLength somente limita o número default de parametros que se usa.
arvreg2 <- train(medv ~ . , data = conj_treino, method = "rpart",
                 trControl = trainControl("cv", number = 10),
                 tuneGrid = data.frame(cp = seq(0.01,0.10, length.out=100)) 
                 )
# Mostra a acurácia vs cp (parametro de complexidade)
plot(arvreg2)
## Indica o melhor valor de cp
arvreg2$bestTune
```

## Desenhando a Árvore

```{r arvore4}
## Apresenta o modelo final de arvore ajustado
## usando o rpart.plot
rpart.plot(arvreg2$finalModel)
```

## Gráfico de importancia das variáveis

A importancia das variáveis é calculada com base nos resultados das melhores partições

```{r}
# Gráfico de Importância de variável
var_imp <- varImp(arvreg2) 
var_imp$importance %>%
  rownames_to_column(var = "Variável") %>%
  ggplot(aes(x = reorder(Variável, Overall), y = Overall)) +
  geom_col(fill = "steelblue") +
  coord_flip() +
  labs(title = "Importância das Variáveis na Árvore de Regressão",
       x = "Variável",
       y = "Importância (Overall)") +
  theme_minimal()
```

## Previsões

```{r previsoes}
# Regras de Decisão
arvreg2$finalModel
# Fazendo Previsões
previsao2 <- arvreg2 %>% predict(conj_teste)
head(previsao2)
# Calcula os erros de previsão
RMSE(previsao2, conj_teste$medv)
```

## Vamos comparar com Regressão Multipla

```{r kfold}
library(leaps)
## Cria uma função de predição para o leaps
predict.regsubsets <- function(object,newdata,id,...){
  form <- as.formula(object$call[[2]])
  mat <- model.matrix(form,newdata)
  coefi <- coef(object,id=id)
  mat[,names(coefi)]%*%coefi
}
set.seed(2025)
envelopes <- sample(rep(1:5,length=nrow(conj_treino)))
table(envelopes)
erro_cv <- matrix(NA,5,13)
for(k in 1:5){
  melh_ajus <- regsubsets(medv ~ ., data=conj_treino[envelopes!=k,], 
                          nvmax=13,method="forward")
  for(i in 1:13){
    prev <- predict(melh_ajus, conj_treino[envelopes==k,],id=i)
    erro_cv[k,i] <- mean( (conj_treino$medv[envelopes==k]-prev)^2)
  }
}
rmse_cv <- sqrt(apply(erro_cv,2,mean))  # Erro medio quadratico de cada modelo
plot(rmse_cv,pch=19,type="b")          
```

## Obtem a fórmula do modelo

```{r formula}
coef(melh_ajus, 11)
```

## Teste com o conjunto de teste

```{r teste2}
previsao3 <- predict(melh_ajus, conj_teste, 11) 
RMSE(previsao3, conj_teste$medv)
```

## E se usarmos outra semente?

```{r}
## Vamos criar os conjuntos de treino teste e desenvolver a arvore 
## com todas as variáveis.
set.seed(23)
indice <- createDataPartition(dados$medv, times=1, p=0.75, list=FALSE)
conj_treino <- dados[indice,]

arvreg1s <- rpart(medv ~ ., 
                data=conj_treino,
                method="anova", #para arvore de regressão
                control=rpart.control(minsplit=30,cp=0.01))
arvreg1s

## Colocar as duas arvores lado a lado
par(mfrow=c(1,2))
rpart.plot(arvreg1s)
rpart.plot(arvreg2$finalModel)

```

Esta é a principal fragilidade da árvore (única), qualquer mudança na amostra pode trazer uma configuração diferente. \## Fim
