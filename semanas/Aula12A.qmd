---
title: "Arvores de Classificação - Única e GBM"
author: "Ricardo Accioly"
date: "`r Sys.Date()`"
output:
  word_document:
    toc: yes
  html_document:
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.height=6.5, fig.width=7.)
```


## Bibliotecas

```{r bibliotecas , message=FALSE}
library(tidyverse)
library(ISLR)
```


## Dados

Vamos começar a aplicar a metodologia de árvores usando **árvores de classificação** para analisar os dados existentes em `Carseats`. Este conjunto de dados (simulado) é sobre venda de assentos de criança para carros. Ele tem 400 observações das seguintes variáveis (11), cujos nomes  serão convertidos para o português:

Sales: vendas em unidades (em mil) em cada local

CompPrice: preço cobrado pelo competidor em cada local

Income: nível de renda da comunidade local (em mil US$)

Advertising: orçamento local de propaganda (em mil US$)

Population: população na região (em mil)

Price: preço cobrado pela empresa em cada local

ShelveLoc: um fator com níveis Ruim, Bom e Medio indicando a qualidade da localização das prateleiras para os assentos em cada lugar

Age: idade media da população local

Education: nível de educação em cada local

Urban: um fator Sim e Não indicando se a loja esta em uma área urbana ou rural 

US: um fator indicando se a loja é nos EUA ou não


Neste dados, `Sales` é a variável resposta, só que ela é uma variável contínua, por este motivo vamos usá-la para criar uma variável binária. 
Vamos usar a função `ifelse()` para criar a variável binária, que chamaremos de **alta**, ela assume os valores `Sim` se `Sales` for maior que 8 e assume o valor `Não` caso contrário:


```{r dados}
data(Carseats)
summary(Carseats)
str(Carseats)
```


## Manipulando os dados

```{r inadimplente}
cad_crianca <- Carseats %>% rename(vendas = Sales, 
                                   preco_comp = CompPrice,
                                   renda = Income,
                                   propaganda = Advertising,
                                   populacao = Population,
                                   preco = Price,
                                   local_prat = ShelveLoc,
                                   idade = Age,
                                   educacao = Education,
                                   urbano = Urban,
                                   eua = US)

cad_crianca <- cad_crianca %>% mutate(alta = ifelse(vendas > 8, "Sim",
                                                   "Não")) %>%
                              mutate(alta = factor(alta))

cad_crianca<- cad_crianca %>% mutate(local_prat =  case_when(
                                      local_prat == "Bad"  ~ "Ruim",
                                      local_prat == "Good" ~ "Bom",
                                      local_prat == "Medium" ~ "Medio"))%>%                               mutate(local_prat = factor(local_prat))

cad_crianca<- cad_crianca %>% mutate(urbano =  case_when(
                                      urbano == "Yes"  ~ "Sim",
                                      urbano == "No" ~ "Não")) %>%                                       mutate(urbano = factor(urbano))

cad_crianca<- cad_crianca %>% mutate(eua =  case_when(
                                      eua == "Yes"  ~ "Sim",
                                      eua == "No" ~ "Não")) %>%                                          mutate(eua = factor(eua))

cad_crianca<- cad_crianca %>% select(-vendas)

str(cad_crianca)
summary(cad_crianca)

```


## Treino e Teste

```{r conjuntos-treino-teste}
library(caret)
set.seed(21)
y <- cad_crianca$alta
indice_teste <- createDataPartition(y, times = 1, p = 0.2, list = FALSE)

conj_treino <- cad_crianca %>% slice(-indice_teste)
conj_teste <- cad_crianca %>% slice(indice_teste)

str(conj_treino)
prop.table(table(conj_treino$alta))
str(conj_teste)
prop.table(table(conj_teste$alta))
```


## Arvore de Classificação

Na biblioteca rpart as arvores de classificação são obtidas usando o método class. Existem alguns controles que podem ser feitos nos parametros da arvore. 

Neste exemplo só definimos o menor conjunto de dados numa partição (minsplit) e o parametro de complexidade cp. Posteriormente vamos ampliar este controle. Um valor de cp muito pequeno ocasiona overfitting e um valor muito grande resulta numa arvore muito pequena (underfitting). Nos dois casos se diminui o desempenho do modelo.

```{r arvore}
##Usando rpart para desenvolver a arvore  
library(rpart)
arvcl <- rpart(alta ~ ., 
                data=conj_treino,
                method="class", #para arvore de classificação
                control=rpart.control(minsplit=30,cp=0.02))

```

\newpage

## Desenhando a Árvore de uma forma mais clara

```{r arvore1}
library(rattle)
library(rpart.plot)
library(RColorBrewer)
fancyRpartPlot(arvcl, caption = NULL)

```


```{r outra semente}
set.seed(121)
y <- cad_crianca$alta
indice_teste <- createDataPartition(y, times = 1, p = 0.2, list = FALSE)

conj_treino <- cad_crianca %>% slice(-indice_teste)
conj_teste <- cad_crianca %>% slice(indice_teste)

str(conj_treino)
prop.table(table(conj_treino$alta))
str(conj_teste)
prop.table(table(conj_teste$alta))
```



```{r arvore2}
##Usando rpart para desenvolver a arvore  
library(rpart)
arvcl <- rpart(alta ~ ., 
                data=conj_treino,
                method="class", #para arvore de classificação
                control=rpart.control(minsplit=30,cp=0.02))

```

\newpage

## Desenhando a Árvore de uma forma mais clara

```{r arvore3}
library(rattle)
library(rpart.plot)
library(RColorBrewer)
fancyRpartPlot(arvcl, caption = NULL)

```

