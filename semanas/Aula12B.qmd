---
title: "Arvores de Regressão"
author: "Ricardo Accioly"
date: "`r Sys.Date()`"
output:
  word_document:
    toc: yes
  html_document:
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.height=6.5, fig.width=7.)
```


## Bibliotecas

```{r bibliotecas , message=FALSE}
library(MASS)
library(tidyverse)
library(psych)
```


## Avaliando, selecionando dados


```{r dados}
data("Boston")
names(Boston)
describe(Boston)
dados <- Boston 
extrato <- dados %>% select(medv, nox, rm)  
summary(extrato)
boxplot(extrato$medv)
```


## Visualizando os dados


```{r ggplot, fig.height=6.5, fig.width=7.}
## Distribuição de dados na maior parte simétrica com valores na cauda direta 
## maior do que o esperado para uam distribuição simétrica
ggplot(extrato, aes(x=medv)) +
  geom_histogram(bins = round(1+3.322*log10(nrow(extrato)),0))

## Grafico de dispersão nox vs rm 
ggplot(extrato, aes(x=rm, y=nox)) + 
  geom_point()
```


## Arvore de Regressão

Na biblioteca rpart as arvores de regressão são obtidas usando o método anova. Existem alguns controles que podem ser feitos nos parametros da arvore. 

Neste exemplo só definimos o menor conjunto de dados numa partição (minsplit) e o parametro de complexidade cp. Posteriormente vamos ampliar este controle.

```{r arvore}
##Usando rpart para desenvolver a arvore  
library(rpart)
arvreg <- rpart(medv ~ ., 
                data=extrato,
                method="anova", #para arvore de regressão
                control=rpart.control(minsplit=30,cp=0.06))
plot(arvreg)
text(arvreg,pretty=0)
```

## Segmentos 

A partir da árvore obtida no item anterior podemos fazer uma representação gráfica das partições obtidas.   

```{r segmentos}
ggplot(extrato, aes(x=rm, y=nox)) + 
  geom_point() +
  geom_segment(aes(x = 0, y = 0.6695, xend = 6.941, yend = 0.6695), 
               linetype="dashed", color="red", size=1) +
  geom_vline(xintercept = 6.941, linetype="dashed", color="red", size=1) +
  geom_vline(xintercept = 7.437, linetype="dashed", color="red", size=1) 
  # scale_y_continuous(limits = c(0.3, 1)) +
```

## Treino e Teste com todas as variáveis

Agora vamos trabalhar com o conjunto completo criando um conjunto de treino e teste.  

```{r conjuntos-treino-teste}
## Vamos criar os conjuntos de treino teste e desenvolver a arvore 
## com todas as variáveis.
library(caret)
set.seed(21)
indice <- createDataPartition(dados$medv, times=1, p=0.75, list=FALSE)
conj_treino <- dados[indice,]
conj_teste <- dados[-indice,]
head(conj_treino)
head(conj_teste)
```



## Arvore de Regressão Treino


```{r arvore2}

## A função rpart tem diversos parametros aqui foi configurado um deles
# cp o parametro de complexidade
# Um valor de cp muito pequeno ocasiona overfitting e um valor muito grande 
# resulta numa arvore muito pequena (underfitting).
# Nos dois casos se diminui o desempenho do modelo.
arvreg1 <- rpart(medv ~ ., 
                data=conj_treino,
                method="anova", #para arvore de regerssão
                control=rpart.control(minsplit=30,cp=0.01))

arvreg1
```


## Erros a partir do conjunto de treino

- O erro relativo (Rel error) é obtido através de 1 - R2
- O xerror é obtido através da validação cruzdada (10 fold)
- O xtsd é o desvio padrão dos valores obtidos na validação cruzada.

```{r erros}
## Mostra 2 gráficos:
# 1) Variação do R2 aparente e relativo vs número de partições
# 2) Erro Relativo vs número de partições
rsq.rpart(arvreg1)
## Mostra a variação do Erro relativo vs cp(parametro de complexidade)
plotcp(arvreg1)

```


## Gráfico de importancia das variáveis

A importancia das variáveis é calculada com base nos resultados das melhores partições


```{r}
# Gráfico de Importância de variável
var_imp <- arvreg1$variable.importance
nomes_var <- names(var_imp)
var_impdf <- data.frame(Importancia=unname(var_imp), Variavel=nomes_var) %>%
                        arrange(Importancia)
var_impdf$Variavel <- factor(var_impdf$Variavel, levels=var_impdf$Variavel)
ggplot(var_impdf, aes(x=Variavel, y=Importancia)) + 
         geom_col() + 
        coord_flip()
```


## Mostrando a árvore e gerando previsões

```{r}
# Mostrando a arvore
par(xpd = NA)
plot(arvreg1)
text(arvreg1,pretty=0)

# Fazendo Previsões
previsao1 <- arvreg1 %>% predict(conj_teste)
head(previsao1)

# Calcula os erros de previsão
RMSE(previsao1, conj_teste$medv)
```



## Arvore de Regressão com caret

Aqui vamos usar a biblioteca caret que tem umas facilidades para otimização do cp e apresentação dos resultados

```{r arvore3}
set.seed(21)
## Otimizamos o valor de cp usando um 10-fold cv
# O parametro tuneLength diz para o algoritmo escolher diferentes valores para cp
# O parametro tuneGrid permite decidir que valores cp deve assumir enquanto que o
# tuneLength somente limita o número default de parametros que se usa.
arvreg2 <- train(medv ~ . , data = conj_treino, method = "rpart",
                 trControl = trainControl("cv", number = 10),
                 tuneGrid = data.frame(cp = seq(0.01,0.10, length.out=100)) 
                 )
# Mostra a acurácia vs cp (parametro de complexidade)
plot(arvreg2)
## Indica o melhor valor de cp
arvreg2$bestTune
```


## Desenhando a Árvore

```{r arvore4}
## Apresenta o modelo final de arvore ajustado
par(xpd = NA)
plot(arvreg2$finalModel)
text(arvreg2$finalModel,  digits = 3)


## usando o rpart.plot
library(rpart.plot)
rpart.plot(arvreg2$finalModel)
```


## Previsões

```{r previsoes}
# Regras de Decisão
arvreg2$finalModel
# Fazendo Previsões
previsao2 <- arvreg2 %>% predict(conj_teste)
head(previsao2)
# Calcula os erros de previsão
RMSE(previsao2, conj_teste$medv)
```


## Vamos comparar com Regressão Multipla


```{r kfold}
library(leaps)
## Cria uma função de predição para o leaps
predict.regsubsets <- function(object,newdata,id,...){
  form <- as.formula(object$call[[2]])
  mat <- model.matrix(form,newdata)
  coefi <- coef(object,id=id)
  mat[,names(coefi)]%*%coefi
}
set.seed(21)
envelopes <- sample(rep(1:5,length=nrow(conj_treino)))
table(envelopes)
erro_cv <- matrix(NA,5,13)
for(k in 1:5){
  melh_ajus <- regsubsets(medv ~ ., data=conj_treino[envelopes!=k,], 
                          nvmax=13,method="forward")
  for(i in 1:13){
    prev <- predict(melh_ajus, conj_treino[envelopes==k,],id=i)
    erro_cv[k,i] <- mean( (conj_treino$medv[envelopes==k]-prev)^2)
  }
}
rmse_cv <- sqrt(apply(erro_cv,2,mean))  # Erro medio quadratico de cada modelo
plot(rmse_cv,pch=19,type="b")          
```

## Obtem a fórmula do modelo

```{r formula}
coef(melh_ajus, 11)
```

## Teste com o conjunto de teste


```{r teste2}
previsao3 <- predict(melh_ajus, conj_teste, 11) 
RMSE(previsao3, conj_teste$medv)
```

