---
title: 'Arvores de Regressão - Gradient Boosting '
author: "Ricardo Accioly"
date: "`r Sys.Date()`"
execute: 
  echo: true
  warning: false
  message: false
  freeze: auto
format:
 html:
    code-link: true
    fig-height: 10
    fig-width: 10
    fig-align: center
    fig-dpi: 300
knitr: 
  opts_chunk: 
    out.width: 90%
    fig.showtext: true
    collapese: true
---

## Bibliotecas

```{r bibliotecas , message=FALSE}
library(MASS)
library(tidyverse)
```

## Avaliando, selecionando dados

```{r dados}
data("Boston")
names(Boston)
dados <- Boston 
```

## Treino e Teste com todas as variáveis

```{r conjuntos-treino-teste}
## Vamos criar os conjuntos de treino teste e desenvolver a arvore 
## com todas as variáveis.
library(caret)
set.seed(21)
indice <- createDataPartition(dados$medv, times=1, p=0.75, list=FALSE)
conj_treino <- dados[indice,]
conj_teste <- dados[-indice,]
head(conj_treino)
head(conj_teste)
```

## Parametros do GBM

```{r gbm1}
library(gbm)
set.seed(21)
# treinar o modelo GBM
# gbm.fit <- gbm(formula = medv ~ .,
#                 distribution = "gaussian", #  minimizar erro quadrático
#                 data = conj_treino,
#                 n.trees = 10000,  # número de árvores
#                 interaction.depth = 3,  # profundidade da arvore
#                 shrinkage = 0.1,   # aprendizado rápido
#                 cv.folds = 5, # 5 envelopes de validaçõa cruzada
#                 n.cores = NULL, # 
#                 verbose = FALSE)

```

## Criando um grid para avaliar os parametros e os respectivos RMSEs

```{r grid}
hiper_grid <- expand.grid(
  shrinkage = c(.01, .05, .1),
  interaction.depth = c(1, 3, 5, 7),
  n.minobsinnode = c(5, 10, 15),
  bag.fraction = c(.65, .8, 1),
  optimal_trees = 0, 
  min_RMSE = 0   
  )

# numero total de combinações
nrow(hiper_grid)
```

## Avaliando o grid de parametros

```{r teste_grid}
# Busca no grid 
for(i in 1:nrow(hiper_grid)) {
  
  # 
  set.seed(21)
  
  # treina o modelo
  gbm.tune <- gbm(
    formula = medv ~ .,
    distribution = "gaussian",
    data = conj_treino,
    n.trees = 6000,
    interaction.depth = hiper_grid$interaction.depth[i],
    shrinkage = hiper_grid$shrinkage[i],
    n.minobsinnode = hiper_grid$n.minobsinnode[i],
    bag.fraction = hiper_grid$bag.fraction[i],
    train.fraction = .75,
    n.cores = NULL, 
    verbose = FALSE
  )
  
 # adiciona os erros de treino e arvores ao grid
  hiper_grid$optimal_trees[i] <- which.min(gbm.tune$valid.error)
  hiper_grid$min_RMSE[i] <- sqrt(min(gbm.tune$valid.error))
}

hiper_grid %>% dplyr::arrange(min_RMSE) %>% head(10)
```

## Modelo final

```{r gbmfinal}
# 
set.seed(21)

# treina o modelo GBM
gbm.fit.final <- gbm(
  formula = medv ~ .,
  distribution = "gaussian",
  data = conj_treino,
  n.trees = 3074,
  interaction.depth = 1,
  shrinkage = 0.10,
  n.minobsinnode = 15,
  bag.fraction = 0.65, 
  train.fraction = 1,
  n.cores = NULL, 
  verbose = FALSE
  )  

```

## Variable importance

```{r vi}
summary(
  gbm.fit.final, 
  cBars = 13,
  method = relative.influence, # também pode ser usado permutation.test.gbm
  las = 2
  )

```

## Previsão

```{r previsão}


# Fazendo Previsões
previsao1 <- predict(gbm.fit.final, 
                     newdata = conj_teste,
                     n.trees=gbm.fit.final$n.trees)
head(previsao1)

# Calcula os erros de previsão
caret::RMSE(previsao1, conj_teste$medv)
```

## Entendendo melhor os resultados

```{r LIME}
library(lime)
model_type.gbm <- function(x, ...) {
  return("regression")
}

predict_model.gbm <- function(x, newdata, ...) {
  pred <- predict(x, newdata, n.trees = x$n.trees)
  return(as.data.frame(pred))
}
# Algumas observações para avaliar
obs_pontuais <- conj_teste[1:2, ]

# aplica o LIME
explicador <- lime(conj_treino, gbm.fit.final)
explicacao <- explain(obs_pontuais, explicador, n_features = 5)
plot_features(explicacao)
```

## Gráfico de Dependencia Parcial (Partial Dependence Plot)

```{r}
graf_rm <- plot(gbm.fit.final, i = "rm")
graf_lstat <- plot(gbm.fit.final, i = "lstat")
gridExtra::grid.arrange(graf_lstat, graf_rm, ncol = 2)
```
