{
  "hash": "ee5cfa56ecc66d89bd831669aa3b7b00",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"KNN\"\nauthor: \"Ricardo Accioly\"\ndate: \"2024-03-05\"\nformat:\n html:\n    code-link: true\n    fig-width: 9\n    fig-height: 7\n    fig-dpi: 300\nknitr:\n  opts_chunk: \n    out.width: 90%\n    comment: \"#>\"\n---\n\n\n## KNN\n\n**O KNN é um algoritmo muito simples no qual cada observação é prevista com base em sua \"semelhança\" com outras observações. Ao contrário da maioria dos métodos, KNN é um algoritmo baseado na memória e não pode ser resumido por um modelo de forma fechada. Isso significa que as amostras de treinamento são necessárias no tempo de execução e as previsões são feitas diretamente das relações amostrais. Consequentemente, os KNNs também são conhecidos como aprendizes preguiçosos**\n\n## Carregando Bibliotecas\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(ISLR)\ndata(Default)\nsummary(Default)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>  default    student       balance           income     \n#>  No :9667   No :7056   Min.   :   0.0   Min.   :  772  \n#>  Yes: 333   Yes:2944   1st Qu.: 481.7   1st Qu.:21340  \n#>                        Median : 823.6   Median :34553  \n#>                        Mean   : 835.4   Mean   :33517  \n#>                        3rd Qu.:1166.3   3rd Qu.:43808  \n#>                        Max.   :2654.3   Max.   :73554\n```\n\n\n:::\n\n```{.r .cell-code}\nstr(Default)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> 'data.frame':\t10000 obs. of  4 variables:\n#>  $ default: Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n#>  $ student: Factor w/ 2 levels \"No\",\"Yes\": 1 2 1 1 1 2 1 2 1 1 ...\n#>  $ balance: num  730 817 1074 529 786 ...\n#>  $ income : num  44362 12106 31767 35704 38463 ...\n```\n\n\n:::\n\n```{.r .cell-code}\nhead(Default)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>   default student   balance    income\n#> 1      No      No  729.5265 44361.625\n#> 2      No     Yes  817.1804 12106.135\n#> 3      No      No 1073.5492 31767.139\n#> 4      No      No  529.2506 35704.494\n#> 5      No      No  785.6559 38463.496\n#> 6      No     Yes  919.5885  7491.559\n```\n\n\n:::\n:::\n\n\n## Manipulando os dados\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncredito <- tibble(Default)\nsummary(credito)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>  default    student       balance           income     \n#>  No :9667   No :7056   Min.   :   0.0   Min.   :  772  \n#>  Yes: 333   Yes:2944   1st Qu.: 481.7   1st Qu.:21340  \n#>                        Median : 823.6   Median :34553  \n#>                        Mean   : 835.4   Mean   :33517  \n#>                        3rd Qu.:1166.3   3rd Qu.:43808  \n#>                        Max.   :2654.3   Max.   :73554\n```\n\n\n:::\n\n```{.r .cell-code}\n# renomeando colunas\ncredito <- credito %>% \n                rename( inadimplente = default, estudante = student, balanco = balance,\n                receita = income)\ncredito <- credito %>% mutate( inadimplente =  case_when(\n                           inadimplente == \"No\"  ~ \"Nao\",\n                           inadimplente == \"Yes\" ~ \"Sim\"\n                          )) %>% mutate(inadimplente = factor(inadimplente))\ncredito <- credito %>% mutate( estudante =  case_when(\n                           estudante == \"No\"  ~ 0,\n                           estudante == \"Yes\" ~ 1\n                          )) \n\nstr(credito)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> tibble [10,000 × 4] (S3: tbl_df/tbl/data.frame)\n#>  $ inadimplente: Factor w/ 2 levels \"Nao\",\"Sim\": 1 1 1 1 1 1 1 1 1 1 ...\n#>  $ estudante   : num [1:10000] 0 1 0 0 0 1 0 1 0 0 ...\n#>  $ balanco     : num [1:10000] 730 817 1074 529 786 ...\n#>  $ receita     : num [1:10000] 44362 12106 31767 35704 38463 ...\n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(credito)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>  inadimplente   estudante         balanco          receita     \n#>  Nao:9667     Min.   :0.0000   Min.   :   0.0   Min.   :  772  \n#>  Sim: 333     1st Qu.:0.0000   1st Qu.: 481.7   1st Qu.:21340  \n#>               Median :0.0000   Median : 823.6   Median :34553  \n#>               Mean   :0.2944   Mean   : 835.4   Mean   :33517  \n#>               3rd Qu.:1.0000   3rd Qu.:1166.3   3rd Qu.:43808  \n#>               Max.   :1.0000   Max.   :2654.3   Max.   :73554\n```\n\n\n:::\n:::\n\n\n## Normalização\n\nAntes de iniciarmos é fundamental fazermos a normalização (padronização) dos dados para que o KNN tenha um melhor desempenho.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncredito_n <- credito\ncredito_n[,3:4] <- scale(credito_n[,3:4])\n```\n:::\n\n\n## Treino e Teste\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\nset.seed(23)\n\ncredito_split <- initial_split(credito_n, prop = 0.75, strata = inadimplente)\n\ncredito_split\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> <Training/Testing/Total>\n#> <7500/2500/10000>\n```\n\n\n:::\n\n```{.r .cell-code}\nconj_treino <- training(credito_split)\nconj_teste <- testing(credito_split)\n\nsummary(conj_treino)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>  inadimplente   estudante         balanco             receita        \n#>  Nao:7247     Min.   :0.0000   Min.   :-1.726998   Min.   :-2.45527  \n#>  Sim: 253     1st Qu.:0.0000   1st Qu.:-0.723434   1st Qu.:-0.92363  \n#>               Median :0.0000   Median :-0.019582   Median : 0.08711  \n#>               Mean   :0.2967   Mean   : 0.007699   Mean   :-0.00120  \n#>               3rd Qu.:1.0000   3rd Qu.: 0.690083   3rd Qu.: 0.77371  \n#>               Max.   :1.0000   Max.   : 3.760371   Max.   : 3.00205\n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(conj_teste)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>  inadimplente   estudante         balanco            receita         \n#>  Nao:2420     Min.   :0.0000   Min.   :-1.72700   Min.   :-2.289610  \n#>  Sim:  80     1st Qu.:0.0000   1st Qu.:-0.75969   1st Qu.:-0.876082  \n#>               Median :0.0000   Median :-0.04094   Median : 0.038895  \n#>               Mean   :0.2876   Mean   :-0.02310   Mean   : 0.003599  \n#>               3rd Qu.:1.0000   3rd Qu.: 0.66322   3rd Qu.: 0.765076  \n#>               Max.   :1.0000   Max.   : 3.60356   Max.   : 2.876421\n```\n\n\n:::\n:::\n\n\n## Matriz de dispersão\n\nVamos agora explorar os dados originais para termos algum visão do comportamento das variáveis explicativas e a variável dependente.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(psych)\npairs.panels(credito, \n             method = \"pearson\", # metodo de correlação\n             hist.col = \"#00AFBB\",\n             density = TRUE,  # mostra graficos de densidade\n             ellipses = FALSE # mostra elipses de correlação\n             )\n```\n\n::: {.cell-output-display}\n![](Aula08_files/figure-html/splom-1.png){width=90%}\n:::\n:::\n\n\n## Avaliando o comportamento das variáveis em função do status (inadimplente / estudante)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(credito, aes(x=inadimplente, y=balanco)) +\n  geom_boxplot()\n```\n\n::: {.cell-output-display}\n![](Aula08_files/figure-html/box-plot-1.png){width=90%}\n:::\n\n```{.r .cell-code}\nggplot(credito, aes(x=inadimplente, y=receita)) +\n  geom_boxplot()\n```\n\n::: {.cell-output-display}\n![](Aula08_files/figure-html/box-plot-2.png){width=90%}\n:::\n\n```{.r .cell-code}\nggplot(credito, aes(x=as.factor(estudante), y=balanco)) +\n  geom_boxplot()\n```\n\n::: {.cell-output-display}\n![](Aula08_files/figure-html/box-plot-3.png){width=90%}\n:::\n\n```{.r .cell-code}\nggplot(credito, aes(x=as.factor(estudante), y=receita)) +\n  geom_boxplot()\n```\n\n::: {.cell-output-display}\n![](Aula08_files/figure-html/box-plot-4.png){width=90%}\n:::\n:::\n\n\n## Explorando um pouco mais Balanço e Receita\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(credito, aes(x=balanco)) +\n  geom_histogram(bins = round(1+3.322*log10(nrow(conj_treino)),0))\n```\n\n::: {.cell-output-display}\n![](Aula08_files/figure-html/histogramas-1.png){width=90%}\n:::\n\n```{.r .cell-code}\nggplot(credito, aes(x=receita)) +\n    geom_histogram(bins = round(1+3.322*log10(nrow(conj_treino)),0))\n```\n\n::: {.cell-output-display}\n![](Aula08_files/figure-html/histogramas-2.png){width=90%}\n:::\n:::\n\n\n## Balanço vs Receita\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data = credito, aes(x=balanco,  y = receita, col = inadimplente)) + geom_point() \n```\n\n::: {.cell-output-display}\n![](Aula08_files/figure-html/dispersao-1.png){width=90%}\n:::\n:::\n\n\n## KNN\n\n**Vamos usar a função knn da biblioteca caret que tem ótimas funcionalidades. Observem que a saída pode ser as classes ou as probabilidades de pertencer a uma classe**\n\n**Como o KNN usa as distancias entre os pontos ele é afetado pela escala dos dados, portanto, é necessário que os dados sejam normalizados (padronizados) para eliminar este efeito.**\n\n**Quando temos diversas variáveis explicativas em diferentes escalas, em geral, elas devem ser transformadas para ter media zero e desvio padrão 1**\n\n## 1a Modelo\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Vamos usar a regra da raiz quadrada do tamnho da amostra\nlibrary(caret)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n#> Carregando pacotes exigidos: lattice\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n#> \n#> Attaching package: 'caret'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n#> The following objects are masked from 'package:yardstick':\n#> \n#>     precision, recall, sensitivity, specificity\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n#> The following object is masked from 'package:purrr':\n#> \n#>     lift\n```\n\n\n:::\n\n```{.r .cell-code}\nsqrt(nrow(conj_treino)) ## ~90\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] 86.60254\n```\n\n\n:::\n\n```{.r .cell-code}\nset.seed(23)\nt_knn1 <- knn3(inadimplente ~ balanco + receita + estudante, data = conj_treino, k = 90)\nt_knn1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> 90-nearest neighbor model\n#> Training set outcome distribution:\n#> \n#>  Nao  Sim \n#> 7247  253\n```\n\n\n:::\n:::\n\n\n## Avaliando o modelo\n\n**A acurácia deu um valor alto, mas isto não é suficiente para considerarmos que temos um bom modelo. Veja que a sensibilidade está muito baixa e que o ideal é que tenhamos valores altos de sensibilidade e especificidade.**\n\n**Observar que a prevalência é muito baixa o que está afetando os resultados do modelo**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## \ny_chapeu_knn1 <- predict(t_knn1, conj_teste, type = \"class\")\n\n# Matriz de confusão para valiar os resultados\nconfusionMatrix(y_chapeu_knn1, conj_teste$inadimplente, positive=\"Sim\") \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> Confusion Matrix and Statistics\n#> \n#>           Reference\n#> Prediction  Nao  Sim\n#>        Nao 2418   69\n#>        Sim    2   11\n#>                                           \n#>                Accuracy : 0.9716          \n#>                  95% CI : (0.9643, 0.9778)\n#>     No Information Rate : 0.968           \n#>     P-Value [Acc > NIR] : 0.1672          \n#>                                           \n#>                   Kappa : 0.2297          \n#>                                           \n#>  Mcnemar's Test P-Value : 4.773e-15       \n#>                                           \n#>             Sensitivity : 0.1375          \n#>             Specificity : 0.9992          \n#>          Pos Pred Value : 0.8462          \n#>          Neg Pred Value : 0.9723          \n#>              Prevalence : 0.0320          \n#>          Detection Rate : 0.0044          \n#>    Detection Prevalence : 0.0052          \n#>       Balanced Accuracy : 0.5683          \n#>                                           \n#>        'Positive' Class : Sim             \n#> \n```\n\n\n:::\n:::\n\n\n## Curva ROC\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(pROC)\n\n# Para a curva ROC preciso das probabilidades e não das classes\np_chapeu_knn1 <- predict(t_knn1, conj_teste, type = \"prob\")\nhead(p_chapeu_knn1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>            Nao        Sim\n#> [1,] 0.9777778 0.02222222\n#> [2,] 1.0000000 0.00000000\n#> [3,] 1.0000000 0.00000000\n#> [4,] 0.9888889 0.01111111\n#> [5,] 1.0000000 0.00000000\n#> [6,] 1.0000000 0.00000000\n```\n\n\n:::\n\n```{.r .cell-code}\n# Aqui gera o curva e salvo numa variável\nroc_knn1 <- roc(conj_teste$inadimplente ~ p_chapeu_knn1[,2], plot = TRUE, print.auc=FALSE, col=\"black\", legacy.axes=TRUE)\n\nlegend(\"bottomright\",legend=c(\"KNN1\"), \n       col=c(\"black\"),lwd=4)\n```\n\n::: {.cell-output-display}\n![](Aula08_files/figure-html/ROC-1.png){width=90%}\n:::\n\n```{.r .cell-code}\n# Area abaixo da Curva (AUC)\nas.numeric(roc_knn1$auc)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] 0.9554416\n```\n\n\n:::\n:::\n\n\n## Variando K\n\n**Anteriormente usamos k=90. Este parametro deve ser ajustado para melhoramos os modelo KNN. Para isto vamos usar a função train da biblioteca caret**\n\n**Observe que a otimização de k é feita através de acurácia**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(23)\n\n# Usando validação cruzada para obter o valor de k através da função train da biblioteca caret e o controle do treino e fazendo um gride de valores para k.\nctrl <- trainControl(method = \"repeatedcv\", \n                     number = 10,\n                     repeats = 5)\nt_knn2 <- train(inadimplente ~ balanco + receita + estudante,\n                method = \"knn\", trControl= ctrl, \n                tuneGrid = data.frame(k = seq(10,200, by=10)),\n                data = conj_treino)\n## Resultados do treino\nt_knn2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> k-Nearest Neighbors \n#> \n#> 7500 samples\n#>    3 predictor\n#>    2 classes: 'Nao', 'Sim' \n#> \n#> No pre-processing\n#> Resampling: Cross-Validated (10 fold, repeated 5 times) \n#> Summary of sample sizes: 6751, 6750, 6751, 6750, 6749, 6750, ... \n#> Resampling results across tuning parameters:\n#> \n#>   k    Accuracy   Kappa      \n#>    10  0.9713601  0.395475047\n#>    20  0.9724005  0.390400181\n#>    30  0.9717869  0.358817460\n#>    40  0.9713068  0.327821536\n#>    50  0.9709604  0.305695087\n#>    60  0.9706936  0.289863038\n#>    70  0.9701338  0.257688316\n#>    80  0.9692001  0.196534456\n#>    90  0.9682936  0.132464948\n#>   100  0.9672805  0.064300570\n#>   110  0.9664537  0.014698327\n#>   120  0.9663737  0.005906813\n#>   130  0.9662937  0.001491003\n#>   140  0.9662670  0.000000000\n#>   150  0.9662670  0.000000000\n#>   160  0.9662670  0.000000000\n#>   170  0.9662670  0.000000000\n#>   180  0.9662670  0.000000000\n#>   190  0.9662670  0.000000000\n#>   200  0.9662670  0.000000000\n#> \n#> Accuracy was used to select the optimal model using the largest value.\n#> The final value used for the model was k = 20.\n```\n\n\n:::\n\n```{.r .cell-code}\nplot(t_knn2)\n```\n\n::: {.cell-output-display}\n![](Aula08_files/figure-html/K1-1.png){width=90%}\n:::\n\n```{.r .cell-code}\n## Previsões com o resultaddos do treino\nprev_knn2 <- predict(t_knn2, conj_teste)\nconfusionMatrix(prev_knn2, conj_teste$inadimplente,  positive=\"Sim\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> Confusion Matrix and Statistics\n#> \n#>           Reference\n#> Prediction  Nao  Sim\n#>        Nao 2411   57\n#>        Sim    9   23\n#>                                           \n#>                Accuracy : 0.9736          \n#>                  95% CI : (0.9665, 0.9795)\n#>     No Information Rate : 0.968           \n#>     P-Value [Acc > NIR] : 0.05929         \n#>                                           \n#>                   Kappa : 0.3997          \n#>                                           \n#>  Mcnemar's Test P-Value : 7.238e-09       \n#>                                           \n#>             Sensitivity : 0.2875          \n#>             Specificity : 0.9963          \n#>          Pos Pred Value : 0.7188          \n#>          Neg Pred Value : 0.9769          \n#>              Prevalence : 0.0320          \n#>          Detection Rate : 0.0092          \n#>    Detection Prevalence : 0.0128          \n#>       Balanced Accuracy : 0.6419          \n#>                                           \n#>        'Positive' Class : Sim             \n#> \n```\n\n\n:::\n:::\n\n\n## Variando K de outra forma\n\n**Vamos adicionar mais opções no trainControl**\n\n**Ao colocar classProb = TRUE e summaryFunction ao invés da acurácia a otimização passa a ser através o ROC**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(23)\n# ctrl <- trainControl(method = \"cv\", classProbs=TRUE, summaryFunction = twoClassSummary)\nctrl <- trainControl(method = \"repeatedcv\", \n                     number = 10,\n                     repeats = 5, \n                     classProbs = TRUE,\n                     summaryFunction = twoClassSummary)\n\nt_knn3 <- train(inadimplente ~ balanco + receita + estudante, \n                method = \"knn\", \n                trControl= ctrl, \n                tuneGrid = data.frame(k = seq(10,200, by=10)),\n                metric = \"ROC\",\n                data = conj_treino)\nt_knn3\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> k-Nearest Neighbors \n#> \n#> 7500 samples\n#>    3 predictor\n#>    2 classes: 'Nao', 'Sim' \n#> \n#> No pre-processing\n#> Resampling: Cross-Validated (10 fold, repeated 5 times) \n#> Summary of sample sizes: 6751, 6750, 6751, 6750, 6749, 6750, ... \n#> Resampling results across tuning parameters:\n#> \n#>   k    ROC        Sens       Spec       \n#>    10  0.8507417  0.9948946  0.297353846\n#>    20  0.8876876  0.9965781  0.280000000\n#>    30  0.9040110  0.9969921  0.249907692\n#>    40  0.9107228  0.9974888  0.221323077\n#>    50  0.9176347  0.9977649  0.203138462\n#>    60  0.9205913  0.9979580  0.189661538\n#>    70  0.9250583  0.9982892  0.163538462\n#>    80  0.9257268  0.9988687  0.119200000\n#>    90  0.9267604  0.9994205  0.076707692\n#>   100  0.9282657  0.9998069  0.035753846\n#>   110  0.9281271  0.9999172  0.007969231\n#>   120  0.9292234  1.0000000  0.003169231\n#>   130  0.9298903  1.0000000  0.000800000\n#>   140  0.9305323  1.0000000  0.000000000\n#>   150  0.9297919  1.0000000  0.000000000\n#>   160  0.9307489  1.0000000  0.000000000\n#>   170  0.9303360  1.0000000  0.000000000\n#>   180  0.9299261  1.0000000  0.000000000\n#>   190  0.9300955  1.0000000  0.000000000\n#>   200  0.9308838  1.0000000  0.000000000\n#> \n#> ROC was used to select the optimal model using the largest value.\n#> The final value used for the model was k = 200.\n```\n\n\n:::\n\n```{.r .cell-code}\nplot(t_knn3)\n```\n\n::: {.cell-output-display}\n![](Aula08_files/figure-html/K2-1.png){width=90%}\n:::\n\n```{.r .cell-code}\nprev_knn3 <- predict(t_knn3, conj_teste)\nconfusionMatrix(prev_knn3, conj_teste$inadimplente,  positive=\"Sim\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> Confusion Matrix and Statistics\n#> \n#>           Reference\n#> Prediction  Nao  Sim\n#>        Nao 2420   80\n#>        Sim    0    0\n#>                                           \n#>                Accuracy : 0.968           \n#>                  95% CI : (0.9603, 0.9745)\n#>     No Information Rate : 0.968           \n#>     P-Value [Acc > NIR] : 0.5297          \n#>                                           \n#>                   Kappa : 0               \n#>                                           \n#>  Mcnemar's Test P-Value : <2e-16          \n#>                                           \n#>             Sensitivity : 0.000           \n#>             Specificity : 1.000           \n#>          Pos Pred Value :   NaN           \n#>          Neg Pred Value : 0.968           \n#>              Prevalence : 0.032           \n#>          Detection Rate : 0.000           \n#>    Detection Prevalence : 0.000           \n#>       Balanced Accuracy : 0.500           \n#>                                           \n#>        'Positive' Class : Sim             \n#> \n```\n\n\n:::\n:::\n\n\n**Veja que ao otimizar pela ROC o modelo escolhido tem sensibilidade zero! Isto obviamente não é um bom modelo! Neste caso a opção de otimização do parametro pela acurácia dá melhores resultados.**\n\n## Curva ROC dos 2 melhores modelos k=90 e k=20\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprev_knn1 <- predict(t_knn1, conj_teste, type = \"prob\")\nprev_knn2 <- predict(t_knn2, conj_teste, type = \"prob\")\nroc_knn1 <- roc(conj_teste$inadimplente ~ prev_knn1[,2], plot = TRUE, print.auc=FALSE, col=\"black\", legacy.axes=TRUE)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n#> Setting levels: control = Nao, case = Sim\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n#> Setting direction: controls < cases\n```\n\n\n:::\n\n```{.r .cell-code}\nroc_knn2 <- roc(conj_teste$inadimplente ~ prev_knn2[,2], plot = TRUE, print.auc=FALSE, col=\"green\", legacy.axes=TRUE, add=TRUE)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n#> Setting levels: control = Nao, case = Sim\n#> Setting direction: controls < cases\n```\n\n\n:::\n\n```{.r .cell-code}\nlegend(\"bottomright\",legend=c(\"KNN1\", \"KNN2\"), \n       col=c(\"black\",\"green\"),lwd=4)\n```\n\n::: {.cell-output-display}\n![](Aula08_files/figure-html/ROC2-1.png){width=90%}\n:::\n\n```{.r .cell-code}\n## Area embaixo das curvas\nas.numeric(roc_knn1$auc)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] 0.9554416\n```\n\n\n:::\n\n```{.r .cell-code}\nas.numeric(roc_knn2$auc)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] 0.9269034\n```\n\n\n:::\n:::\n\n\n**Observe que os resultados de área abaixo da ROC não são suficientes para a escolha do k, pois precisamos estar atentos a sensibilidade e especificidade!**\n\n**Os resultados encontrados apontam k=20 como a melhor opção**\n",
    "supporting": [
      "Aula08_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}