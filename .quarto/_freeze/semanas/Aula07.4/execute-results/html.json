{
  "hash": "a2ff513bdc9a0328aa2532b28934e3e9",
  "result": {
    "markdown": "---\ntitle: \"Regularização de Modelos\"\nauthor: \"Ricardo Accioly\"\ndate: \"2023-05-05\"\noutput:\n html_document:\n    toc: yes\n    code_download: yes\n---\n\n\n## Regularização de modelos\n\n\n\n\n\n## Carregando Bibliotecas\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(MASS)\nlibrary(tidyverse)\nlibrary(glmnet)\ndata(Boston)\n```\n:::\n\n\n## Carregando os dados\n\nVamos utilizar neste exemplo os dados contidos na biblioteca MASS. A base de dados Boston tem 506 de valores preços medianos de casas na região de Boston com 13 outras variáveis explicativas (potencialmente). Vamos explorar os dados e ajustar modelos com penalização o Ridge e o LASSO e depois vamos comparar com os mínimos quadrados.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(Boston)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     crim zn indus chas   nox    rm  age    dis rad tax ptratio  black lstat\n1 0.00632 18  2.31    0 0.538 6.575 65.2 4.0900   1 296    15.3 396.90  4.98\n2 0.02731  0  7.07    0 0.469 6.421 78.9 4.9671   2 242    17.8 396.90  9.14\n3 0.02729  0  7.07    0 0.469 7.185 61.1 4.9671   2 242    17.8 392.83  4.03\n4 0.03237  0  2.18    0 0.458 6.998 45.8 6.0622   3 222    18.7 394.63  2.94\n5 0.06905  0  2.18    0 0.458 7.147 54.2 6.0622   3 222    18.7 396.90  5.33\n6 0.02985  0  2.18    0 0.458 6.430 58.7 6.0622   3 222    18.7 394.12  5.21\n  medv\n1 24.0\n2 21.6\n3 34.7\n4 33.4\n5 36.2\n6 28.7\n```\n:::\n\n```{.r .cell-code}\nsummary(Boston)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      crim                zn             indus            chas        \n Min.   : 0.00632   Min.   :  0.00   Min.   : 0.46   Min.   :0.00000  \n 1st Qu.: 0.08205   1st Qu.:  0.00   1st Qu.: 5.19   1st Qu.:0.00000  \n Median : 0.25651   Median :  0.00   Median : 9.69   Median :0.00000  \n Mean   : 3.61352   Mean   : 11.36   Mean   :11.14   Mean   :0.06917  \n 3rd Qu.: 3.67708   3rd Qu.: 12.50   3rd Qu.:18.10   3rd Qu.:0.00000  \n Max.   :88.97620   Max.   :100.00   Max.   :27.74   Max.   :1.00000  \n      nox               rm             age              dis        \n Min.   :0.3850   Min.   :3.561   Min.   :  2.90   Min.   : 1.130  \n 1st Qu.:0.4490   1st Qu.:5.886   1st Qu.: 45.02   1st Qu.: 2.100  \n Median :0.5380   Median :6.208   Median : 77.50   Median : 3.207  \n Mean   :0.5547   Mean   :6.285   Mean   : 68.57   Mean   : 3.795  \n 3rd Qu.:0.6240   3rd Qu.:6.623   3rd Qu.: 94.08   3rd Qu.: 5.188  \n Max.   :0.8710   Max.   :8.780   Max.   :100.00   Max.   :12.127  \n      rad              tax           ptratio          black       \n Min.   : 1.000   Min.   :187.0   Min.   :12.60   Min.   :  0.32  \n 1st Qu.: 4.000   1st Qu.:279.0   1st Qu.:17.40   1st Qu.:375.38  \n Median : 5.000   Median :330.0   Median :19.05   Median :391.44  \n Mean   : 9.549   Mean   :408.2   Mean   :18.46   Mean   :356.67  \n 3rd Qu.:24.000   3rd Qu.:666.0   3rd Qu.:20.20   3rd Qu.:396.23  \n Max.   :24.000   Max.   :711.0   Max.   :22.00   Max.   :396.90  \n     lstat            medv      \n Min.   : 1.73   Min.   : 5.00  \n 1st Qu.: 6.95   1st Qu.:17.02  \n Median :11.36   Median :21.20  \n Mean   :12.65   Mean   :22.53  \n 3rd Qu.:16.95   3rd Qu.:25.00  \n Max.   :37.97   Max.   :50.00  \n```\n:::\n:::\n\n\nObservamos acima que todas as variáveis são quantitativas e que não há necessidade de transformações.\n\n## Significado das variáveis\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Boston Database\n# \n#1) crim - taxa de criminalidade per capita por cidade.\n# \n#2) zn - proporção de terrenos residenciais para lotes acima de 25,000 sq.ft.\n# \n#3) indus - proporção de negócios não comerciais por acres e por cidade.\n# \n#4) chas - variável dummy do Rio Charles(= 1 se próximo do rio; 0 de outra forma).\n# \n#5) nox - concentração de óxido de nitrogênio (partes por 10 milhões).\n# \n#6) rm - número médio de quartos por habitação\n# \n#7) age - proporção da unidade ocupadas pelos proprietários construídas antes 1940.\n# \n#8) dis - média ponderada das distâncias dos 5 pontos de emprego em Boston.\n# \n#9) rad - indice de acessibilidade das avenidas radiais.\n# \n#10) tax - valor cheio da taxa de propriedade por $10,000.\n# \n#11) ptratio - razão aluno-professor por cidade.\n# \n#12) black - 1000(Bk−0.63)21000(Bk−0.63)2 proporção de negros por cidade.\n# \n#13) lstat - percentual de baixo status da população.\n# \n#14) medv - valor mediano das cas ocupadas pelos proprietário em $1000s. (Var. Resposta)\n```\n:::\n\n\n## Conjunto de treino e de teste\n\nObservar que retiramos a variável **rad**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(caret)\nset.seed(21)\ny <- Boston$medv\nindice_teste <- createDataPartition(y, times = 1, p = 0.2, list = FALSE)\n\nconj_treino <- Boston %>% slice(-indice_teste)\nconj_treino <- conj_treino %>% select(-rad)\nconj_teste <- Boston %>% slice(indice_teste)\nconj_teste <- conj_teste %>% select(-rad)\nstr(conj_treino)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'data.frame':\t403 obs. of  13 variables:\n $ crim   : num  0.00632 0.02731 0.02729 0.03237 0.06905 ...\n $ zn     : num  18 0 0 0 0 0 12.5 12.5 12.5 12.5 ...\n $ indus  : num  2.31 7.07 7.07 2.18 2.18 2.18 7.87 7.87 7.87 7.87 ...\n $ chas   : int  0 0 0 0 0 0 0 0 0 0 ...\n $ nox    : num  0.538 0.469 0.469 0.458 0.458 0.458 0.524 0.524 0.524 0.524 ...\n $ rm     : num  6.58 6.42 7.18 7 7.15 ...\n $ age    : num  65.2 78.9 61.1 45.8 54.2 58.7 66.6 96.1 85.9 94.3 ...\n $ dis    : num  4.09 4.97 4.97 6.06 6.06 ...\n $ tax    : num  296 242 242 222 222 222 311 311 311 311 ...\n $ ptratio: num  15.3 17.8 17.8 18.7 18.7 18.7 15.2 15.2 15.2 15.2 ...\n $ black  : num  397 397 393 395 397 ...\n $ lstat  : num  4.98 9.14 4.03 2.94 5.33 ...\n $ medv   : num  24 21.6 34.7 33.4 36.2 28.7 22.9 27.1 18.9 15 ...\n```\n:::\n\n```{.r .cell-code}\nstr(conj_teste)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'data.frame':\t103 obs. of  13 variables:\n $ crim   : num  0.211 0.63 0.627 1.252 0.852 ...\n $ zn     : num  12.5 0 0 0 0 0 0 75 0 0 ...\n $ indus  : num  7.87 8.14 8.14 8.14 8.14 8.14 8.14 2.95 6.91 6.91 ...\n $ chas   : int  0 0 0 0 0 0 0 0 0 0 ...\n $ nox    : num  0.524 0.538 0.538 0.538 0.538 0.538 0.538 0.428 0.448 0.448 ...\n $ rm     : num  5.63 5.95 5.83 5.57 5.96 ...\n $ age    : num  100 61.8 56.5 98.1 89.2 94.1 96.9 21.8 6.5 95.3 ...\n $ dis    : num  6.08 4.71 4.5 3.8 4.01 ...\n $ tax    : num  311 307 307 307 307 307 307 252 233 233 ...\n $ ptratio: num  15.2 21 21 21 21 21 21 18.3 17.9 17.9 ...\n $ black  : num  387 397 396 377 393 ...\n $ lstat  : num  29.93 8.26 8.47 21.02 13.83 ...\n $ medv   : num  16.5 20.4 19.9 13.6 19.6 12.7 13.5 30.8 24.7 14.4 ...\n```\n:::\n:::\n\n\n## Métodos de Regularização\n\nO pacote glmnet não usa a linguagem de formula, em particular nós devemos passar $x$ como uma matriz e $y$ como um vetor, pois não se usa a sintaxe $y \\sim x$. Com isso será necessário ajustar x e y. A função model.matrix() é particularmente útil para criar x; não só produz uma matriz correspondente as variáveis explicativas, **mas também transforma automaticamente quaisquer variáveis qualitativas em variáveis dummy. Esta última propriedade é importante porque o glmnet() só pode tomar insumos numéricos e quantitativos.**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx_treino <- model.matrix(medv ~ . , data = conj_treino)[, -1]\ny_treino <- conj_treino$medv\n\nx_teste <- model.matrix(medv ~ . , data = conj_teste)[, -1]\ny_teste = conj_teste$medv\n```\n:::\n\n\n## Regressão Ridge\n\nPrimeiro vamos ajustar um modelo de regressão Ridge. Isso é conseguido chamando `glmnet()` com `alpha=0`, se `alpha=1` então `glmnet()` ajusta um lasso.(veja o arquivo de ajuda).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Estabelecendo um grid de valores para lambda\ngrid <- 10^seq(-2, 10, length = 100)\najusreg.ridge <- glmnet(x_treino, y_treino, alpha=0, lambda = grid)\n```\n:::\n\n\nPor padrão, a função `glmnet()` executa a regressão ridge automaticamente selecionando a faixa de valores de $\\lambda$. No entanto, aqui nós escolhemos implementar usando uma grade de valores que variam de $\\lambda = 10^{-2}$ a $\\lambda = 10^{10}$, cobrindo toda a gama de cenários do modelo nulo contendo apenas o coeficiente linear até o ajuste dos mínimos quadrados.\n\nTambém podemos calcular o modelo para um valor particular de $\\lambda$ que não é um dos valores de grade. Observe que, por padrão, a função `glmnet()` padroniza as variáveis para que elas estejam na mesma escala. **Esta padronização é muito importante no caso da regressão Ridge, pois ela é afetada pela mudança de escala das variáveis explicativas.**\n\nAssociado a cada valor de $\\lambda$ existe um vetor de coeficientes de regressão de ridge, que é armazenado em uma matriz que pode ser acessada por 'coef()'. Neste caso, é uma matriz $13 \\times 100$, com 13 linhas (uma para cada preditor, mais uma para o coeficiente linear) e 100 colunas (uma para cada valor de $\\lambda$).\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndim(coef(ajusreg.ridge))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1]  13 100\n```\n:::\n\n```{.r .cell-code}\nplot(ajusreg.ridge, xvar=\"lambda\", label=TRUE) # Representando os coeficientes\n```\n\n::: {.cell-output-display}\n![](imagens/Aula07_4/r1-1.png){width=864}\n:::\n:::\n\n\nQuando $\\lambda$ é grande o esperado é que os coeficentes sejam pequenos e quando $\\lambda$ é pequeno os coeficientes assumem valores maiores.\n\n\n::: {.cell}\n\n```{.r .cell-code}\najusreg.ridge$lambda[1] # Mostra primeiro valor de lambda\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1e+10\n```\n:::\n\n```{.r .cell-code}\ncoef(ajusreg.ridge)[,1] # Mostra os coeficientes associados com o primeiro valor\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  (Intercept)          crim            zn         indus          chas \n 2.247246e+01 -4.109894e-10  1.380771e-10 -6.245296e-10  6.798952e-09 \n          nox            rm           age           dis           tax \n-3.138930e-08  9.031398e-09 -1.154044e-10  1.094430e-09 -2.354379e-11 \n      ptratio         black         lstat \n-2.000167e-09  3.131126e-11 -8.446650e-10 \n```\n:::\n\n```{.r .cell-code}\najusreg.ridge$lambda[100] # Mostra centésimo valor de lambda\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.01\n```\n:::\n\n```{.r .cell-code}\ncoef(ajusreg.ridge)[,100] # Mostra os coeficientes associados com o centésimo valor\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  (Intercept)          crim            zn         indus          chas \n 1.678767e+01 -1.657453e-02  2.307677e-02 -6.932521e-02  3.914065e+00 \n          nox            rm           age           dis           tax \n-1.315187e+01  5.586213e+00 -2.297883e-02 -1.312388e+00  6.917237e-04 \n      ptratio         black         lstat \n-8.347461e-01  1.262501e-02 -3.575121e-01 \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(plotmo)\nplot_glmnet(ajusreg.ridge)\n```\n\n::: {.cell-output-display}\n![](imagens/Aula07_4/r3-1.png){width=864}\n:::\n:::\n\n\n## Cross-Validation no Ridge\n\nNós podemos usar o k-fold cross validation para identificar o melhor valor de $\\lambda$\n\nA biblioteca glmnet já tem internamente uma função para uso do crosss validation. O default são 10 envelopes de dados `nfold=10`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(21)\nridge_cv <- cv.glmnet(x_treino,y_treino, alpha=0) ## por padrão k=10\nplot(ridge_cv)\n```\n\n::: {.cell-output-display}\n![](imagens/Aula07_4/r4-1.png){width=864}\n:::\n\n```{.r .cell-code}\nm_lamb <- ridge_cv$lambda.min  # Seleciona o lambda que minimiza o MSE (EQM) de treino\nm_lamb\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.6844251\n```\n:::\n\n```{.r .cell-code}\nlog(m_lamb)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] -0.3791761\n```\n:::\n\n```{.r .cell-code}\ncoef(ridge_cv, s=m_lamb)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n13 x 1 sparse Matrix of class \"dgCMatrix\"\n                       s1\n(Intercept) 14.7144706557\ncrim        -0.0214927174\nzn           0.0200504882\nindus       -0.0735317662\nchas         3.8343862492\nnox         -9.4460044931\nrm           5.3120091904\nage         -0.0185326152\ndis         -1.0110801641\ntax         -0.0003434427\nptratio     -0.7785448609\nblack        0.0116393260\nlstat       -0.3465721929\n```\n:::\n:::\n\n\n## Avaliando com conjunto de teste\n\nEm seguida avaliamos seu MSE no conjunto de teste, usando $\\lambda$ = m_lamb. Observe o uso da função 'predict()': desta vez temos previsões para um conjunto de teste, com o argumento `newx`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\najusreg.ridge2 <- glmnet(x_treino, y_treino, alpha=0, lambda = m_lamb)\ny_prev <- predict(ajusreg.ridge2, s = m_lamb, newx = x_teste)\n# Metricas de desempenho\nsqrt(mean((y_prev - y_teste)^2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 5.980257\n```\n:::\n:::\n\n\n## LASSO\n\nPrimeiro ajustamos com todos os dados como no caso do Ridge\n\n\n::: {.cell}\n\n```{.r .cell-code}\najusreg.lasso <- glmnet(x_treino,y_treino, alpha = 1)\nplot(ajusreg.lasso, xvar=\"lambda\", label=TRUE) # Representando os coeficientes\n```\n\n::: {.cell-output-display}\n![](imagens/Aula07_4/LASSO-1.png){width=864}\n:::\n\n```{.r .cell-code}\nplot_glmnet(ajusreg.lasso)\n```\n\n::: {.cell-output-display}\n![](imagens/Aula07_4/LASSO-2.png){width=864}\n:::\n:::\n\n\n## Validação Cruzada no LASSO\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlasso_cv <- cv.glmnet(x_treino,y_treino, alpha = 1)\nplot(lasso_cv)\n```\n\n::: {.cell-output-display}\n![](imagens/Aula07_4/Lasso2-1.png){width=864}\n:::\n\n```{.r .cell-code}\nm_lamb1 <- lasso_cv$lambda.min  # Seleciona o lambda que minimiza o MSE de treino\nm_lamb1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.0595278\n```\n:::\n\n```{.r .cell-code}\nlog(m_lamb1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] -2.821312\n```\n:::\n\n```{.r .cell-code}\ncoef(lasso_cv, s=m_lamb1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n13 x 1 sparse Matrix of class \"dgCMatrix\"\n                       s1\n(Intercept)  14.763435253\ncrim         -0.006217225\nzn            0.017479823\nindus        -0.050830256\nchas          3.644367387\nnox         -11.463040415\nrm            5.608066407\nage          -0.018352749\ndis          -1.107502465\ntax           .          \nptratio      -0.825247011\nblack         0.012263992\nlstat        -0.362939273\n```\n:::\n:::\n\n\n## Avaliando com conjunto de teste\n\n\n::: {.cell}\n\n```{.r .cell-code}\najusreg.lasso2 <- glmnet(x_treino, y_treino, alpha=1, lambda = m_lamb1)\ny_prev <- predict(ajusreg.lasso2, s = m_lamb1, newx = x_teste)\n# Metricas de desempenho\nsqrt(mean((y_prev - y_teste)^2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 6.027343\n```\n:::\n:::\n\n\n## Comparando com a seleção de modelos usando o Cp\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(leaps)\najusreg.comp <- regsubsets(medv ~ ., data=conj_treino, nvmax=12)\nsumario.reg <- summary(ajusreg.comp)\n## Os modelos vão ser escolhidos com base no menor Cp\nplot(sumario.reg$cp,xlab=\"Número de Variáveis\",ylab=\"Cp\")\nwhich.min(sumario.reg$cp)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 9\n```\n:::\n\n```{.r .cell-code}\npoints(9,sumario.reg$cp[9],pch=20,col=\"red\")\n```\n\n::: {.cell-output-display}\n![](imagens/Aula07_4/outro-1.png){width=864}\n:::\n:::\n\n\n## Ajustando no lm() e vendo o erro no conjunto de teste\n\nObservando so resultados de erro vemos que tanto a regressão Ridge como o LASSO apresentaram valores de erro maiores que o modelo definido através da melhor seleção de modelos (best subset regression). Aqui usamos o Cp de Mallows como critério de deleção de variáveis.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncoef(ajusreg.comp,9) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n (Intercept)           zn         chas          nox           rm          age \n 17.03537172   0.02326045   3.81378291 -14.60482205   5.66380782  -0.02328124 \n         dis      ptratio        black        lstat \n -1.26249225  -0.87101806   0.01301181  -0.36615078 \n```\n:::\n\n```{.r .cell-code}\noutro_mod <- lm(medv ~ zn + chas + nox + rm + age + dis + ptratio + black + lstat, data=conj_treino)\nsummary(outro_mod)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = medv ~ zn + chas + nox + rm + age + dis + ptratio + \n    black + lstat, data = conj_treino)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-18.9383  -2.6575  -0.5304   1.7899  27.0979 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  17.035372   5.393193   3.159 0.001707 ** \nzn            0.023260   0.014828   1.569 0.117534    \nchas          3.813783   1.008205   3.783 0.000179 ***\nnox         -14.604822   3.654556  -3.996 7.68e-05 ***\nrm            5.663808   0.473979  11.949  < 2e-16 ***\nage          -0.023281   0.014296  -1.629 0.104207    \ndis          -1.262492   0.209990  -6.012 4.19e-09 ***\nptratio      -0.871018   0.125529  -6.939 1.64e-11 ***\nblack         0.013012   0.002708   4.806 2.20e-06 ***\nlstat        -0.366151   0.057248  -6.396 4.54e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.608 on 393 degrees of freedom\nMultiple R-squared:  0.7549,\tAdjusted R-squared:  0.7493 \nF-statistic: 134.5 on 9 and 393 DF,  p-value: < 2.2e-16\n```\n:::\n\n```{.r .cell-code}\nsqrt(mean((conj_teste$medv - predict(outro_mod, conj_teste)) ^ 2)) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 6.032965\n```\n:::\n:::\n\n\n## E o BIC?\n\nE se escolhessemos o BIC como critério de seleção de variáveis explicativas? Neste caso os resultados foram iguais ao Cp. Entretanto, dá para perceber que o BIC apresentou uma certa estabilidade entre 7 e 9 variáveis. Se quisermos ter um modelo mais enxuto poderiamos optar por 7 variáveis.\n\n\n::: {.cell}\n\n```{.r .cell-code}\najusreg.comp1 <- regsubsets(medv ~ ., data=conj_treino, nvmax=12)\nsumario.reg1 <- summary(ajusreg.comp1)\n## Os modelos vão ser escolhidos com base no menor BIC\nplot(sumario.reg1$bic,xlab=\"Número de Variáveis\",ylab=\"BIC\")\nwhich.min(sumario.reg1$bic)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 7\n```\n:::\n\n```{.r .cell-code}\npoints(7,sumario.reg1$bic[7],pch=20,col=\"red\")\n```\n\n::: {.cell-output-display}\n![](imagens/Aula07_4/BIC-1.png){width=864}\n:::\n\n```{.r .cell-code}\ncoef(ajusreg.comp1,7) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n (Intercept)         chas          nox           rm          dis      ptratio \n 18.15699005   3.62527238 -16.32477597   5.61737907  -1.00175093  -0.95906300 \n       black        lstat \n  0.01229926  -0.39019735 \n```\n:::\n\n```{.r .cell-code}\noutro_mod1 <- lm(medv ~ chas + nox + rm + dis + ptratio + black + lstat, data=conj_treino)\nsummary(outro_mod1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = medv ~ chas + nox + rm + dis + ptratio + black + \n    lstat, data = conj_treino)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-18.4971  -2.7789  -0.5478   1.7933  26.9857 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  18.156990   5.385606   3.371 0.000822 ***\nchas          3.625272   1.009714   3.590 0.000372 ***\nnox         -16.324776   3.534591  -4.619 5.24e-06 ***\nrm            5.617379   0.457084  12.290  < 2e-16 ***\ndis          -1.001751   0.180856  -5.539 5.56e-08 ***\nptratio      -0.959063   0.119626  -8.017 1.24e-14 ***\nblack         0.012299   0.002702   4.552 7.07e-06 ***\nlstat        -0.390197   0.053934  -7.235 2.44e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.631 on 395 degrees of freedom\nMultiple R-squared:  0.7512,\tAdjusted R-squared:  0.7468 \nF-statistic: 170.4 on 7 and 395 DF,  p-value: < 2.2e-16\n```\n:::\n\n```{.r .cell-code}\nsqrt(mean((conj_teste$medv - predict(outro_mod1, conj_teste)) ^ 2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 6.023672\n```\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}