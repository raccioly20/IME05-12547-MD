[
  {
    "objectID": "visao-do-curso.html",
    "href": "visao-do-curso.html",
    "title": "Mineração de Dados",
    "section": "",
    "text": "Introdução a mineração de dados\nVisualização e preparação de Dados\nRegressão linear e seleção de modelos\nMétodos de Reamostragem\nMétodos de encolhimento (Ridge e Lasso)\nClassificação (KNN, Regressão Logística, LDA e QDA)\nMétodos baseados em árvores",
    "crumbs": [
      "Informação da disciplina",
      "Visão Geral"
    ]
  },
  {
    "objectID": "semanas/Aula13.html",
    "href": "semanas/Aula13.html",
    "title": "Análise de Clusters",
    "section": "",
    "text": "Este conteúdo foi adaptado de: https://mhahsler.github.io/Introduction_to_Data_Mining_R_Examples/book/clustering-analysis.html\n\nlibrary(tidyverse)\nlibrary(cluster)",
    "crumbs": [
      "Conteúdo Semanal",
      "Análise de Clusters"
    ]
  },
  {
    "objectID": "semanas/Aula13.html#bibliotecas",
    "href": "semanas/Aula13.html#bibliotecas",
    "title": "Análise de Clusters",
    "section": "",
    "text": "Este conteúdo foi adaptado de: https://mhahsler.github.io/Introduction_to_Data_Mining_R_Examples/book/clustering-analysis.html\n\nlibrary(tidyverse)\nlibrary(cluster)",
    "crumbs": [
      "Conteúdo Semanal",
      "Análise de Clusters"
    ]
  },
  {
    "objectID": "semanas/Aula13.html#dados",
    "href": "semanas/Aula13.html#dados",
    "title": "Análise de Clusters",
    "section": "Dados",
    "text": "Dados\nO conjunto de dados Ruspini, que consiste em 75 pontos dividido em quatro grupos, ele é popular para ilustrar técnicas de agrupamento. É um conjunto de dados muito simples com clusters bem separados. O conjunto de dados original tem os pontos ordenados por grupo. Podemos embaralhar os dados (linhas) usando sample_frac.\n\ndata(ruspini, package=\"cluster\")",
    "crumbs": [
      "Conteúdo Semanal",
      "Análise de Clusters"
    ]
  },
  {
    "objectID": "semanas/Aula13.html#manipulando-os-dados",
    "href": "semanas/Aula13.html#manipulando-os-dados",
    "title": "Análise de Clusters",
    "section": "Manipulando os dados",
    "text": "Manipulando os dados\n\nruspini &lt;- as_tibble(ruspini) %&gt;% sample_frac()\nruspini\n\n# A tibble: 75 × 2\n       x     y\n   &lt;int&gt; &lt;int&gt;\n 1    41   150\n 2    76    27\n 3    31    60\n 4    66    18\n 5    27    72\n 6    70     4\n 7    60   136\n 8    28    60\n 9    61    25\n10    83    21\n# ℹ 65 more rows",
    "crumbs": [
      "Conteúdo Semanal",
      "Análise de Clusters"
    ]
  },
  {
    "objectID": "semanas/Aula13.html#explorando-os-dados",
    "href": "semanas/Aula13.html#explorando-os-dados",
    "title": "Análise de Clusters",
    "section": "Explorando os dados",
    "text": "Explorando os dados\nNesta etapa os dados são avaliados, pois eventualmente temos situações de dados ausentes, pontos afastados.\n\nggplot(ruspini, aes(x = x, y = y)) + geom_point()\n\n\n\n\n\n\nsummary(ruspini)\n\n       x                y         \n Min.   :  4.00   Min.   :  4.00  \n 1st Qu.: 31.50   1st Qu.: 56.50  \n Median : 52.00   Median : 96.00  \n Mean   : 54.88   Mean   : 92.03  \n 3rd Qu.: 76.50   3rd Qu.:141.50  \n Max.   :117.00   Max.   :156.00",
    "crumbs": [
      "Conteúdo Semanal",
      "Análise de Clusters"
    ]
  },
  {
    "objectID": "semanas/Aula13.html#normalização",
    "href": "semanas/Aula13.html#normalização",
    "title": "Análise de Clusters",
    "section": "Normalização",
    "text": "Normalização\nComo os algoritmos usam medidas de distância é necessário usarmos a normalização para que os resultados naõ sejam afetados pela escala dos dados.\n\n## Aqui vamos essa função para fazer a normalização\nescala_numerica &lt;- function(x) x %&gt;% mutate_if(is.numeric, function(y) as.vector(scale(y)))\n\nruspini_norm &lt;- ruspini %&gt;% escala_numerica()\nsummary(ruspini_norm)\n\n       x                  y           \n Min.   :-1.66806   Min.   :-1.80743  \n 1st Qu.:-0.76649   1st Qu.:-0.72946  \n Median :-0.09442   Median : 0.08158  \n Mean   : 0.00000   Mean   : 0.00000  \n 3rd Qu.: 0.70879   3rd Qu.: 1.01582  \n Max.   : 2.03655   Max.   : 1.31355",
    "crumbs": [
      "Conteúdo Semanal",
      "Análise de Clusters"
    ]
  },
  {
    "objectID": "semanas/Aula13.html#métodos-para-obtenção-de-clusters",
    "href": "semanas/Aula13.html#métodos-para-obtenção-de-clusters",
    "title": "Análise de Clusters",
    "section": "Métodos para obtenção de Clusters",
    "text": "Métodos para obtenção de Clusters\nK-médias\nO algoritmo do k-médias usa a distância Eucliadiana quadrática. Aqui vamos usar k=4 e vamos rodar o algoritmo 10 vezes\n\nkm &lt;- kmeans(ruspini_norm, centers = 4, nstart = 10)\nkm\n\nK-means clustering with 4 clusters of sizes 23, 15, 20, 17\n\nCluster means:\n           x          y\n1 -0.3595425  1.1091151\n2  0.4607268 -1.4912271\n3 -1.1385941 -0.5559591\n4  1.4194387  0.4692907\n\nClustering vector:\n [1] 1 2 3 2 3 2 1 3 2 2 1 4 1 2 3 1 4 4 2 4 1 4 3 1 3 4 3 3 3 2 2 4 4 4 4 1 4 3\n[39] 1 1 1 4 4 3 4 2 1 3 1 4 3 4 1 1 2 1 3 1 1 1 2 2 1 4 3 3 3 1 1 3 1 2 3 2 3\n\nWithin cluster sum of squares by cluster:\n[1] 2.658679 1.082373 2.705477 3.641276\n (between_SS / total_SS =  93.2 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\n\n\nruspini_clusters &lt;- ruspini_norm %&gt;% add_column(cluster = factor(km$cluster))\nruspini_clusters\n\n# A tibble: 75 × 3\n        x      y cluster\n    &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;  \n 1 -0.455  1.19  1      \n 2  0.692 -1.34  2      \n 3 -0.783 -0.658 3      \n 4  0.365 -1.52  2      \n 5 -0.914 -0.411 3      \n 6  0.496 -1.81  2      \n 7  0.168  0.903 1      \n 8 -0.881 -0.658 3      \n 9  0.201 -1.38  2      \n10  0.922 -1.46  2      \n# ℹ 65 more rows\n\n\n\nggplot(ruspini_clusters, aes(x = x, y = y, color = cluster)) + geom_point()\n\n\n\n\n\n\n\nAdicionando os centroides aos gráficos\n\ncentroids &lt;- as_tibble(km$centers, rownames = \"cluster\")\ncentroids\n\n# A tibble: 4 × 3\n  cluster      x      y\n  &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n1 1       -0.360  1.11 \n2 2        0.461 -1.49 \n3 3       -1.14  -0.556\n4 4        1.42   0.469\n\nggplot(ruspini_clusters, aes(x = x, y = y, color = cluster)) + geom_point() + geom_point(data = centroids, aes(x = x, y = y, color = cluster), shape = 3, size = 10)\n\n\n\n\n\n\n\nVamos usar a biblioteca factoextra para visualizarmos os clusters\n\nlibrary(factoextra)\nfviz_cluster(km, data = ruspini_norm, centroids = TRUE, repel = TRUE, ellipse.type = \"norm\")\n\n\n\n\n\n\n\nk-medoides\nOs medoides pertencem ao proprio conjunto de dados. Podemos observar que o resultado é semelhante ao obtido no k-médias, mas o algoritmo é mais lento.\n\n#library(cluster)\nkmed &lt;- pam(ruspini_norm, k = 4)\nsummary(kmed)\n\nMedoids:\n     ID          x          y\n[1,] 24 -0.3566917  1.1698207\n[2,] 19  0.4629124 -1.4583746\n[3,] 67 -1.1762959 -0.5549325\n[4,] 12  1.4464374  0.5538374\nClustering vector:\n [1] 1 2 3 2 3 2 1 3 2 2 1 4 1 2 3 1 4 4 2 4 1 4 3 1 3 4 3 3 3 2 2 4 4 4 4 1 4 3\n[39] 1 1 1 4 4 3 4 2 1 3 1 4 3 4 1 1 2 1 3 1 1 1 2 2 1 4 3 3 3 1 1 3 1 2 3 2 3\nObjective function:\n    build      swap \n0.4422977 0.3187056 \n\nNumerical information per cluster:\n     size  max_diss   av_diss  diameter separation\n[1,]   23 0.6558680 0.2993397 1.1591436   0.767612\n[2,]   15 0.4589783 0.2433250 0.8359025   1.157682\n[3,]   20 0.5755656 0.3401125 1.1192822   1.157682\n[4,]   17 0.9459253 0.3862345 1.4627043   0.767612\n\nIsolated clusters:\n L-clusters: character(0)\n L*-clusters: [1] 2 3\n\nSilhouette plot information:\n   cluster neighbor sil_width\n24       1        3 0.8368407\n1        1        3 0.8305019\n49       1        4 0.8222142\n69       1        3 0.8220686\n21       1        3 0.8158429\n39       1        4 0.8150398\n56       1        3 0.8134280\n11       1        3 0.8064757\n58       1        4 0.7984225\n53       1        3 0.7969057\n47       1        3 0.7841631\n59       1        4 0.7794889\n40       1        4 0.7605512\n36       1        3 0.7591035\n13       1        3 0.7473901\n16       1        4 0.7423529\n63       1        3 0.7402623\n68       1        3 0.7249133\n41       1        4 0.7007372\n71       1        3 0.6739284\n54       1        4 0.5661372\n7        1        4 0.5413082\n60       1        4 0.4673917\n19       2        3 0.8592059\n74       2        3 0.8553255\n4        2        3 0.8530741\n46       2        3 0.8449473\n72       2        3 0.8361633\n14       2        3 0.8187150\n62       2        3 0.8178795\n2        2        4 0.8087015\n31       2        3 0.8013799\n6        2        3 0.7983516\n55       2        3 0.7918724\n61       2        3 0.7768261\n9        2        3 0.7727269\n10       2        4 0.7425993\n30       2        3 0.7328306\n67       3        1 0.8094377\n57       3        2 0.8027447\n66       3        1 0.7782513\n44       3        2 0.7704646\n28       3        2 0.7700388\n27       3        1 0.7597906\n29       3        1 0.7530091\n5        3        1 0.7436695\n8        3        2 0.7412965\n23       3        1 0.7270442\n73       3        2 0.7255183\n70       3        2 0.7226938\n25       3        2 0.7042349\n38       3        1 0.7026960\n48       3        1 0.6966533\n3        3        2 0.6921822\n15       3        2 0.6756339\n65       3        2 0.6463656\n75       3        1 0.6005277\n51       3        2 0.6004543\n18       4        1 0.7898609\n12       4        1 0.7834341\n43       4        1 0.7822308\n42       4        1 0.7790446\n34       4        1 0.7780891\n45       4        1 0.7694930\n33       4        1 0.7624335\n50       4        1 0.7609359\n22       4        1 0.7400337\n35       4        1 0.7392052\n64       4        1 0.7390493\n26       4        1 0.7234199\n52       4        1 0.5894345\n17       4        2 0.5666610\n32       4        1 0.5114355\n37       4        1 0.4358476\n20       4        1 0.3312348\nAverage silhouette width per cluster:\n[1] 0.7454551 0.8073733 0.7211353 0.6812849\nAverage silhouette width of total data set:\n[1] 0.7368082\n\n2775 dissimilarities, summarized :\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n0.03868 1.16780 1.92500 1.78630 2.47760 3.91720 \nMetric :  euclidean \nNumber of objects : 75\n\nAvailable components:\n [1] \"medoids\"    \"id.med\"     \"clustering\" \"objective\"  \"isolation\" \n [6] \"clusinfo\"   \"silinfo\"    \"diss\"       \"call\"       \"data\"      \n\nplot(kmed)\n\n\n\n\n\n\n\n\n\n\n\n\n\nOutra forma de visualização\n\nfviz_cluster(kmed, ruspini_norm,\n             ellipse.type = \"convex\",\n             repel =TRUE,\n             ggtheme =theme_minimal())\n\n\n\n\n\n\nknitr::kable(kmed$medoids)\n\n\n\nx\ny\n\n\n\n-0.3566917\n1.1698207\n\n\n0.4629124\n-1.4583746\n\n\n-1.1762959\n-0.5549325\n\n\n1.4464374\n0.5538374\n\n\n\n\nlibrary(janitor)\ntabyl(kmed$clustering)\n\n kmed$clustering  n   percent\n               1 23 0.3066667\n               2 15 0.2000000\n               3 20 0.2666667\n               4 17 0.2266667\n\n\nClusters Hierarquicos\nO agrupamento hierárquico começa com uma matriz de distância ´dist()´ e tem como padrão method=“Euclidiano”. As matrizes de distância tornam-se muito grandes rapidamente (tamanho e complexidade de tempo é O(n2) onde n é o número se pontos de dados. Só é possível calcular e armazenar a matriz para pequenos conjuntos de dados.\n\nd &lt;- dist(ruspini_norm)\n\nA função hclust() implementa o HCA, ou seja, o cluster hierarquico aglomerativo. Vamos começar usando o método da média.\n\nhc &lt;- hclust(d, method = \"average\")\n\nO HCA retorna um dendrograma e não uma definição de clusters.\n\nplot(hc)\n\n\n\n\n\n\n\nSe usarmos a biblioteca factoextra podemos definir o número de clusters que queremos visualizar.\n\nfviz_dend(hc, k=4, horiz=TRUE)\n\n\n\n\n\n\n\nPodemos extrair as atribuições de cluster cortando o dendrograma em 4 partes e adicionando a identidade aos dados.\n\nclusters &lt;- cutree(hc, k = 4)\ncluster_completo &lt;- ruspini_norm %&gt;%\n  add_column(cluster = factor(clusters))\ncluster_completo\n\n# A tibble: 75 × 3\n        x      y cluster\n    &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;  \n 1 -0.455  1.19  1      \n 2  0.692 -1.34  2      \n 3 -0.783 -0.658 3      \n 4  0.365 -1.52  2      \n 5 -0.914 -0.411 3      \n 6  0.496 -1.81  2      \n 7  0.168  0.903 1      \n 8 -0.881 -0.658 3      \n 9  0.201 -1.38  2      \n10  0.922 -1.46  2      \n# ℹ 65 more rows\n\n\nPodemos usar o método de Ward para obter o cluster.\n\nhc_w &lt;- hclust(d, method = \"ward.D\")\n\nO HCA retorna um dendrograma e não uma definição de clusters.\n\nplot(hc_w)\n\n\n\n\n\n\n\nSe usarmos a biblioteca factoextra podemos definir o número de clusters que queremos visualizar.\n\nfviz_dend(hc_w, k=4, horiz=TRUE)\n\n\n\n\n\n\n\n\nfviz_cluster(list(data = ruspini_norm, cluster = cutree(hc_w, k = 4)), geom = \"point\")",
    "crumbs": [
      "Conteúdo Semanal",
      "Análise de Clusters"
    ]
  },
  {
    "objectID": "semanas/Aula13.html#validação-dos-clusters",
    "href": "semanas/Aula13.html#validação-dos-clusters",
    "title": "Análise de Clusters",
    "section": "Validação dos Clusters",
    "text": "Validação dos Clusters\nSilhouette\n\n#library(cluster)\nplot(silhouette(kmed$clustering,d))\n\n\n\n\n\n\n\n\nfviz_silhouette(silhouette(kmed$clustering, d))\n\n  cluster size ave.sil.width\n1       1   23          0.75\n2       2   15          0.81\n3       3   20          0.72\n4       4   17          0.68",
    "crumbs": [
      "Conteúdo Semanal",
      "Análise de Clusters"
    ]
  },
  {
    "objectID": "semanas/Aula13.html#numero-ótimo-de-clusters",
    "href": "semanas/Aula13.html#numero-ótimo-de-clusters",
    "title": "Análise de Clusters",
    "section": "Numero ótimo de clusters",
    "text": "Numero ótimo de clusters\n\n## Usando o silhouette\nfviz_nbclust(ruspini_norm, pam, method =\"silhouette\", k.max = 8)\n\n\n\n\n\n\n## Metodo do cotovelo\nfviz_nbclust(ruspini_norm, kmeans, method =\"wss\", k.max = 8)",
    "crumbs": [
      "Conteúdo Semanal",
      "Análise de Clusters"
    ]
  },
  {
    "objectID": "semanas/Aula12.3.html",
    "href": "semanas/Aula12.3.html",
    "title": "Arvores de Classificação - XGboost",
    "section": "",
    "text": "library(ISLR)\nlibrary(xgboost)\nlibrary(dplyr)\nlibrary(caret)\nlibrary(pROC)",
    "crumbs": [
      "Conteúdo Semanal",
      "Arvore de Classificação - XGBooost"
    ]
  },
  {
    "objectID": "semanas/Aula12.3.html#bibliotecas",
    "href": "semanas/Aula12.3.html#bibliotecas",
    "title": "Arvores de Classificação - XGboost",
    "section": "",
    "text": "library(ISLR)\nlibrary(xgboost)\nlibrary(dplyr)\nlibrary(caret)\nlibrary(pROC)",
    "crumbs": [
      "Conteúdo Semanal",
      "Arvore de Classificação - XGBooost"
    ]
  },
  {
    "objectID": "semanas/Aula12.3.html#dados",
    "href": "semanas/Aula12.3.html#dados",
    "title": "Arvores de Classificação - XGboost",
    "section": "Dados",
    "text": "Dados\nVamos começar a aplicar a metodologia de árvores usando árvores de classificação para analisar os dados existentes em Carseats. Este conjunto de dados (simulado) é sobre venda de assentos de criança para carros. Ele tem 400 observações das seguintes variáveis (11), cujos nomes serão convertidos para o português:\nSales: vendas em unidades (em mil) em cada local\nCompPrice: preço cobrado pelo competidor em cada local\nIncome: nível de renda da comunidade local (em mil US$)\nAdvertising: orçamento local de propaganda (em mil US$)\nPopulation: população na região (em mil)\nPrice: preço cobrado pela empresa em cada local\nShelveLoc: um fator com níveis Ruim, Bom e Medio indicando a qualidade da localização das prateleiras para os assentos em cada lugar\nAge: idade media da população local\nEducation: nível de educação em cada local\nUrban: um fator Sim e Não indicando se a loja esta em uma área urbana ou rural\nUS: um fator indicando se a loja é nos EUA ou não\nNeste dados, Sales é a variável resposta, só que ela é uma variável contínua, por este motivo vamos usá-la para criar uma variável binária. Vamos usar a função ifelse() para criar a variável binária, que chamaremos de alta, ela assume os valores Sim se Sales for maior que 8 e assume o valor Não caso contrário:\n\ndata(Carseats)\nsummary(Carseats)\n\n     Sales          CompPrice       Income        Advertising    \n Min.   : 0.000   Min.   : 77   Min.   : 21.00   Min.   : 0.000  \n 1st Qu.: 5.390   1st Qu.:115   1st Qu.: 42.75   1st Qu.: 0.000  \n Median : 7.490   Median :125   Median : 69.00   Median : 5.000  \n Mean   : 7.496   Mean   :125   Mean   : 68.66   Mean   : 6.635  \n 3rd Qu.: 9.320   3rd Qu.:135   3rd Qu.: 91.00   3rd Qu.:12.000  \n Max.   :16.270   Max.   :175   Max.   :120.00   Max.   :29.000  \n   Population        Price        ShelveLoc        Age          Education   \n Min.   : 10.0   Min.   : 24.0   Bad   : 96   Min.   :25.00   Min.   :10.0  \n 1st Qu.:139.0   1st Qu.:100.0   Good  : 85   1st Qu.:39.75   1st Qu.:12.0  \n Median :272.0   Median :117.0   Medium:219   Median :54.50   Median :14.0  \n Mean   :264.8   Mean   :115.8                Mean   :53.32   Mean   :13.9  \n 3rd Qu.:398.5   3rd Qu.:131.0                3rd Qu.:66.00   3rd Qu.:16.0  \n Max.   :509.0   Max.   :191.0                Max.   :80.00   Max.   :18.0  \n Urban       US     \n No :118   No :142  \n Yes:282   Yes:258  \n                    \n                    \n                    \n                    \n\nstr(Carseats)\n\n'data.frame':   400 obs. of  11 variables:\n $ Sales      : num  9.5 11.22 10.06 7.4 4.15 ...\n $ CompPrice  : num  138 111 113 117 141 124 115 136 132 132 ...\n $ Income     : num  73 48 35 100 64 113 105 81 110 113 ...\n $ Advertising: num  11 16 10 4 3 13 0 15 0 0 ...\n $ Population : num  276 260 269 466 340 501 45 425 108 131 ...\n $ Price      : num  120 83 80 97 128 72 108 120 124 124 ...\n $ ShelveLoc  : Factor w/ 3 levels \"Bad\",\"Good\",\"Medium\": 1 2 3 3 1 1 3 2 3 3 ...\n $ Age        : num  42 65 59 55 38 78 71 67 76 76 ...\n $ Education  : num  17 10 12 14 13 16 15 10 10 17 ...\n $ Urban      : Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 2 2 1 2 2 1 1 ...\n $ US         : Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 2 1 2 1 2 1 2 ...\n\n\n\n# Manipulando os dados\ncad_crianca &lt;- Carseats %&gt;% rename(vendas = Sales, \n                                   preco_comp = CompPrice,\n                                   renda = Income,\n                                   propaganda = Advertising,\n                                   populacao = Population,\n                                   preco = Price,\n                                   local_prat = ShelveLoc,\n                                   idade = Age,\n                                   educacao = Education,\n                                   urbano = Urban,\n                                   eua = US)\n\ncad_crianca &lt;- cad_crianca %&gt;% mutate(vendaAlta = ifelse(vendas &gt; 8, 1, 0)) %&gt;%  select(-vendas)\n\n# Dividir em treino e teste\nset.seed(21)\nindice &lt;- createDataPartition(cad_crianca$vendaAlta, p = 0.7, list = FALSE)\nconj_treino &lt;- cad_crianca[indice, ]\nconj_teste &lt;- cad_crianca[-indice, ]\n\n# Codificação dummy para variáveis categóricas\ndummies &lt;- dummyVars(vendaAlta ~ ., data = conj_treino)\nX_treino &lt;- predict(dummies, newdata = conj_treino)\nX_teste &lt;- predict(dummies, newdata = conj_teste)\n\n# Criar matrizes DMatrix\ndtreino &lt;- xgb.DMatrix(data = X_treino, label = conj_treino$vendaAlta)\ndteste &lt;- xgb.DMatrix(data = X_teste, label = conj_teste$vendaAlta)\n\n\n# Parâmetros do modelo\nparam &lt;- list(\n  objective = \"binary:logistic\",\n  eval_metric = \"error\",\n  max_depth = 4,\n  eta = 0.1\n)\n\n# Treinamento\nmodel &lt;- xgb.train(\n  params = param,\n  data = dtreino,\n  nrounds = 100,\n  watchlist = list(train = dtreino, test = dteste),\n  verbose = 0\n)\n\n\n# Previsões\npred_prob &lt;- predict(model, dteste)\npred_class &lt;- ifelse(pred_prob &gt; 0.5, 1, 0)\n\n# Matriz de confusão\nconf_matrix &lt;- confusionMatrix(factor(pred_class), factor(conj_teste$vendaAlta))\nconf_matrix\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  0  1\n         0 56 14\n         1 12 38\n                                          \n               Accuracy : 0.7833          \n                 95% CI : (0.6989, 0.8533)\n    No Information Rate : 0.5667          \n    P-Value [Acc &gt; NIR] : 5.495e-07       \n                                          \n                  Kappa : 0.5568          \n                                          \n Mcnemar's Test P-Value : 0.8445          \n                                          \n            Sensitivity : 0.8235          \n            Specificity : 0.7308          \n         Pos Pred Value : 0.8000          \n         Neg Pred Value : 0.7600          \n             Prevalence : 0.5667          \n         Detection Rate : 0.4667          \n   Detection Prevalence : 0.5833          \n      Balanced Accuracy : 0.7771          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\n\nroc_obj &lt;- roc(conj_teste$vendaAlta, pred_prob)\nplot(roc_obj, main = \"Curva ROC - XGBoost\", col = \"blue\")\n\n\n\n\n\n\nauc(roc_obj)\n\nArea under the curve: 0.8767\n\n\n\nimportance &lt;- xgb.importance(model = model)\nxgb.plot.importance(importance_matrix = importance)",
    "crumbs": [
      "Conteúdo Semanal",
      "Arvore de Classificação - XGBooost"
    ]
  },
  {
    "objectID": "semanas/Aula12.1.html",
    "href": "semanas/Aula12.1.html",
    "title": "Arvores de Classificação - Única",
    "section": "",
    "text": "library(tidyverse)\nlibrary(ISLR)",
    "crumbs": [
      "Conteúdo Semanal",
      "Arvore de Classificação - Única"
    ]
  },
  {
    "objectID": "semanas/Aula12.1.html#bibliotecas",
    "href": "semanas/Aula12.1.html#bibliotecas",
    "title": "Arvores de Classificação - Única",
    "section": "",
    "text": "library(tidyverse)\nlibrary(ISLR)",
    "crumbs": [
      "Conteúdo Semanal",
      "Arvore de Classificação - Única"
    ]
  },
  {
    "objectID": "semanas/Aula12.1.html#dados",
    "href": "semanas/Aula12.1.html#dados",
    "title": "Arvores de Classificação - Única",
    "section": "Dados",
    "text": "Dados\nVamos começar a aplicar a metodologia de árvores usando árvores de classificação para analisar os dados existentes em Carseats. Este conjunto de dados (simulado) é sobre venda de assentos de criança para carros. Ele tem 400 observações das seguintes variáveis (11), cujos nomes serão convertidos para o português:\nSales: vendas em unidades (em mil) em cada local\nCompPrice: preço cobrado pelo competidor em cada local\nIncome: nível de renda da comunidade local (em mil US$)\nAdvertising: orçamento local de propaganda (em mil US$)\nPopulation: população na região (em mil)\nPrice: preço cobrado pela empresa em cada local\nShelveLoc: um fator com níveis Ruim, Bom e Medio indicando a qualidade da localização das prateleiras para os assentos em cada lugar\nAge: idade media da população local\nEducation: nível de educação em cada local\nUrban: um fator Sim e Não indicando se a loja esta em uma área urbana ou rural\nUS: um fator indicando se a loja é nos EUA ou não\nNeste dados, Sales é a variável resposta, só que ela é uma variável contínua, por este motivo vamos usá-la para criar uma variável binária. Vamos usar a função ifelse() para criar a variável binária, que chamaremos de alta, ela assume os valores Sim se Sales for maior que 8 e assume o valor Não caso contrário:\n\ndata(Carseats)\nsummary(Carseats)\n\n     Sales          CompPrice       Income        Advertising    \n Min.   : 0.000   Min.   : 77   Min.   : 21.00   Min.   : 0.000  \n 1st Qu.: 5.390   1st Qu.:115   1st Qu.: 42.75   1st Qu.: 0.000  \n Median : 7.490   Median :125   Median : 69.00   Median : 5.000  \n Mean   : 7.496   Mean   :125   Mean   : 68.66   Mean   : 6.635  \n 3rd Qu.: 9.320   3rd Qu.:135   3rd Qu.: 91.00   3rd Qu.:12.000  \n Max.   :16.270   Max.   :175   Max.   :120.00   Max.   :29.000  \n   Population        Price        ShelveLoc        Age          Education   \n Min.   : 10.0   Min.   : 24.0   Bad   : 96   Min.   :25.00   Min.   :10.0  \n 1st Qu.:139.0   1st Qu.:100.0   Good  : 85   1st Qu.:39.75   1st Qu.:12.0  \n Median :272.0   Median :117.0   Medium:219   Median :54.50   Median :14.0  \n Mean   :264.8   Mean   :115.8                Mean   :53.32   Mean   :13.9  \n 3rd Qu.:398.5   3rd Qu.:131.0                3rd Qu.:66.00   3rd Qu.:16.0  \n Max.   :509.0   Max.   :191.0                Max.   :80.00   Max.   :18.0  \n Urban       US     \n No :118   No :142  \n Yes:282   Yes:258  \n                    \n                    \n                    \n                    \n\nstr(Carseats)\n\n'data.frame':   400 obs. of  11 variables:\n $ Sales      : num  9.5 11.22 10.06 7.4 4.15 ...\n $ CompPrice  : num  138 111 113 117 141 124 115 136 132 132 ...\n $ Income     : num  73 48 35 100 64 113 105 81 110 113 ...\n $ Advertising: num  11 16 10 4 3 13 0 15 0 0 ...\n $ Population : num  276 260 269 466 340 501 45 425 108 131 ...\n $ Price      : num  120 83 80 97 128 72 108 120 124 124 ...\n $ ShelveLoc  : Factor w/ 3 levels \"Bad\",\"Good\",\"Medium\": 1 2 3 3 1 1 3 2 3 3 ...\n $ Age        : num  42 65 59 55 38 78 71 67 76 76 ...\n $ Education  : num  17 10 12 14 13 16 15 10 10 17 ...\n $ Urban      : Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 2 2 1 2 2 1 1 ...\n $ US         : Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 2 1 2 1 2 1 2 ...",
    "crumbs": [
      "Conteúdo Semanal",
      "Arvore de Classificação - Única"
    ]
  },
  {
    "objectID": "semanas/Aula12.1.html#manipulando-os-dados",
    "href": "semanas/Aula12.1.html#manipulando-os-dados",
    "title": "Arvores de Classificação - Única",
    "section": "Manipulando os dados",
    "text": "Manipulando os dados\n\ncad_crianca &lt;- Carseats %&gt;% rename(vendas = Sales, \n                                   preco_comp = CompPrice,\n                                   renda = Income,\n                                   propaganda = Advertising,\n                                   populacao = Population,\n                                   preco = Price,\n                                   local_prat = ShelveLoc,\n                                   idade = Age,\n                                   educacao = Education,\n                                   urbano = Urban,\n                                   eua = US)\n\ncad_crianca &lt;- cad_crianca %&gt;% mutate(alta = ifelse(vendas &gt; 8, \"Sim\",\n                                                   \"Não\")) %&gt;%\n                              mutate(alta = factor(alta))\n\ncad_crianca&lt;- cad_crianca %&gt;% mutate(local_prat =  case_when(\n                                      local_prat == \"Bad\"  ~ \"Ruim\",\n                                      local_prat == \"Good\" ~ \"Bom\",\n                                      local_prat == \"Medium\" ~ \"Medio\"))%&gt;%                               mutate(local_prat = factor(local_prat))\n\ncad_crianca&lt;- cad_crianca %&gt;% mutate(urbano =  case_when(\n                                      urbano == \"Yes\"  ~ \"Sim\",\n                                      urbano == \"No\" ~ \"Não\")) %&gt;%                                       mutate(urbano = factor(urbano))\n\ncad_crianca&lt;- cad_crianca %&gt;% mutate(eua =  case_when(\n                                      eua == \"Yes\"  ~ \"Sim\",\n                                      eua == \"No\" ~ \"Não\")) %&gt;%                                          mutate(eua = factor(eua))\n\ncad_crianca&lt;- cad_crianca %&gt;% select(-vendas)\n\nstr(cad_crianca)\n\n'data.frame':   400 obs. of  11 variables:\n $ preco_comp: num  138 111 113 117 141 124 115 136 132 132 ...\n $ renda     : num  73 48 35 100 64 113 105 81 110 113 ...\n $ propaganda: num  11 16 10 4 3 13 0 15 0 0 ...\n $ populacao : num  276 260 269 466 340 501 45 425 108 131 ...\n $ preco     : num  120 83 80 97 128 72 108 120 124 124 ...\n $ local_prat: Factor w/ 3 levels \"Bom\",\"Medio\",..: 3 1 2 2 3 3 2 1 2 2 ...\n $ idade     : num  42 65 59 55 38 78 71 67 76 76 ...\n $ educacao  : num  17 10 12 14 13 16 15 10 10 17 ...\n $ urbano    : Factor w/ 2 levels \"Não\",\"Sim\": 2 2 2 2 2 1 2 2 1 1 ...\n $ eua       : Factor w/ 2 levels \"Não\",\"Sim\": 2 2 2 2 1 2 1 2 1 2 ...\n $ alta      : Factor w/ 2 levels \"Não\",\"Sim\": 2 2 2 1 1 2 1 2 1 1 ...\n\nsummary(cad_crianca)\n\n   preco_comp      renda          propaganda       populacao    \n Min.   : 77   Min.   : 21.00   Min.   : 0.000   Min.   : 10.0  \n 1st Qu.:115   1st Qu.: 42.75   1st Qu.: 0.000   1st Qu.:139.0  \n Median :125   Median : 69.00   Median : 5.000   Median :272.0  \n Mean   :125   Mean   : 68.66   Mean   : 6.635   Mean   :264.8  \n 3rd Qu.:135   3rd Qu.: 91.00   3rd Qu.:12.000   3rd Qu.:398.5  \n Max.   :175   Max.   :120.00   Max.   :29.000   Max.   :509.0  \n     preco       local_prat      idade          educacao    urbano     eua     \n Min.   : 24.0   Bom  : 85   Min.   :25.00   Min.   :10.0   Não:118   Não:142  \n 1st Qu.:100.0   Medio:219   1st Qu.:39.75   1st Qu.:12.0   Sim:282   Sim:258  \n Median :117.0   Ruim : 96   Median :54.50   Median :14.0                      \n Mean   :115.8               Mean   :53.32   Mean   :13.9                      \n 3rd Qu.:131.0               3rd Qu.:66.00   3rd Qu.:16.0                      \n Max.   :191.0               Max.   :80.00   Max.   :18.0                      \n  alta    \n Não:236  \n Sim:164",
    "crumbs": [
      "Conteúdo Semanal",
      "Arvore de Classificação - Única"
    ]
  },
  {
    "objectID": "semanas/Aula12.1.html#treino-e-teste",
    "href": "semanas/Aula12.1.html#treino-e-teste",
    "title": "Arvores de Classificação - Única",
    "section": "Treino e Teste",
    "text": "Treino e Teste\n\nlibrary(caret)\nset.seed(21)\ny &lt;- cad_crianca$alta\nindice_teste &lt;- createDataPartition(y, times = 1, p = 0.2, list = FALSE)\n\nconj_treino &lt;- cad_crianca %&gt;% slice(-indice_teste)\nconj_teste &lt;- cad_crianca %&gt;% slice(indice_teste)\n\nstr(conj_treino)\n\n'data.frame':   319 obs. of  11 variables:\n $ preco_comp: num  138 111 113 141 124 136 132 121 117 122 ...\n $ renda     : num  73 48 35 64 113 81 110 78 94 35 ...\n $ propaganda: num  11 16 10 3 13 15 0 9 4 2 ...\n $ populacao : num  276 260 269 340 501 425 108 150 503 393 ...\n $ preco     : num  120 83 80 128 72 120 124 100 94 136 ...\n $ local_prat: Factor w/ 3 levels \"Bom\",\"Medio\",..: 3 1 2 3 3 1 2 3 1 2 ...\n $ idade     : num  42 65 59 38 78 67 76 26 50 62 ...\n $ educacao  : num  17 10 12 13 16 10 10 10 13 18 ...\n $ urbano    : Factor w/ 2 levels \"Não\",\"Sim\": 2 2 2 2 1 2 1 1 2 2 ...\n $ eua       : Factor w/ 2 levels \"Não\",\"Sim\": 2 2 2 1 2 2 1 2 2 1 ...\n $ alta      : Factor w/ 2 levels \"Não\",\"Sim\": 2 2 2 1 2 2 1 2 2 1 ...\n\nprop.table(table(conj_treino$alta))\n\n\n      Não       Sim \n0.5893417 0.4106583 \n\nstr(conj_teste)\n\n'data.frame':   81 obs. of  11 variables:\n $ preco_comp: num  117 115 132 115 147 145 114 121 123 103 ...\n $ renda     : num  100 105 113 28 74 119 38 41 42 93 ...\n $ propaganda: num  4 0 0 11 13 16 13 5 11 15 ...\n $ populacao : num  466 45 131 29 251 294 317 412 16 188 ...\n $ preco     : num  97 108 124 86 131 113 128 110 134 103 ...\n $ local_prat: Factor w/ 3 levels \"Bom\",\"Medio\",..: 2 2 2 1 1 3 1 2 2 3 ...\n $ idade     : num  55 71 76 53 52 42 50 54 59 74 ...\n $ educacao  : num  14 15 17 18 10 12 16 10 13 16 ...\n $ urbano    : Factor w/ 2 levels \"Não\",\"Sim\": 2 2 1 2 2 2 2 2 2 2 ...\n $ eua       : Factor w/ 2 levels \"Não\",\"Sim\": 2 1 2 2 2 2 2 2 2 2 ...\n $ alta      : Factor w/ 2 levels \"Não\",\"Sim\": 1 1 1 2 2 2 2 1 1 1 ...\n\nprop.table(table(conj_teste$alta))\n\n\n      Não       Sim \n0.5925926 0.4074074",
    "crumbs": [
      "Conteúdo Semanal",
      "Arvore de Classificação - Única"
    ]
  },
  {
    "objectID": "semanas/Aula12.1.html#arvore-de-classificação",
    "href": "semanas/Aula12.1.html#arvore-de-classificação",
    "title": "Arvores de Classificação - Única",
    "section": "Arvore de Classificação",
    "text": "Arvore de Classificação\nNa biblioteca rpart as arvores de classificação são obtidas usando o método class. Existem alguns controles que podem ser feitos nos parametros da arvore.\nNeste exemplo só definimos o menor conjunto de dados numa partição (minsplit) e o parametro de complexidade cp. Posteriormente vamos ampliar este controle. Um valor de cp muito pequeno ocasiona overfitting e um valor muito grande resulta numa arvore muito pequena (underfitting). Nos dois casos se diminui o desempenho do modelo.\n\n##Usando rpart para desenvolver a arvore  \nlibrary(rpart)\narvcl &lt;- rpart(alta ~ ., \n                data=conj_treino,\n                method=\"class\", #para arvore de classificação\n                control=rpart.control(minsplit=30,cp=0.02))\nplot(arvcl)\ntext(arvcl,pretty=0)",
    "crumbs": [
      "Conteúdo Semanal",
      "Arvore de Classificação - Única"
    ]
  },
  {
    "objectID": "semanas/Aula12.1.html#regras",
    "href": "semanas/Aula12.1.html#regras",
    "title": "Arvores de Classificação - Única",
    "section": "Regras",
    "text": "Regras\n\n# Regras de Decisão\narvcl\n\nn= 319 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n 1) root 319 131 Não (0.58934169 0.41065831)  \n   2) local_prat=Medio,Ruim 249  77 Não (0.69076305 0.30923695)  \n     4) preco&gt;=92 213  51 Não (0.76056338 0.23943662)  \n       8) idade&gt;=49.5 123  15 Não (0.87804878 0.12195122) *\n       9) idade&lt; 49.5 90  36 Não (0.60000000 0.40000000)  \n        18) preco&gt;=124.5 44   6 Não (0.86363636 0.13636364) *\n        19) preco&lt; 124.5 46  16 Sim (0.34782609 0.65217391) *\n     5) preco&lt; 92 36  10 Sim (0.27777778 0.72222222)  \n      10) local_prat=Ruim 13   5 Não (0.61538462 0.38461538) *\n      11) local_prat=Medio 23   2 Sim (0.08695652 0.91304348) *\n   3) local_prat=Bom 70  16 Sim (0.22857143 0.77142857)  \n     6) preco&gt;=142.5 10   3 Não (0.70000000 0.30000000) *\n     7) preco&lt; 142.5 60   9 Sim (0.15000000 0.85000000) *",
    "crumbs": [
      "Conteúdo Semanal",
      "Arvore de Classificação - Única"
    ]
  },
  {
    "objectID": "semanas/Aula12.1.html#desenhando-a-árvore-de-uma-forma-mais-clara",
    "href": "semanas/Aula12.1.html#desenhando-a-árvore-de-uma-forma-mais-clara",
    "title": "Arvores de Classificação - Única",
    "section": "Desenhando a Árvore de uma forma mais clara",
    "text": "Desenhando a Árvore de uma forma mais clara\n\nlibrary(rattle)\nlibrary(rpart.plot)\nlibrary(RColorBrewer)\nfancyRpartPlot(arvcl, caption = NULL)",
    "crumbs": [
      "Conteúdo Semanal",
      "Arvore de Classificação - Única"
    ]
  },
  {
    "objectID": "semanas/Aula12.1.html#previsões",
    "href": "semanas/Aula12.1.html#previsões",
    "title": "Arvores de Classificação - Única",
    "section": "Previsões",
    "text": "Previsões\n\n# Fazendo Previsões\ny_chapeu &lt;- predict(arvcl, newdata = conj_teste, type=\"class\")\n\nconfusionMatrix(y_chapeu, conj_teste$alta, positive=\"Sim\") \n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction Não Sim\n       Não  37  13\n       Sim  11  20\n                                          \n               Accuracy : 0.7037          \n                 95% CI : (0.5919, 0.8001)\n    No Information Rate : 0.5926          \n    P-Value [Acc &gt; NIR] : 0.02573         \n                                          \n                  Kappa : 0.3805          \n                                          \n Mcnemar's Test P-Value : 0.83826         \n                                          \n            Sensitivity : 0.6061          \n            Specificity : 0.7708          \n         Pos Pred Value : 0.6452          \n         Neg Pred Value : 0.7400          \n             Prevalence : 0.4074          \n         Detection Rate : 0.2469          \n   Detection Prevalence : 0.3827          \n      Balanced Accuracy : 0.6884          \n                                          \n       'Positive' Class : Sim",
    "crumbs": [
      "Conteúdo Semanal",
      "Arvore de Classificação - Única"
    ]
  },
  {
    "objectID": "semanas/Aula12.1.html#arvore-de-classificação-no-caret",
    "href": "semanas/Aula12.1.html#arvore-de-classificação-no-caret",
    "title": "Arvores de Classificação - Única",
    "section": "Arvore de Classificação no caret",
    "text": "Arvore de Classificação no caret\n\n##Usando rpart para desenvolver a arvore  \nlibrary(rpart)\nset.seed(21)\n## Otimizamos o valor de cp usando um 10-fold cv\n# O parametro tuneLength diz para o algoritmo escolher diferentes valores para cp\n# O parametro tuneGrid permite decidir que valores cp deve assumir enquanto que o\n# tuneLength somente limita o número default de parametros que se usa.\ntgrid &lt;- expand.grid(cp = seq(0.01,0.10,0.001))\nctrl &lt;- trainControl(method = \"cv\", classProbs=TRUE)\narvclass &lt;- train(alta ~ . , data = conj_treino, method = \"rpart\",\n                 trControl = ctrl,\n                 tuneGrid = tgrid\n                 )\n# Mostra a acurácia vs cp (parametro de complexidade)\nplot(arvclass)\n\n\n\n\n\n\n## Indica o melhor valor de cp\narvclass$bestTune\n\n     cp\n7 0.016",
    "crumbs": [
      "Conteúdo Semanal",
      "Arvore de Classificação - Única"
    ]
  },
  {
    "objectID": "semanas/Aula12.1.html#uma-forma-melhor-de-ver-a-árvore",
    "href": "semanas/Aula12.1.html#uma-forma-melhor-de-ver-a-árvore",
    "title": "Arvores de Classificação - Única",
    "section": "Uma forma melhor de ver a Árvore",
    "text": "Uma forma melhor de ver a Árvore\n\n## melhorando apresentação da árvore\nlibrary(rattle)\nlibrary(rpart.plot)\nlibrary(RColorBrewer)\nfancyRpartPlot(arvclass$finalModel, caption = NULL)",
    "crumbs": [
      "Conteúdo Semanal",
      "Arvore de Classificação - Única"
    ]
  },
  {
    "objectID": "semanas/Aula12.1.html#previsões-1",
    "href": "semanas/Aula12.1.html#previsões-1",
    "title": "Arvores de Classificação - Única",
    "section": "Previsões",
    "text": "Previsões\n\n# Fazendo Previsões\ny_chapeu &lt;- arvclass %&gt;% predict(conj_teste) %&gt;% \n                   factor(levels = levels(conj_teste$alta))\nhead(y_chapeu)\n\n[1] Não Não Não Sim Sim Não\nLevels: Não Sim\n\nconfusionMatrix(y_chapeu, conj_teste$alta, positive=\"Sim\") \n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction Não Sim\n       Não  40  13\n       Sim   8  20\n                                          \n               Accuracy : 0.7407          \n                 95% CI : (0.6314, 0.8318)\n    No Information Rate : 0.5926          \n    P-Value [Acc &gt; NIR] : 0.003896        \n                                          \n                  Kappa : 0.45            \n                                          \n Mcnemar's Test P-Value : 0.382733        \n                                          \n            Sensitivity : 0.6061          \n            Specificity : 0.8333          \n         Pos Pred Value : 0.7143          \n         Neg Pred Value : 0.7547          \n             Prevalence : 0.4074          \n         Detection Rate : 0.2469          \n   Detection Prevalence : 0.3457          \n      Balanced Accuracy : 0.7197          \n                                          \n       'Positive' Class : Sim",
    "crumbs": [
      "Conteúdo Semanal",
      "Arvore de Classificação - Única"
    ]
  },
  {
    "objectID": "semanas/Aula12.1.html#verificando-a-consistencia-dos-resultados",
    "href": "semanas/Aula12.1.html#verificando-a-consistencia-dos-resultados",
    "title": "Arvores de Classificação - Única",
    "section": "Verificando a consistencia dos resultados",
    "text": "Verificando a consistencia dos resultados\n\nset.seed(121)\ny &lt;- cad_crianca$alta\nindice_teste &lt;- createDataPartition(y, times = 1, p = 0.2, list = FALSE)\n\nconj_treino &lt;- cad_crianca %&gt;% slice(-indice_teste)\nconj_teste &lt;- cad_crianca %&gt;% slice(indice_teste)\n\nstr(conj_treino)\n\n'data.frame':   319 obs. of  11 variables:\n $ preco_comp: num  138 113 141 124 115 136 132 121 117 115 ...\n $ renda     : num  73 35 64 113 105 81 113 78 94 28 ...\n $ propaganda: num  11 10 3 13 0 15 0 9 4 11 ...\n $ populacao : num  276 269 340 501 45 425 131 150 503 29 ...\n $ preco     : num  120 80 128 72 108 120 124 100 94 86 ...\n $ local_prat: Factor w/ 3 levels \"Bom\",\"Medio\",..: 3 2 3 3 2 1 2 3 1 1 ...\n $ idade     : num  42 59 38 78 71 67 76 26 50 53 ...\n $ educacao  : num  17 12 13 16 15 10 17 10 13 18 ...\n $ urbano    : Factor w/ 2 levels \"Não\",\"Sim\": 2 2 2 1 2 2 1 1 2 2 ...\n $ eua       : Factor w/ 2 levels \"Não\",\"Sim\": 2 2 1 2 1 2 2 2 2 2 ...\n $ alta      : Factor w/ 2 levels \"Não\",\"Sim\": 2 2 1 2 1 2 1 2 2 2 ...\n\nprop.table(table(conj_treino$alta))\n\n\n      Não       Sim \n0.5893417 0.4106583 \n\nstr(conj_teste)\n\n'data.frame':   81 obs. of  11 variables:\n $ preco_comp: num  111 117 132 122 125 139 103 125 122 121 ...\n $ renda     : num  48 100 110 35 90 32 74 94 76 90 ...\n $ propaganda: num  16 4 0 2 2 0 0 0 0 0 ...\n $ populacao : num  260 466 108 393 367 176 359 447 270 150 ...\n $ preco     : num  83 97 124 136 131 82 97 89 100 108 ...\n $ local_prat: Factor w/ 3 levels \"Bom\",\"Medio\",..: 1 2 2 2 2 1 3 1 1 3 ...\n $ idade     : num  65 55 76 62 35 54 55 30 60 75 ...\n $ educacao  : num  10 14 10 18 18 11 11 12 18 16 ...\n $ urbano    : Factor w/ 2 levels \"Não\",\"Sim\": 2 2 1 2 2 1 2 2 1 2 ...\n $ eua       : Factor w/ 2 levels \"Não\",\"Sim\": 2 2 1 1 2 1 2 1 1 1 ...\n $ alta      : Factor w/ 2 levels \"Não\",\"Sim\": 2 1 1 1 1 2 1 2 2 1 ...\n\nprop.table(table(conj_teste$alta))\n\n\n      Não       Sim \n0.5925926 0.4074074",
    "crumbs": [
      "Conteúdo Semanal",
      "Arvore de Classificação - Única"
    ]
  },
  {
    "objectID": "semanas/Aula12.1.html#obtendo-a-arvore",
    "href": "semanas/Aula12.1.html#obtendo-a-arvore",
    "title": "Arvores de Classificação - Única",
    "section": "Obtendo a arvore",
    "text": "Obtendo a arvore\n\n##Usando rpart para desenvolver a arvore  \nlibrary(rpart)\narvcl &lt;- rpart(alta ~ ., \n                data=conj_treino,\n                method=\"class\", #para arvore de classificação\n                control=rpart.control(minsplit=30,cp=0.02))",
    "crumbs": [
      "Conteúdo Semanal",
      "Arvore de Classificação - Única"
    ]
  },
  {
    "objectID": "semanas/Aula12.1.html#desenhando-a-árvore-de-uma-forma-mais-clara-1",
    "href": "semanas/Aula12.1.html#desenhando-a-árvore-de-uma-forma-mais-clara-1",
    "title": "Arvores de Classificação - Única",
    "section": "Desenhando a Árvore de uma forma mais clara",
    "text": "Desenhando a Árvore de uma forma mais clara\n\nlibrary(rattle)\nlibrary(rpart.plot)\nlibrary(RColorBrewer)\nfancyRpartPlot(arvcl, caption = NULL)",
    "crumbs": [
      "Conteúdo Semanal",
      "Arvore de Classificação - Única"
    ]
  },
  {
    "objectID": "semanas/Aula11.2B.html",
    "href": "semanas/Aula11.2B.html",
    "title": "Arvores de Regressão - Gradient Boosting",
    "section": "",
    "text": "library(MASS)\nlibrary(tidyverse)",
    "crumbs": [
      "Conteúdo Semanal",
      "Arvore de Regressão (GBM)"
    ]
  },
  {
    "objectID": "semanas/Aula11.2B.html#bibliotecas",
    "href": "semanas/Aula11.2B.html#bibliotecas",
    "title": "Arvores de Regressão - Gradient Boosting",
    "section": "",
    "text": "library(MASS)\nlibrary(tidyverse)",
    "crumbs": [
      "Conteúdo Semanal",
      "Arvore de Regressão (GBM)"
    ]
  },
  {
    "objectID": "semanas/Aula11.2B.html#avaliando-selecionando-dados",
    "href": "semanas/Aula11.2B.html#avaliando-selecionando-dados",
    "title": "Arvores de Regressão - Gradient Boosting",
    "section": "Avaliando, selecionando dados",
    "text": "Avaliando, selecionando dados\n\ndata(\"Boston\")\nnames(Boston)\n\n [1] \"crim\"    \"zn\"      \"indus\"   \"chas\"    \"nox\"     \"rm\"      \"age\"    \n [8] \"dis\"     \"rad\"     \"tax\"     \"ptratio\" \"black\"   \"lstat\"   \"medv\"   \n\ndados &lt;- Boston",
    "crumbs": [
      "Conteúdo Semanal",
      "Arvore de Regressão (GBM)"
    ]
  },
  {
    "objectID": "semanas/Aula11.2B.html#treino-e-teste-com-todas-as-variáveis",
    "href": "semanas/Aula11.2B.html#treino-e-teste-com-todas-as-variáveis",
    "title": "Arvores de Regressão - Gradient Boosting",
    "section": "Treino e Teste com todas as variáveis",
    "text": "Treino e Teste com todas as variáveis\n\n## Vamos criar os conjuntos de treino teste e desenvolver a arvore \n## com todas as variáveis.\nlibrary(caret)\nset.seed(21)\nindice &lt;- createDataPartition(dados$medv, times=1, p=0.75, list=FALSE)\nconj_treino &lt;- dados[indice,]\nconj_teste &lt;- dados[-indice,]\nhead(conj_treino)\n\n     crim   zn indus chas   nox    rm  age    dis rad tax ptratio  black lstat\n1 0.00632 18.0  2.31    0 0.538 6.575 65.2 4.0900   1 296    15.3 396.90  4.98\n3 0.02729  0.0  7.07    0 0.469 7.185 61.1 4.9671   2 242    17.8 392.83  4.03\n4 0.03237  0.0  2.18    0 0.458 6.998 45.8 6.0622   3 222    18.7 394.63  2.94\n5 0.06905  0.0  2.18    0 0.458 7.147 54.2 6.0622   3 222    18.7 396.90  5.33\n6 0.02985  0.0  2.18    0 0.458 6.430 58.7 6.0622   3 222    18.7 394.12  5.21\n7 0.08829 12.5  7.87    0 0.524 6.012 66.6 5.5605   5 311    15.2 395.60 12.43\n  medv\n1 24.0\n3 34.7\n4 33.4\n5 36.2\n6 28.7\n7 22.9\n\nhead(conj_teste)\n\n      crim   zn indus chas   nox    rm  age    dis rad tax ptratio  black lstat\n2  0.02731  0.0  7.07    0 0.469 6.421 78.9 4.9671   2 242    17.8 396.90  9.14\n10 0.17004 12.5  7.87    0 0.524 6.004 85.9 6.5921   5 311    15.2 386.71 17.10\n12 0.11747 12.5  7.87    0 0.524 6.009 82.9 6.2267   5 311    15.2 396.90 13.27\n16 0.62739  0.0  8.14    0 0.538 5.834 56.5 4.4986   4 307    21.0 395.62  8.47\n19 0.80271  0.0  8.14    0 0.538 5.456 36.6 3.7965   4 307    21.0 288.99 11.69\n23 1.23247  0.0  8.14    0 0.538 6.142 91.7 3.9769   4 307    21.0 396.90 18.72\n   medv\n2  21.6\n10 18.9\n12 18.9\n16 19.9\n19 20.2\n23 15.2",
    "crumbs": [
      "Conteúdo Semanal",
      "Arvore de Regressão (GBM)"
    ]
  },
  {
    "objectID": "semanas/Aula11.2B.html#parametros-do-gbm",
    "href": "semanas/Aula11.2B.html#parametros-do-gbm",
    "title": "Arvores de Regressão - Gradient Boosting",
    "section": "Parametros do GBM",
    "text": "Parametros do GBM\n\nlibrary(gbm)\nset.seed(21)\n# treinar o modelo GBM\n# gbm.fit &lt;- gbm(formula = medv ~ .,\n#                 distribution = \"gaussian\", #  minimizar erro quadrático\n#                 data = conj_treino,\n#                 n.trees = 10000,  # número de árvores\n#                 interaction.depth = 3,  # profundidade da arvore\n#                 shrinkage = 0.1,   # aprendizado rápido\n#                 cv.folds = 5, # 5 envelopes de validaçõa cruzada\n#                 n.cores = NULL, # \n#                 verbose = FALSE)",
    "crumbs": [
      "Conteúdo Semanal",
      "Arvore de Regressão (GBM)"
    ]
  },
  {
    "objectID": "semanas/Aula11.2B.html#criando-um-grid-para-avaliar-os-parametros-e-os-respectivos-rmses",
    "href": "semanas/Aula11.2B.html#criando-um-grid-para-avaliar-os-parametros-e-os-respectivos-rmses",
    "title": "Arvores de Regressão - Gradient Boosting",
    "section": "Criando um grid para avaliar os parametros e os respectivos RMSEs",
    "text": "Criando um grid para avaliar os parametros e os respectivos RMSEs\n\nhiper_grid &lt;- expand.grid(\n  shrinkage = c(.01, .05, .1),\n  interaction.depth = c(1, 3, 5, 7),\n  n.minobsinnode = c(5, 10, 15),\n  bag.fraction = c(.65, .8, 1),\n  optimal_trees = 0, \n  min_RMSE = 0   \n  )\n\n# numero total de combinações\nnrow(hiper_grid)\n\n[1] 108",
    "crumbs": [
      "Conteúdo Semanal",
      "Arvore de Regressão (GBM)"
    ]
  },
  {
    "objectID": "semanas/Aula11.2B.html#avaliando-o-grid-de-parametros",
    "href": "semanas/Aula11.2B.html#avaliando-o-grid-de-parametros",
    "title": "Arvores de Regressão - Gradient Boosting",
    "section": "Avaliando o grid de parametros",
    "text": "Avaliando o grid de parametros\n\n# Busca no grid \nfor(i in 1:nrow(hiper_grid)) {\n  \n  # \n  set.seed(21)\n  \n  # treina o modelo\n  gbm.tune &lt;- gbm(\n    formula = medv ~ .,\n    distribution = \"gaussian\",\n    data = conj_treino,\n    n.trees = 6000,\n    interaction.depth = hiper_grid$interaction.depth[i],\n    shrinkage = hiper_grid$shrinkage[i],\n    n.minobsinnode = hiper_grid$n.minobsinnode[i],\n    bag.fraction = hiper_grid$bag.fraction[i],\n    train.fraction = .75,\n    n.cores = NULL, \n    verbose = FALSE\n  )\n  \n # adiciona os erros de treino e arvores ao grid\n  hiper_grid$optimal_trees[i] &lt;- which.min(gbm.tune$valid.error)\n  hiper_grid$min_RMSE[i] &lt;- sqrt(min(gbm.tune$valid.error))\n}\n\nhiper_grid %&gt;% dplyr::arrange(min_RMSE) %&gt;% head(10)\n\n   shrinkage interaction.depth n.minobsinnode bag.fraction optimal_trees\n1       0.10                 1             15         0.65          3074\n2       0.10                 1             10         0.65          5262\n3       0.05                 1             15         0.80          4392\n4       0.10                 1             15         0.80          3321\n5       0.05                 1             15         0.65          3074\n6       0.05                 1             10         1.00          4455\n7       0.10                 1             10         1.00          2073\n8       0.10                 1             15         1.00          4124\n9       0.05                 1             15         1.00          5996\n10      0.05                 5             15         0.65           178\n   min_RMSE\n1  4.420519\n2  4.655791\n3  4.667658\n4  4.686840\n5  4.699871\n6  4.775397\n7  4.780322\n8  4.788251\n9  4.818321\n10 4.818439",
    "crumbs": [
      "Conteúdo Semanal",
      "Arvore de Regressão (GBM)"
    ]
  },
  {
    "objectID": "semanas/Aula11.2B.html#modelo-final",
    "href": "semanas/Aula11.2B.html#modelo-final",
    "title": "Arvores de Regressão - Gradient Boosting",
    "section": "Modelo final",
    "text": "Modelo final\n\n# \nset.seed(21)\n\n# treina o modelo GBM\ngbm.fit.final &lt;- gbm(\n  formula = medv ~ .,\n  distribution = \"gaussian\",\n  data = conj_treino,\n  n.trees = 3074,\n  interaction.depth = 1,\n  shrinkage = 0.10,\n  n.minobsinnode = 15,\n  bag.fraction = 0.65, \n  train.fraction = 1,\n  n.cores = NULL, \n  verbose = FALSE\n  )",
    "crumbs": [
      "Conteúdo Semanal",
      "Arvore de Regressão (GBM)"
    ]
  },
  {
    "objectID": "semanas/Aula11.2B.html#variable-importance",
    "href": "semanas/Aula11.2B.html#variable-importance",
    "title": "Arvores de Regressão - Gradient Boosting",
    "section": "Variable importance",
    "text": "Variable importance\n\nsummary(\n  gbm.fit.final, \n  cBars = 13,\n  method = relative.influence, # também pode ser usado permutation.test.gbm\n  las = 2\n  )\n\n\n\n\n\n\n\n            var    rel.inf\nrm           rm 34.2998187\nlstat     lstat 29.4374141\ndis         dis  8.3574770\nnox         nox  5.9614131\ncrim       crim  5.5011677\nchas       chas  4.5023266\nptratio ptratio  3.5157834\nblack     black  3.4315023\nage         age  2.3514968\ntax         tax  1.4830798\nindus     indus  0.6084729\nrad         rad  0.4554056\nzn           zn  0.0946421",
    "crumbs": [
      "Conteúdo Semanal",
      "Arvore de Regressão (GBM)"
    ]
  },
  {
    "objectID": "semanas/Aula11.2B.html#previsão",
    "href": "semanas/Aula11.2B.html#previsão",
    "title": "Arvores de Regressão - Gradient Boosting",
    "section": "Previsão",
    "text": "Previsão\n\n# Fazendo Previsões\nprevisao1 &lt;- predict(gbm.fit.final, \n                     newdata = conj_teste,\n                     n.trees=gbm.fit.final$n.trees)\nhead(previsao1)\n\n[1] 19.73374 18.76262 19.89281 20.76348 18.34094 16.44815\n\n# Calcula os erros de previsão\ncaret::RMSE(previsao1, conj_teste$medv)\n\n[1] 4.189849\n\ncaret::postResample(previsao1, conj_teste$medv)\n\n     RMSE  Rsquared       MAE \n4.1898489 0.8118316 2.7829890",
    "crumbs": [
      "Conteúdo Semanal",
      "Arvore de Regressão (GBM)"
    ]
  },
  {
    "objectID": "semanas/Aula11.2B.html#comparação-com-outro-modelo-regressão-linear",
    "href": "semanas/Aula11.2B.html#comparação-com-outro-modelo-regressão-linear",
    "title": "Arvores de Regressão - Gradient Boosting",
    "section": "Comparação com outro modelo (Regressão Linear)",
    "text": "Comparação com outro modelo (Regressão Linear)\n\nctrl &lt;- trainControl(method = \"cv\", number = 5)\n\nmodel_lm &lt;- train(\n  medv ~ ., data = conj_treino,\n  method = \"lm\",\n  trControl = ctrl\n)\n\npred_lm &lt;- predict(model_lm, newdata = conj_teste)\npostResample(pred_lm, conj_teste$medv)\n\n     RMSE  Rsquared       MAE \n5.9426142 0.6315418 3.9610503",
    "crumbs": [
      "Conteúdo Semanal",
      "Arvore de Regressão (GBM)"
    ]
  },
  {
    "objectID": "semanas/Aula11.2B.html#entendendo-melhor-os-resultados",
    "href": "semanas/Aula11.2B.html#entendendo-melhor-os-resultados",
    "title": "Arvores de Regressão - Gradient Boosting",
    "section": "Entendendo melhor os resultados",
    "text": "Entendendo melhor os resultados\n\nlibrary(lime)\nmodel_type.gbm &lt;- function(x, ...) {\n  return(\"regression\")\n}\n\npredict_model.gbm &lt;- function(x, newdata, ...) {\n  pred &lt;- predict(x, newdata, n.trees = x$n.trees)\n  return(as.data.frame(pred))\n}\n# Algumas observações para avaliar\nobs_pontuais &lt;- conj_teste[1:2, ]\n\n# aplica o LIME\nexplicador &lt;- lime(conj_treino, gbm.fit.final)\nexplicacao &lt;- explain(obs_pontuais, explicador, n_features = 5)\nplot_features(explicacao)",
    "crumbs": [
      "Conteúdo Semanal",
      "Arvore de Regressão (GBM)"
    ]
  },
  {
    "objectID": "semanas/Aula11.2B.html#gráfico-de-dependencia-parcial-partial-dependence-plot",
    "href": "semanas/Aula11.2B.html#gráfico-de-dependencia-parcial-partial-dependence-plot",
    "title": "Arvores de Regressão - Gradient Boosting",
    "section": "Gráfico de Dependencia Parcial (Partial Dependence Plot)",
    "text": "Gráfico de Dependencia Parcial (Partial Dependence Plot)\n\ngraf_rm &lt;- plot(gbm.fit.final, i = \"rm\")\ngraf_lstat &lt;- plot(gbm.fit.final, i = \"lstat\")\ngridExtra::grid.arrange(graf_lstat, graf_rm, ncol = 2)",
    "crumbs": [
      "Conteúdo Semanal",
      "Arvore de Regressão (GBM)"
    ]
  },
  {
    "objectID": "semanas/Aula11.1.html",
    "href": "semanas/Aula11.1.html",
    "title": "Arvores de Regressão",
    "section": "",
    "text": "library(MASS)\nlibrary(tidyverse)\nlibrary(psych)",
    "crumbs": [
      "Conteúdo Semanal",
      "Arvore de Regressão"
    ]
  },
  {
    "objectID": "semanas/Aula11.1.html#bibliotecas",
    "href": "semanas/Aula11.1.html#bibliotecas",
    "title": "Arvores de Regressão",
    "section": "",
    "text": "library(MASS)\nlibrary(tidyverse)\nlibrary(psych)",
    "crumbs": [
      "Conteúdo Semanal",
      "Arvore de Regressão"
    ]
  },
  {
    "objectID": "semanas/Aula11.1.html#carregando-os-dados",
    "href": "semanas/Aula11.1.html#carregando-os-dados",
    "title": "Arvores de Regressão",
    "section": "Carregando os dados",
    "text": "Carregando os dados\n\ndata(\"Boston\")\nnames(Boston)\n\n [1] \"crim\"    \"zn\"      \"indus\"   \"chas\"    \"nox\"     \"rm\"      \"age\"    \n [8] \"dis\"     \"rad\"     \"tax\"     \"ptratio\" \"black\"   \"lstat\"   \"medv\"   \n\ndescribe(Boston)\n\n        vars   n   mean     sd median trimmed    mad    min    max  range  skew\ncrim       1 506   3.61   8.60   0.26    1.68   0.33   0.01  88.98  88.97  5.19\nzn         2 506  11.36  23.32   0.00    5.08   0.00   0.00 100.00 100.00  2.21\nindus      3 506  11.14   6.86   9.69   10.93   9.37   0.46  27.74  27.28  0.29\nchas       4 506   0.07   0.25   0.00    0.00   0.00   0.00   1.00   1.00  3.39\nnox        5 506   0.55   0.12   0.54    0.55   0.13   0.38   0.87   0.49  0.72\nrm         6 506   6.28   0.70   6.21    6.25   0.51   3.56   8.78   5.22  0.40\nage        7 506  68.57  28.15  77.50   71.20  28.98   2.90 100.00  97.10 -0.60\ndis        8 506   3.80   2.11   3.21    3.54   1.91   1.13  12.13  11.00  1.01\nrad        9 506   9.55   8.71   5.00    8.73   2.97   1.00  24.00  23.00  1.00\ntax       10 506 408.24 168.54 330.00  400.04 108.23 187.00 711.00 524.00  0.67\nptratio   11 506  18.46   2.16  19.05   18.66   1.70  12.60  22.00   9.40 -0.80\nblack     12 506 356.67  91.29 391.44  383.17   8.09   0.32 396.90 396.58 -2.87\nlstat     13 506  12.65   7.14  11.36   11.90   7.11   1.73  37.97  36.24  0.90\nmedv      14 506  22.53   9.20  21.20   21.56   5.93   5.00  50.00  45.00  1.10\n        kurtosis   se\ncrim       36.60 0.38\nzn          3.95 1.04\nindus      -1.24 0.30\nchas        9.48 0.01\nnox        -0.09 0.01\nrm          1.84 0.03\nage        -0.98 1.25\ndis         0.46 0.09\nrad        -0.88 0.39\ntax        -1.15 7.49\nptratio    -0.30 0.10\nblack       7.10 4.06\nlstat       0.46 0.32\nmedv        1.45 0.41",
    "crumbs": [
      "Conteúdo Semanal",
      "Arvore de Regressão"
    ]
  },
  {
    "objectID": "semanas/Aula11.1.html#conhecendo-as-variáveis",
    "href": "semanas/Aula11.1.html#conhecendo-as-variáveis",
    "title": "Arvores de Regressão",
    "section": "Conhecendo as variáveis",
    "text": "Conhecendo as variáveis\n\ncrim: taxa de criminalidade per capita por cidade.\nzn: proporção de terrenos residenciais zoneados para lotes acima de 25.000 pés quadrados.\nindus: proporção de acres de negócios não varejistas por cidade.\nchas: variável fictícia do rio Charles (= 1 se o trato limita o rio; 0 caso contrário). nox concentração de óxidos de nitrogênio (partes por 10 milhões).\nrm: número médio de cômodos por habitação.\nage: proporção de unidades ocupadas pelo proprietário construídas antes de 1940.\ndis: média ponderada das distâncias até cinco centros de emprego de Boston.\nrad: índice de acessibilidade a rodovias radiais. tax taxa de imposto sobre a propriedade do valor total por $ 10.000.\nptratio: proporção aluno-professor por cidade.\nblack: 1000(Bk−0,63)21000(Bk−0,63)2 onde BkBk é a proporção de negros por cidade.\nlstat: status inferior da população (por cento).\nmedv: valor médio das casas ocupadas pelo proprietário em $ 1000s.",
    "crumbs": [
      "Conteúdo Semanal",
      "Arvore de Regressão"
    ]
  },
  {
    "objectID": "semanas/Aula11.1.html#selecionando-os-dados",
    "href": "semanas/Aula11.1.html#selecionando-os-dados",
    "title": "Arvores de Regressão",
    "section": "Selecionando os dados",
    "text": "Selecionando os dados\n\ndados &lt;- Boston \nextrato &lt;- dados %&gt;% select(medv, nox, rm)  \nsummary(extrato)\n\n      medv            nox               rm       \n Min.   : 5.00   Min.   :0.3850   Min.   :3.561  \n 1st Qu.:17.02   1st Qu.:0.4490   1st Qu.:5.886  \n Median :21.20   Median :0.5380   Median :6.208  \n Mean   :22.53   Mean   :0.5547   Mean   :6.285  \n 3rd Qu.:25.00   3rd Qu.:0.6240   3rd Qu.:6.623  \n Max.   :50.00   Max.   :0.8710   Max.   :8.780  \n\nboxplot(extrato$medv)",
    "crumbs": [
      "Conteúdo Semanal",
      "Arvore de Regressão"
    ]
  },
  {
    "objectID": "semanas/Aula11.1.html#visualizando-os-dados",
    "href": "semanas/Aula11.1.html#visualizando-os-dados",
    "title": "Arvores de Regressão",
    "section": "Visualizando os dados",
    "text": "Visualizando os dados\n\n## Distribuição de dados na maior parte simétrica com valores na cauda direta \n## maior do que o esperado para uam distribuição simétrica\nggplot(extrato, aes(x=medv)) +\n  geom_histogram(bins = round(1+3.322*log10(nrow(extrato)),0))\n\n\n\n\n\n\n## Grafico de dispersão nox vs rm \nggplot(extrato, aes(x=rm, y=nox)) + \n  geom_point()",
    "crumbs": [
      "Conteúdo Semanal",
      "Arvore de Regressão"
    ]
  },
  {
    "objectID": "semanas/Aula11.1.html#arvore-de-regressão",
    "href": "semanas/Aula11.1.html#arvore-de-regressão",
    "title": "Arvores de Regressão",
    "section": "Arvore de Regressão",
    "text": "Arvore de Regressão\nNa biblioteca rpart as arvores de regressão são obtidas usando o método anova. Existem alguns controles que podem ser feitos nos parametros da arvore.\nNeste exemplo só definimos o menor conjunto de dados numa partição (minsplit) e o parametro de complexidade cp. Qualquer partição/divisão que não melhore o ajuste por um fator de cp não é tentada. Por exemplo, com a partição pela anova, isso significa que o R-quadrado geral deve aumentar pelo valor de cp a cada etapa. O principal papel deste parâmetro é economizar tempo de computação podando divisões que obviamente não valem a pena. Essencialmente, o usuário informa ao programa que qualquer divisão que não melhore o ajuste pelo cp, provavelmente será podada por validação cruzada, e que, portanto, não é necessário persegui-lo.\n\n##Usando rpart para desenvolver a arvore  \nlibrary(rpart)\narvreg &lt;- rpart(medv ~ ., \n                data=extrato,\n                method=\"anova\", #para arvore de regressão\n                control=rpart.control(minsplit=30,cp=0.06))\nplot(arvreg)\ntext(arvreg,pretty=0)\n\n\n\n\n\n\narvreg\n\nn= 506 \n\nnode), split, n, deviance, yval\n      * denotes terminal node\n\n1) root 506 42716.300 22.53281  \n  2) rm&lt; 6.941 430 17317.320 19.93372  \n    4) nox&gt;=0.6695 97  2214.391 13.73918 *\n    5) nox&lt; 0.6695 333 10296.590 21.73814 *\n  3) rm&gt;=6.941 76  6059.419 37.23816  \n    6) rm&lt; 7.437 46  1899.612 32.11304 *\n    7) rm&gt;=7.437 30  1098.850 45.09667 *",
    "crumbs": [
      "Conteúdo Semanal",
      "Arvore de Regressão"
    ]
  },
  {
    "objectID": "semanas/Aula11.1.html#segmentos",
    "href": "semanas/Aula11.1.html#segmentos",
    "title": "Arvores de Regressão",
    "section": "Segmentos",
    "text": "Segmentos\nA partir da árvore obtida no item anterior podemos fazer uma representação gráfica das partições obtidas.\n\nggplot(extrato, aes(x=rm, y=nox)) + \n  geom_point() +\n  geom_segment(aes(x = 0, y = 0.6695, xend = 6.941, yend = 0.6695), \n               linetype=\"dashed\", color=\"red\", size=1) +\n  geom_vline(xintercept = 6.941, linetype=\"dashed\", color=\"red\", size=1) +\n  geom_vline(xintercept = 7.437, linetype=\"dashed\", color=\"red\", size=1) \n\n\n\n\n\n\n  # scale_y_continuous(limits = c(0.3, 1)) +",
    "crumbs": [
      "Conteúdo Semanal",
      "Arvore de Regressão"
    ]
  },
  {
    "objectID": "semanas/Aula11.1.html#treino-e-teste-com-todas-as-variáveis",
    "href": "semanas/Aula11.1.html#treino-e-teste-com-todas-as-variáveis",
    "title": "Arvores de Regressão",
    "section": "Treino e Teste com todas as variáveis",
    "text": "Treino e Teste com todas as variáveis\nAgora vamos trabalhar com o conjunto completo criando um conjunto de treino e teste.\n\n## Vamos criar os conjuntos de treino teste e desenvolver a arvore \n## com todas as variáveis.\nlibrary(caret)\nset.seed(21)\nindice &lt;- createDataPartition(dados$medv, times=1, p=0.75, list=FALSE)\nconj_treino &lt;- dados[indice,]\nconj_teste &lt;- dados[-indice,]\nhead(conj_treino)\n\n     crim   zn indus chas   nox    rm  age    dis rad tax ptratio  black lstat\n1 0.00632 18.0  2.31    0 0.538 6.575 65.2 4.0900   1 296    15.3 396.90  4.98\n3 0.02729  0.0  7.07    0 0.469 7.185 61.1 4.9671   2 242    17.8 392.83  4.03\n4 0.03237  0.0  2.18    0 0.458 6.998 45.8 6.0622   3 222    18.7 394.63  2.94\n5 0.06905  0.0  2.18    0 0.458 7.147 54.2 6.0622   3 222    18.7 396.90  5.33\n6 0.02985  0.0  2.18    0 0.458 6.430 58.7 6.0622   3 222    18.7 394.12  5.21\n7 0.08829 12.5  7.87    0 0.524 6.012 66.6 5.5605   5 311    15.2 395.60 12.43\n  medv\n1 24.0\n3 34.7\n4 33.4\n5 36.2\n6 28.7\n7 22.9\n\nhead(conj_teste)\n\n      crim   zn indus chas   nox    rm  age    dis rad tax ptratio  black lstat\n2  0.02731  0.0  7.07    0 0.469 6.421 78.9 4.9671   2 242    17.8 396.90  9.14\n10 0.17004 12.5  7.87    0 0.524 6.004 85.9 6.5921   5 311    15.2 386.71 17.10\n12 0.11747 12.5  7.87    0 0.524 6.009 82.9 6.2267   5 311    15.2 396.90 13.27\n16 0.62739  0.0  8.14    0 0.538 5.834 56.5 4.4986   4 307    21.0 395.62  8.47\n19 0.80271  0.0  8.14    0 0.538 5.456 36.6 3.7965   4 307    21.0 288.99 11.69\n23 1.23247  0.0  8.14    0 0.538 6.142 91.7 3.9769   4 307    21.0 396.90 18.72\n   medv\n2  21.6\n10 18.9\n12 18.9\n16 19.9\n19 20.2\n23 15.2",
    "crumbs": [
      "Conteúdo Semanal",
      "Arvore de Regressão"
    ]
  },
  {
    "objectID": "semanas/Aula11.1.html#arvore-de-regressão-treino",
    "href": "semanas/Aula11.1.html#arvore-de-regressão-treino",
    "title": "Arvores de Regressão",
    "section": "Arvore de Regressão Treino",
    "text": "Arvore de Regressão Treino\n\n## A função rpart tem diversos parametros aqui foi configurado um deles\n# cp o parametro de complexidade\n# Um valor de cp muito pequeno ocasiona overfitting e um valor muito grande \n# resulta numa arvore muito pequena (underfitting).\n# Nos dois casos se diminui o desempenho do modelo.\narvreg1 &lt;- rpart(medv ~ ., \n                data=conj_treino,\n                method=\"anova\", #para arvore de regressão\n                control=rpart.control(minsplit=30,cp=0.01))\nplot(arvreg1)\ntext(arvreg1,pretty=0)\n\n\n\n\n\n\narvreg1\n\nn= 381 \n\nnode), split, n, deviance, yval\n      * denotes terminal node\n\n 1) root 381 31196.9300 22.34672  \n   2) rm&lt; 6.8375 311 10862.7700 19.42958  \n     4) lstat&gt;=14.405 131  2579.9460 14.70534  \n       8) crim&gt;=7.006285 53   523.4645 11.23774 *\n       9) crim&lt; 7.006285 78   986.1646 17.06154 *\n     5) lstat&lt; 14.405 180  3231.2930 22.86778  \n      10) rm&lt; 6.5445 145  1867.9540 21.78414 *\n      11) rm&gt;=6.5445 35   487.6657 27.35714 *\n   3) rm&gt;=6.8375 70  5929.5660 35.30714  \n     6) rm&lt; 7.435 49  2010.0620 31.05102  \n      12) lstat&gt;=9.65 10   467.4640 23.84000 *\n      13) lstat&lt; 9.65 39   889.2800 32.90000 *\n     7) rm&gt;=7.435 21   960.7895 45.23810 *",
    "crumbs": [
      "Conteúdo Semanal",
      "Arvore de Regressão"
    ]
  },
  {
    "objectID": "semanas/Aula11.1.html#erros-a-partir-do-conjunto-de-treino",
    "href": "semanas/Aula11.1.html#erros-a-partir-do-conjunto-de-treino",
    "title": "Arvores de Regressão",
    "section": "Erros a partir do conjunto de treino",
    "text": "Erros a partir do conjunto de treino\n\nO erro relativo (Rel error) é obtido através de 1 - R2\nO xerror é obtido através da validação cruzadada (10 fold)\nO xtsd é o desvio padrão dos valores obtidos na validação cruzada.\n\n\n## Mostra 2 gráficos:\n# 1) Variação do R2 aparente e relativo vs número de partições\n# 2) Erro Relativo vs número de partições\nrsq.rpart(arvreg1)\n\n\nRegression tree:\nrpart(formula = medv ~ ., data = conj_treino, method = \"anova\", \n    control = rpart.control(minsplit = 30, cp = 0.01))\n\nVariables actually used in tree construction:\n[1] crim  lstat rm   \n\nRoot node error: 31197/381 = 81.882\n\nn= 381 \n\n        CP nsplit rel error  xerror     xstd\n1 0.461731      0   1.00000 1.01028 0.096645\n2 0.161924      1   0.53827 0.65330 0.065046\n3 0.094840      2   0.37634 0.49315 0.059728\n4 0.034308      3   0.28151 0.37245 0.050769\n5 0.028069      4   0.24720 0.34074 0.051145\n6 0.020942      5   0.21913 0.29254 0.042692\n7 0.010000      6   0.19819 0.28562 0.042529\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Mostra a variação do Erro relativo vs cp(parametro de complexidade)\nplotcp(arvreg1)",
    "crumbs": [
      "Conteúdo Semanal",
      "Arvore de Regressão"
    ]
  },
  {
    "objectID": "semanas/Aula11.1.html#gráfico-de-importancia-das-variáveis",
    "href": "semanas/Aula11.1.html#gráfico-de-importancia-das-variáveis",
    "title": "Arvores de Regressão",
    "section": "Gráfico de importancia das variáveis",
    "text": "Gráfico de importancia das variáveis\nA importancia das variáveis é calculada com base nos resultados das melhores partições\n\n# Gráfico de Importância de variável\nvar_imp &lt;- arvreg1$variable.importance\nnomes_var &lt;- names(var_imp)\nvar_impdf &lt;- data.frame(Importancia=unname(var_imp), Variavel=nomes_var) %&gt;%\n                        arrange(Importancia)\nvar_impdf$Variavel &lt;- factor(var_impdf$Variavel, levels=var_impdf$Variavel)\nggplot(var_impdf, aes(x=Variavel, y=Importancia)) + \n         geom_col() + \n        coord_flip()",
    "crumbs": [
      "Conteúdo Semanal",
      "Arvore de Regressão"
    ]
  },
  {
    "objectID": "semanas/Aula11.1.html#mostrando-a-árvore-e-gerando-previsões",
    "href": "semanas/Aula11.1.html#mostrando-a-árvore-e-gerando-previsões",
    "title": "Arvores de Regressão",
    "section": "Mostrando a árvore e gerando previsões",
    "text": "Mostrando a árvore e gerando previsões\n\n# Mostrando a arvore\npar(xpd = NA)\nplot(arvreg1)\ntext(arvreg1,pretty=0)\n\n\n\n\n\n\n# Fazendo Previsões\nprevisao1 &lt;- arvreg1 %&gt;% predict(conj_teste)\nhead(previsao1)\n\n       2       10       12       16       19       23 \n21.78414 17.06154 21.78414 21.78414 21.78414 17.06154 \n\n# Calcula os erros de previsão\nRMSE(previsao1, conj_teste$medv)\n\n[1] 5.303593",
    "crumbs": [
      "Conteúdo Semanal",
      "Arvore de Regressão"
    ]
  },
  {
    "objectID": "semanas/Aula11.1.html#arvore-de-regressão-com-caret",
    "href": "semanas/Aula11.1.html#arvore-de-regressão-com-caret",
    "title": "Arvores de Regressão",
    "section": "Arvore de Regressão com caret",
    "text": "Arvore de Regressão com caret\nAqui vamos usar a biblioteca caret que tem umas facilidades para otimização do cp e apresentação dos resultados\n\nset.seed(21)\n## Otimizamos o valor de cp usando um 10-fold cv\n# O parametro tuneLength diz para o algoritmo escolher diferentes valores para cp\n# O parametro tuneGrid permite decidir que valores cp deve assumir enquanto que o\n# tuneLength somente limita o número default de parametros que se usa.\narvreg2 &lt;- train(medv ~ . , data = conj_treino, method = \"rpart\",\n                 trControl = trainControl(\"cv\", number = 10),\n                 tuneGrid = data.frame(cp = seq(0.01,0.10, length.out=100)) \n                 )\n# Mostra a acurácia vs cp (parametro de complexidade)\nplot(arvreg2)\n\n\n\n\n\n\n## Indica o melhor valor de cp\narvreg2$bestTune\n\n          cp\n4 0.01272727",
    "crumbs": [
      "Conteúdo Semanal",
      "Arvore de Regressão"
    ]
  },
  {
    "objectID": "semanas/Aula11.1.html#desenhando-a-árvore",
    "href": "semanas/Aula11.1.html#desenhando-a-árvore",
    "title": "Arvores de Regressão",
    "section": "Desenhando a Árvore",
    "text": "Desenhando a Árvore\n\n## Apresenta o modelo final de arvore ajustado\npar(xpd = NA)\nplot(arvreg2$finalModel)\ntext(arvreg2$finalModel,  digits = 3)\n\n\n\n\n\n\n## usando o rpart.plot\nlibrary(rpart.plot)\nrpart.plot(arvreg2$finalModel)",
    "crumbs": [
      "Conteúdo Semanal",
      "Arvore de Regressão"
    ]
  },
  {
    "objectID": "semanas/Aula11.1.html#previsões",
    "href": "semanas/Aula11.1.html#previsões",
    "title": "Arvores de Regressão",
    "section": "Previsões",
    "text": "Previsões\n\n# Regras de Decisão\narvreg2$finalModel\n\nn= 381 \n\nnode), split, n, deviance, yval\n      * denotes terminal node\n\n 1) root 381 31196.9300 22.34672  \n   2) rm&lt; 6.8375 311 10862.7700 19.42958  \n     4) lstat&gt;=14.405 131  2579.9460 14.70534  \n       8) crim&gt;=7.006285 53   523.4645 11.23774 *\n       9) crim&lt; 7.006285 78   986.1646 17.06154 *\n     5) lstat&lt; 14.405 180  3231.2930 22.86778  \n      10) rm&lt; 6.5445 145  1867.9540 21.78414 *\n      11) rm&gt;=6.5445 35   487.6657 27.35714 *\n   3) rm&gt;=6.8375 70  5929.5660 35.30714  \n     6) rm&lt; 7.435 49  2010.0620 31.05102  \n      12) lstat&gt;=11.315 7   367.8886 21.88571 *\n      13) lstat&lt; 11.315 42   956.1507 32.57857 *\n     7) rm&gt;=7.435 21   960.7895 45.23810 *\n\n# Fazendo Previsões\nprevisao2 &lt;- arvreg2 %&gt;% predict(conj_teste)\nhead(previsao2)\n\n       2       10       12       16       19       23 \n21.78414 17.06154 21.78414 21.78414 21.78414 17.06154 \n\n# Calcula os erros de previsão\nRMSE(previsao2, conj_teste$medv)\n\n[1] 5.304604",
    "crumbs": [
      "Conteúdo Semanal",
      "Arvore de Regressão"
    ]
  },
  {
    "objectID": "semanas/Aula11.1.html#vamos-comparar-com-regressão-multipla",
    "href": "semanas/Aula11.1.html#vamos-comparar-com-regressão-multipla",
    "title": "Arvores de Regressão",
    "section": "Vamos comparar com Regressão Multipla",
    "text": "Vamos comparar com Regressão Multipla\n\nlibrary(leaps)\n## Cria uma função de predição para o leaps\npredict.regsubsets &lt;- function(object,newdata,id,...){\n  form &lt;- as.formula(object$call[[2]])\n  mat &lt;- model.matrix(form,newdata)\n  coefi &lt;- coef(object,id=id)\n  mat[,names(coefi)]%*%coefi\n}\nset.seed(21)\nenvelopes &lt;- sample(rep(1:5,length=nrow(conj_treino)))\ntable(envelopes)\n\nenvelopes\n 1  2  3  4  5 \n77 76 76 76 76 \n\nerro_cv &lt;- matrix(NA,5,13)\nfor(k in 1:5){\n  melh_ajus &lt;- regsubsets(medv ~ ., data=conj_treino[envelopes!=k,], \n                          nvmax=13,method=\"forward\")\n  for(i in 1:13){\n    prev &lt;- predict(melh_ajus, conj_treino[envelopes==k,],id=i)\n    erro_cv[k,i] &lt;- mean( (conj_treino$medv[envelopes==k]-prev)^2)\n  }\n}\nrmse_cv &lt;- sqrt(apply(erro_cv,2,mean))  # Erro medio quadratico de cada modelo\nplot(rmse_cv,pch=19,type=\"b\")",
    "crumbs": [
      "Conteúdo Semanal",
      "Arvore de Regressão"
    ]
  },
  {
    "objectID": "semanas/Aula11.1.html#obtem-a-fórmula-do-modelo",
    "href": "semanas/Aula11.1.html#obtem-a-fórmula-do-modelo",
    "title": "Arvores de Regressão",
    "section": "Obtem a fórmula do modelo",
    "text": "Obtem a fórmula do modelo\n\ncoef(melh_ajus, 11)\n\n  (Intercept)          crim            zn          chas           nox \n 37.051974686  -0.144753575   0.047778839   1.780110617 -14.931943579 \n           rm           dis           rad           tax       ptratio \n  3.815637016  -1.500993904   0.349882023  -0.015845438  -0.986230946 \n        black         lstat \n  0.009024705  -0.534163142",
    "crumbs": [
      "Conteúdo Semanal",
      "Arvore de Regressão"
    ]
  },
  {
    "objectID": "semanas/Aula11.1.html#teste-com-o-conjunto-de-teste",
    "href": "semanas/Aula11.1.html#teste-com-o-conjunto-de-teste",
    "title": "Arvores de Regressão",
    "section": "Teste com o conjunto de teste",
    "text": "Teste com o conjunto de teste\n\nprevisao3 &lt;- predict(melh_ajus, conj_teste, 11) \nRMSE(previsao3, conj_teste$medv)\n\n[1] 5.936577",
    "crumbs": [
      "Conteúdo Semanal",
      "Arvore de Regressão"
    ]
  },
  {
    "objectID": "semanas/Aula11.1.html#e-se-usarmos-outra-semente",
    "href": "semanas/Aula11.1.html#e-se-usarmos-outra-semente",
    "title": "Arvores de Regressão",
    "section": "E se usarmos outra semente?",
    "text": "E se usarmos outra semente?\n\n## Vamos criar os conjuntos de treino teste e desenvolver a arvore \n## com todas as variáveis.\nlibrary(caret)\nset.seed(23)\nindice &lt;- createDataPartition(dados$medv, times=1, p=0.75, list=FALSE)\nconj_treino &lt;- dados[indice,]\n\narvreg1s &lt;- rpart(medv ~ ., \n                data=conj_treino,\n                method=\"anova\", #para arvore de regressão\n                control=rpart.control(minsplit=30,cp=0.01))\nplot(arvreg1s)\ntext(arvreg1s,pretty=0)\n\n\n\n\n\n\narvreg1s\n\nn= 381 \n\nnode), split, n, deviance, yval\n      * denotes terminal node\n\n 1) root 381 31257.8400 22.41496  \n   2) rm&lt; 6.92 321 11786.7500 19.74361  \n     4) lstat&gt;=14.4 133  2558.5280 14.88722  \n       8) crim&gt;=6.99237 59   866.3051 11.96949 *\n       9) crim&lt; 6.99237 74   789.4865 17.21351 *\n     5) lstat&lt; 14.4 188  3872.3890 23.17926  \n      10) lstat&gt;=5.51 165  2166.8920 22.28970  \n        20) lstat&gt;=9.675 86   527.0414 20.76977 *\n        21) lstat&lt; 9.675 79  1224.8950 23.94430 *\n      11) lstat&lt; 5.51 23   638.2548 29.56087 *\n   3) rm&gt;=6.92 60  4925.2370 36.70667  \n     6) rm&lt; 7.437 39  1510.0630 31.43077  \n      12) ptratio&gt;=18.55 10   750.6440 25.84000 *\n      13) ptratio&lt; 18.55 29   339.0703 33.35862 *\n     7) rm&gt;=7.437 21   313.5495 46.50476 *\n\n\nEsta é a principal fragilidade da árvore (única), qualquer mudança na amostra pode trazer uma configuração diferente.",
    "crumbs": [
      "Conteúdo Semanal",
      "Arvore de Regressão"
    ]
  },
  {
    "objectID": "semanas/Aula09A.html",
    "href": "semanas/Aula09A.html",
    "title": "Regressão Logística - SMOTE",
    "section": "",
    "text": "library(tidyverse)\nlibrary(ISLR)\ndata(Default)\nsummary(Default)\n\n default    student       balance           income     \n No :9667   No :7056   Min.   :   0.0   Min.   :  772  \n Yes: 333   Yes:2944   1st Qu.: 481.7   1st Qu.:21340  \n                       Median : 823.6   Median :34553  \n                       Mean   : 835.4   Mean   :33517  \n                       3rd Qu.:1166.3   3rd Qu.:43808  \n                       Max.   :2654.3   Max.   :73554  \n\nstr(Default)\n\n'data.frame':   10000 obs. of  4 variables:\n $ default: Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n $ student: Factor w/ 2 levels \"No\",\"Yes\": 1 2 1 1 1 2 1 2 1 1 ...\n $ balance: num  730 817 1074 529 786 ...\n $ income : num  44362 12106 31767 35704 38463 ...\n\nhead(Default)\n\n  default student   balance    income\n1      No      No  729.5265 44361.625\n2      No     Yes  817.1804 12106.135\n3      No      No 1073.5492 31767.139\n4      No      No  529.2506 35704.494\n5      No      No  785.6559 38463.496\n6      No     Yes  919.5885  7491.559",
    "crumbs": [
      "Conteúdo Semanal",
      "SMOTE"
    ]
  },
  {
    "objectID": "semanas/Aula09A.html#carregando-bibliotecas",
    "href": "semanas/Aula09A.html#carregando-bibliotecas",
    "title": "Regressão Logística - SMOTE",
    "section": "",
    "text": "library(tidyverse)\nlibrary(ISLR)\ndata(Default)\nsummary(Default)\n\n default    student       balance           income     \n No :9667   No :7056   Min.   :   0.0   Min.   :  772  \n Yes: 333   Yes:2944   1st Qu.: 481.7   1st Qu.:21340  \n                       Median : 823.6   Median :34553  \n                       Mean   : 835.4   Mean   :33517  \n                       3rd Qu.:1166.3   3rd Qu.:43808  \n                       Max.   :2654.3   Max.   :73554  \n\nstr(Default)\n\n'data.frame':   10000 obs. of  4 variables:\n $ default: Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n $ student: Factor w/ 2 levels \"No\",\"Yes\": 1 2 1 1 1 2 1 2 1 1 ...\n $ balance: num  730 817 1074 529 786 ...\n $ income : num  44362 12106 31767 35704 38463 ...\n\nhead(Default)\n\n  default student   balance    income\n1      No      No  729.5265 44361.625\n2      No     Yes  817.1804 12106.135\n3      No      No 1073.5492 31767.139\n4      No      No  529.2506 35704.494\n5      No      No  785.6559 38463.496\n6      No     Yes  919.5885  7491.559",
    "crumbs": [
      "Conteúdo Semanal",
      "SMOTE"
    ]
  },
  {
    "objectID": "semanas/Aula09A.html#manipulando-os-dados",
    "href": "semanas/Aula09A.html#manipulando-os-dados",
    "title": "Regressão Logística - SMOTE",
    "section": "Manipulando os dados",
    "text": "Manipulando os dados\n\ncredito &lt;- tibble(Default)\nsummary(credito)\n\n default    student       balance           income     \n No :9667   No :7056   Min.   :   0.0   Min.   :  772  \n Yes: 333   Yes:2944   1st Qu.: 481.7   1st Qu.:21340  \n                       Median : 823.6   Median :34553  \n                       Mean   : 835.4   Mean   :33517  \n                       3rd Qu.:1166.3   3rd Qu.:43808  \n                       Max.   :2654.3   Max.   :73554  \n\n# renomeando colunas\ncredito &lt;- credito %&gt;% \n                rename( inadimplente = default, estudante = student, balanco = balance,\n                receita = income)\ncredito &lt;- credito %&gt;% mutate( inadimplente =  case_when(\n                           inadimplente == \"No\"  ~ \"Nao\",\n                           inadimplente == \"Yes\" ~ \"Sim\"\n                          )) %&gt;% mutate(inadimplente = factor(inadimplente))\ncredito &lt;- credito %&gt;% mutate( estudante =  case_when(\n                           estudante == \"No\"  ~ 0,\n                           estudante == \"Yes\" ~ 1\n                          ))\n\nstr(credito)\n\ntibble [10,000 × 4] (S3: tbl_df/tbl/data.frame)\n $ inadimplente: Factor w/ 2 levels \"Nao\",\"Sim\": 1 1 1 1 1 1 1 1 1 1 ...\n $ estudante   : num [1:10000] 0 1 0 0 0 1 0 1 0 0 ...\n $ balanco     : num [1:10000] 730 817 1074 529 786 ...\n $ receita     : num [1:10000] 44362 12106 31767 35704 38463 ...\n\nsummary(credito)\n\n inadimplente   estudante         balanco          receita     \n Nao:9667     Min.   :0.0000   Min.   :   0.0   Min.   :  772  \n Sim: 333     1st Qu.:0.0000   1st Qu.: 481.7   1st Qu.:21340  \n              Median :0.0000   Median : 823.6   Median :34553  \n              Mean   :0.2944   Mean   : 835.4   Mean   :33517  \n              3rd Qu.:1.0000   3rd Qu.:1166.3   3rd Qu.:43808  \n              Max.   :1.0000   Max.   :2654.3   Max.   :73554",
    "crumbs": [
      "Conteúdo Semanal",
      "SMOTE"
    ]
  },
  {
    "objectID": "semanas/Aula09A.html#treino-e-teste",
    "href": "semanas/Aula09A.html#treino-e-teste",
    "title": "Regressão Logística - SMOTE",
    "section": "Treino e Teste",
    "text": "Treino e Teste\n\nlibrary(caret)\nset.seed(21)\ny &lt;- credito$inadimplente\nindice_teste &lt;- createDataPartition(y, times = 1, p = 0.2, list = FALSE)\n\nconj_treino &lt;- credito %&gt;% slice(-indice_teste)\nconj_teste &lt;- credito %&gt;% slice(indice_teste)\n\nsummary(conj_treino)\n\n inadimplente   estudante         balanco          receita     \n Nao:7733     Min.   :0.0000   Min.   :   0.0   Min.   :  772  \n Sim: 266     1st Qu.:0.0000   1st Qu.: 481.3   1st Qu.:21339  \n              Median :0.0000   Median : 819.1   Median :34541  \n              Mean   :0.2953   Mean   : 832.7   Mean   :33541  \n              3rd Qu.:1.0000   3rd Qu.:1167.1   3rd Qu.:43840  \n              Max.   :1.0000   Max.   :2654.3   Max.   :73554  \n\nsummary(conj_teste)\n\n inadimplente   estudante         balanco          receita     \n Nao:1934     Min.   :0.0000   Min.   :   0.0   Min.   : 4755  \n Sim:  67     1st Qu.:0.0000   1st Qu.: 483.5   1st Qu.:21371  \n              Median :0.0000   Median : 836.6   Median :34591  \n              Mean   :0.2909   Mean   : 846.2   Mean   :33423  \n              3rd Qu.:1.0000   3rd Qu.:1163.1   3rd Qu.:43646  \n              Max.   :1.0000   Max.   :2461.5   Max.   :71239",
    "crumbs": [
      "Conteúdo Semanal",
      "SMOTE"
    ]
  },
  {
    "objectID": "semanas/Aula09A.html#smote",
    "href": "semanas/Aula09A.html#smote",
    "title": "Regressão Logística - SMOTE",
    "section": "SMOTE",
    "text": "SMOTE\n\nlibrary(smotefamily)\nset.seed(123)\nteste &lt;- SMOTE(conj_treino[,-1], target = conj_treino$inadimplente, K=5)\nconj_treinoS &lt;- teste$data\nconj_treinoS$class &lt;- as.factor(conj_treinoS$class)\nconj_treinoS &lt;- conj_treinoS %&gt;% rename( inadimplente = class)\nprop.table(table(conj_treinoS$inadimplente))\n\n\n     Nao      Sim \n0.500615 0.499385 \n\nsummary(conj_treinoS)\n\n   estudante         balanco          receita      inadimplente\n Min.   :0.0000   Min.   :   0.0   Min.   :  772   Nao:7733    \n 1st Qu.:0.0000   1st Qu.: 795.5   1st Qu.:20166   Sim:7714    \n Median :0.0000   Median :1392.3   Median :33703               \n Mean   :0.3394   Mean   :1272.9   Mean   :32841               \n 3rd Qu.:1.0000   3rd Qu.:1769.2   3rd Qu.:43745               \n Max.   :1.0000   Max.   :2654.3   Max.   :73554",
    "crumbs": [
      "Conteúdo Semanal",
      "SMOTE"
    ]
  },
  {
    "objectID": "semanas/Aula09A.html#a-regressão-logística",
    "href": "semanas/Aula09A.html#a-regressão-logística",
    "title": "Regressão Logística - SMOTE",
    "section": "1a Regressão logística",
    "text": "1a Regressão logística\n\nmod1 &lt;- glm(inadimplente ~ balanco + receita + estudante,data=conj_treinoS,family=binomial)\nsummary(mod1)\n\n\nCall:\nglm(formula = inadimplente ~ balanco + receita + estudante, family = binomial, \n    data = conj_treinoS)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -9.778e+00  2.279e-01 -42.898  &lt; 2e-16 ***\nbalanco      7.115e-03  1.199e-04  59.356  &lt; 2e-16 ***\nreceita      9.409e-06  3.645e-06   2.582  0.00984 ** \nestudante   -6.092e-01  1.071e-01  -5.688 1.28e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 21414.1  on 15446  degrees of freedom\nResidual deviance:  7490.3  on 15443  degrees of freedom\nAIC: 7498.3\n\nNumber of Fisher Scoring iterations: 7\n\ncoef(mod1)\n\n  (Intercept)       balanco       receita     estudante \n-9.778189e+00  7.114940e-03  9.409475e-06 -6.092070e-01 \n\nsummary(mod1)$coef\n\n                 Estimate   Std. Error    z value     Pr(&gt;|z|)\n(Intercept) -9.778189e+00 2.279388e-01 -42.898311 0.000000e+00\nbalanco      7.114940e-03 1.198694e-04  59.355775 0.000000e+00\nreceita      9.409475e-06 3.644856e-06   2.581576 9.835023e-03\nestudante   -6.092070e-01 1.070945e-01  -5.688499 1.281605e-08",
    "crumbs": [
      "Conteúdo Semanal",
      "SMOTE"
    ]
  },
  {
    "objectID": "semanas/Aula09A.html#avaliando-o-modelo-novamente",
    "href": "semanas/Aula09A.html#avaliando-o-modelo-novamente",
    "title": "Regressão Logística - SMOTE",
    "section": "Avaliando o modelo novamente",
    "text": "Avaliando o modelo novamente\n\nprop.table(table(conj_teste$inadimplente))\n\n\n       Nao        Sim \n0.96651674 0.03348326 \n\np_chapeu &lt;- predict(mod1, newdata = conj_teste, type = \"response\")\ny_chapeu &lt;- ifelse(p_chapeu &gt; 0.5, \"Sim\", \"Nao\") %&gt;% factor(levels = levels(conj_teste$inadimplente))\nconfusionMatrix(y_chapeu, conj_teste$inadimplente, positive=\"Sim\") \n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  Nao  Sim\n       Nao 1696   11\n       Sim  238   56\n                                          \n               Accuracy : 0.8756          \n                 95% CI : (0.8603, 0.8897)\n    No Information Rate : 0.9665          \n    P-Value [Acc &gt; NIR] : 1               \n                                          \n                  Kappa : 0.2705          \n                                          \n Mcnemar's Test P-Value : &lt;2e-16          \n                                          \n            Sensitivity : 0.83582         \n            Specificity : 0.87694         \n         Pos Pred Value : 0.19048         \n         Neg Pred Value : 0.99356         \n             Prevalence : 0.03348         \n         Detection Rate : 0.02799         \n   Detection Prevalence : 0.14693         \n      Balanced Accuracy : 0.85638         \n                                          \n       'Positive' Class : Sim",
    "crumbs": [
      "Conteúdo Semanal",
      "SMOTE"
    ]
  },
  {
    "objectID": "semanas/Aula09A.html#curva-roc",
    "href": "semanas/Aula09A.html#curva-roc",
    "title": "Regressão Logística - SMOTE",
    "section": "Curva ROC",
    "text": "Curva ROC\n\nlibrary(pROC)\np_chapeu_log &lt;- predict(mod1, newdata = conj_teste, type = \"response\")\nhead(p_chapeu_log)\n\n           1            2            3            4            5            6 \n1.710555e-01 3.991424e-04 9.094011e-05 9.398313e-02 3.185776e-03 8.710220e-04 \n\nroc_log &lt;- roc(conj_teste$inadimplente ~ p_chapeu_log, plot = TRUE, print.auc=TRUE,\n                 legacy.axes=TRUE) \n\n\n\n\n\n\nas.numeric(roc_log$auc)\n\n[1] 0.941186",
    "crumbs": [
      "Conteúdo Semanal",
      "SMOTE"
    ]
  },
  {
    "objectID": "semanas/Aula08A.html",
    "href": "semanas/Aula08A.html",
    "title": "KNN com Tidymodels",
    "section": "",
    "text": "Vamos ver como seria utilizar o KNN a partir do tidymodels"
  },
  {
    "objectID": "semanas/Aula08A.html#knn",
    "href": "semanas/Aula08A.html#knn",
    "title": "KNN com Tidymodels",
    "section": "",
    "text": "Vamos ver como seria utilizar o KNN a partir do tidymodels"
  },
  {
    "objectID": "semanas/Aula08A.html#carregando-bibliotecas",
    "href": "semanas/Aula08A.html#carregando-bibliotecas",
    "title": "KNN com Tidymodels",
    "section": "Carregando Bibliotecas",
    "text": "Carregando Bibliotecas\n\nlibrary(tidyverse)\nlibrary(ISLR)\ndata(Default)\nsummary(Default)\n\n#&gt;  default    student       balance           income     \n#&gt;  No :9667   No :7056   Min.   :   0.0   Min.   :  772  \n#&gt;  Yes: 333   Yes:2944   1st Qu.: 481.7   1st Qu.:21340  \n#&gt;                        Median : 823.6   Median :34553  \n#&gt;                        Mean   : 835.4   Mean   :33517  \n#&gt;                        3rd Qu.:1166.3   3rd Qu.:43808  \n#&gt;                        Max.   :2654.3   Max.   :73554\n\nstr(Default)\n\n#&gt; 'data.frame':    10000 obs. of  4 variables:\n#&gt;  $ default: Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n#&gt;  $ student: Factor w/ 2 levels \"No\",\"Yes\": 1 2 1 1 1 2 1 2 1 1 ...\n#&gt;  $ balance: num  730 817 1074 529 786 ...\n#&gt;  $ income : num  44362 12106 31767 35704 38463 ...\n\nhead(Default)\n\n#&gt;   default student   balance    income\n#&gt; 1      No      No  729.5265 44361.625\n#&gt; 2      No     Yes  817.1804 12106.135\n#&gt; 3      No      No 1073.5492 31767.139\n#&gt; 4      No      No  529.2506 35704.494\n#&gt; 5      No      No  785.6559 38463.496\n#&gt; 6      No     Yes  919.5885  7491.559"
  },
  {
    "objectID": "semanas/Aula08A.html#manipulando-os-dados",
    "href": "semanas/Aula08A.html#manipulando-os-dados",
    "title": "KNN com Tidymodels",
    "section": "Manipulando os dados",
    "text": "Manipulando os dados\n\ncredito &lt;- tibble(Default)\nsummary(credito)\n\n#&gt;  default    student       balance           income     \n#&gt;  No :9667   No :7056   Min.   :   0.0   Min.   :  772  \n#&gt;  Yes: 333   Yes:2944   1st Qu.: 481.7   1st Qu.:21340  \n#&gt;                        Median : 823.6   Median :34553  \n#&gt;                        Mean   : 835.4   Mean   :33517  \n#&gt;                        3rd Qu.:1166.3   3rd Qu.:43808  \n#&gt;                        Max.   :2654.3   Max.   :73554\n\n# renomeando colunas\ncredito &lt;- credito %&gt;% \n                rename( inadimplente = default, estudante = student, balanco = balance,\n                receita = income)\ncredito &lt;- credito %&gt;% mutate( inadimplente =  case_when(\n                           inadimplente == \"No\"  ~ \"Nao\",\n                           inadimplente == \"Yes\" ~ \"Sim\"\n                          )) %&gt;% mutate(inadimplente = factor(inadimplente))\ncredito &lt;- credito %&gt;% mutate( estudante =  case_when(\n                           estudante == \"No\"  ~ 0,\n                           estudante == \"Yes\" ~ 1\n                          )) \n\nstr(credito)\n\n#&gt; tibble [10,000 × 4] (S3: tbl_df/tbl/data.frame)\n#&gt;  $ inadimplente: Factor w/ 2 levels \"Nao\",\"Sim\": 1 1 1 1 1 1 1 1 1 1 ...\n#&gt;  $ estudante   : num [1:10000] 0 1 0 0 0 1 0 1 0 0 ...\n#&gt;  $ balanco     : num [1:10000] 730 817 1074 529 786 ...\n#&gt;  $ receita     : num [1:10000] 44362 12106 31767 35704 38463 ...\n\nsummary(credito)\n\n#&gt;  inadimplente   estudante         balanco          receita     \n#&gt;  Nao:9667     Min.   :0.0000   Min.   :   0.0   Min.   :  772  \n#&gt;  Sim: 333     1st Qu.:0.0000   1st Qu.: 481.7   1st Qu.:21340  \n#&gt;               Median :0.0000   Median : 823.6   Median :34553  \n#&gt;               Mean   :0.2944   Mean   : 835.4   Mean   :33517  \n#&gt;               3rd Qu.:1.0000   3rd Qu.:1166.3   3rd Qu.:43808  \n#&gt;               Max.   :1.0000   Max.   :2654.3   Max.   :73554"
  },
  {
    "objectID": "semanas/Aula08A.html#knn-1",
    "href": "semanas/Aula08A.html#knn-1",
    "title": "KNN com Tidymodels",
    "section": "KNN",
    "text": "KNN\nO KNN é um algoritmo de classificação que se baseia na distância entre os pontos. Como o KNN usa as distancias entre os pontos ele é afetado pela escala dos dados, portanto, é necessário que os dados sejam normalizados (padronizados) para eliminar este efeito.\nQuando temos diversas variáveis explicativas em diferentes escalas, em geral, elas devem ser transformadas para ter media zero e desvio padrão 1"
  },
  {
    "objectID": "semanas/Aula08A.html#criando-conjuntos-de-treino-e-teste",
    "href": "semanas/Aula08A.html#criando-conjuntos-de-treino-e-teste",
    "title": "KNN com Tidymodels",
    "section": "Criando conjuntos de treino e teste",
    "text": "Criando conjuntos de treino e teste\n\nlibrary(tidymodels)\nset.seed(2024)\ncredito_split &lt;- initial_split(prop = 0.80, strata = inadimplente, data = credito)\n\nconj_treino &lt;- training(credito_split)\nconj_teste &lt;- testing(credito_split)"
  },
  {
    "objectID": "semanas/Aula08A.html#section",
    "href": "semanas/Aula08A.html#section",
    "title": "KNN com Tidymodels",
    "section": "",
    "text": "## Validação Cruzada\nset.seed(2024)\ndf_cv &lt;- vfold_cv(conj_treino, v = 5)\ndf_cv\n\n#&gt; #  5-fold cross-validation \n#&gt; # A tibble: 5 × 2\n#&gt;   splits              id   \n#&gt;   &lt;list&gt;              &lt;chr&gt;\n#&gt; 1 &lt;split [6400/1600]&gt; Fold1\n#&gt; 2 &lt;split [6400/1600]&gt; Fold2\n#&gt; 3 &lt;split [6400/1600]&gt; Fold3\n#&gt; 4 &lt;split [6400/1600]&gt; Fold4\n#&gt; 5 &lt;split [6400/1600]&gt; Fold5\n\n\n\nmod_knn &lt;- nearest_neighbor(neighbors = tune()) %&gt;% \n            set_engine(\"kknn\") %&gt;%\n            set_mode(\"classification\")\n\n\nmod_knn_recipe &lt;- recipe(inadimplente ~ estudante + balanco + receita, data = conj_treino) %&gt;%  step_normalize(all_predictors())\nmod_knn_recipe %&gt;% prep()\nmod_knn_recipe %&gt;% prep() %&gt;% bake(new_data = NULL)\n\n#&gt; # A tibble: 8,000 × 4\n#&gt;    estudante balanco receita inadimplente\n#&gt;        &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;fct&gt;       \n#&gt;  1    -0.649 -0.219    0.819 Nao         \n#&gt;  2    -0.649  0.491   -0.127 Nao         \n#&gt;  3    -0.649 -0.631    0.169 Nao         \n#&gt;  4    -0.649 -0.103    0.376 Nao         \n#&gt;  5     1.54   0.173   -1.95  Nao         \n#&gt;  6    -0.649 -0.0207  -0.642 Nao         \n#&gt;  7     1.54  -0.0554  -1.19  Nao         \n#&gt;  8    -0.649 -1.72    -0.314 Nao         \n#&gt;  9     1.54  -1.72    -0.870 Nao         \n#&gt; 10     1.54   0.794   -1.52  Nao         \n#&gt; # ℹ 7,990 more rows\n\n\n\nmod_knn_workflow &lt;- workflow() %&gt;% \n  add_recipe(mod_knn_recipe) %&gt;% \n  add_model(mod_knn)\nmod_knn_workflow\n\n#&gt; ══ Workflow ════════════════════════════════════════════════════════════════════\n#&gt; Preprocessor: Recipe\n#&gt; Model: nearest_neighbor()\n#&gt; \n#&gt; ── Preprocessor ────────────────────────────────────────────────────────────────\n#&gt; 1 Recipe Step\n#&gt; \n#&gt; • step_normalize()\n#&gt; \n#&gt; ── Model ───────────────────────────────────────────────────────────────────────\n#&gt; K-Nearest Neighbor Model Specification (classification)\n#&gt; \n#&gt; Main Arguments:\n#&gt;   neighbors = tune()\n#&gt; \n#&gt; Computational engine: kknn\n\n\n\nknn_tune_grid &lt;- tibble(neighbors = c(10, 15, 25, 45, 60, 80, 100, 120, 140, 180, 200))\nctrl &lt;- control_resamples(save_pred = TRUE)\nmod_knn_tune &lt;- mod_knn_workflow %&gt;% tune_grid(resamples = df_cv, \n               grid = knn_tune_grid, \n               control = ctrl)\nmod_knn_tune\n\n#&gt; # Tuning results\n#&gt; # 5-fold cross-validation \n#&gt; # A tibble: 5 × 5\n#&gt;   splits              id    .metrics          .notes           .predictions\n#&gt;   &lt;list&gt;              &lt;chr&gt; &lt;list&gt;            &lt;list&gt;           &lt;list&gt;      \n#&gt; 1 &lt;split [6400/1600]&gt; Fold1 &lt;tibble [33 × 5]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n#&gt; 2 &lt;split [6400/1600]&gt; Fold2 &lt;tibble [33 × 5]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n#&gt; 3 &lt;split [6400/1600]&gt; Fold3 &lt;tibble [33 × 5]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n#&gt; 4 &lt;split [6400/1600]&gt; Fold4 &lt;tibble [33 × 5]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n#&gt; 5 &lt;split [6400/1600]&gt; Fold5 &lt;tibble [33 × 5]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;\n\n\n\nmod_knn_tune %&gt;% collect_metrics()\n\n#&gt; # A tibble: 33 × 7\n#&gt;    neighbors .metric     .estimator   mean     n std_err .config              \n#&gt;        &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n#&gt;  1        10 accuracy    binary     0.970      5 0.00250 Preprocessor1_Model01\n#&gt;  2        10 brier_class binary     0.0244     5 0.00155 Preprocessor1_Model01\n#&gt;  3        10 roc_auc     binary     0.849      5 0.0123  Preprocessor1_Model01\n#&gt;  4        15 accuracy    binary     0.972      5 0.00212 Preprocessor1_Model02\n#&gt;  5        15 brier_class binary     0.0232     5 0.00148 Preprocessor1_Model02\n#&gt;  6        15 roc_auc     binary     0.866      5 0.0117  Preprocessor1_Model02\n#&gt;  7        25 accuracy    binary     0.974      5 0.00192 Preprocessor1_Model03\n#&gt;  8        25 brier_class binary     0.0223     5 0.00138 Preprocessor1_Model03\n#&gt;  9        25 roc_auc     binary     0.899      5 0.0126  Preprocessor1_Model03\n#&gt; 10        45 accuracy    binary     0.974      5 0.00166 Preprocessor1_Model04\n#&gt; # ℹ 23 more rows\n\n\n\nmelhor_k &lt;- mod_knn_tune %&gt;% select_best()\nmod_knn_final &lt;- mod_knn_workflow %&gt;% finalize_workflow(melhor_k)\n\n\nresultados_knn &lt;- mod_knn_final %&gt;% fit(data = conj_treino) %&gt;% \n  predict(new_data = conj_teste) %&gt;% bind_cols(conj_teste) %&gt;% \n  metrics(truth = inadimplente, estimate = .pred_class)"
  },
  {
    "objectID": "semanas/Aula08.0.html",
    "href": "semanas/Aula08.0.html",
    "title": "Classificação - Primeiros Passos",
    "section": "",
    "text": "Baseado no livro Introduction to Data Science: Data Analysis and Prediction Algorithms with R, Rafael A. Irizarry. (http://rafalab.dfci.harvard.edu/dsbook-part-2/)\n\n\nlibrary(tidyverse)\nlibrary(caret)\nlibrary(dslabs)\ndata(heights)\n\n\n\nhead(heights)\n\n     sex height\n1   Male     75\n2   Male     70\n3   Male     68\n4   Male     74\n5   Male     61\n6 Female     65\n\nstr(heights)\n\n'data.frame':   1050 obs. of  2 variables:\n $ sex   : Factor w/ 2 levels \"Female\",\"Male\": 2 2 2 2 2 1 1 1 1 2 ...\n $ height: num  75 70 68 74 61 65 66 62 66 67 ...\n\nalturas &lt;- heights %&gt;%\n  # traduz dados para o português e transforma polegada para metro\n  mutate(\n          sex = case_when(\n                           sex == \"Male\" ~ \"Masculino\",\n                           sex == \"Female\" ~ \"Feminino\"\n                          ), \n          altura = height * 2.54 / 100 ) %&gt;%\n  rename( sexo = sex) %&gt;% \n  select(sexo , altura)\nhead(alturas)\n\n       sexo altura\n1 Masculino 1.9050\n2 Masculino 1.7780\n3 Masculino 1.7272\n4 Masculino 1.8796\n5 Masculino 1.5494\n6  Feminino 1.6510\n\nstr(alturas)\n\n'data.frame':   1050 obs. of  2 variables:\n $ sexo  : chr  \"Masculino\" \"Masculino\" \"Masculino\" \"Masculino\" ...\n $ altura: num  1.9 1.78 1.73 1.88 1.55 ...\n\n\n\n\nalturas &lt;- data.frame(sexo = factor(alturas$sexo), altura = alturas$altura)\nstr(alturas)\n\n'data.frame':   1050 obs. of  2 variables:\n $ sexo  : Factor w/ 2 levels \"Feminino\",\"Masculino\": 2 2 2 2 2 1 1 1 1 2 ...\n $ altura: num  1.9 1.78 1.73 1.88 1.55 ...\n\nhead(alturas)\n\n       sexo altura\n1 Masculino 1.9050\n2 Masculino 1.7780\n3 Masculino 1.7272\n4 Masculino 1.8796\n5 Masculino 1.5494\n6  Feminino 1.6510\n\n\n\n\ny &lt;- alturas$sexo\nx &lt;- alturas$altura\n\n\n\nset.seed(1234)\nindice_teste &lt;- createDataPartition(y, times = 1, p = 0.2, list = FALSE)\nconj_teste &lt;- alturas[indice_teste, ]\nconj_treino &lt;- alturas[-indice_teste, ]\nhead(conj_teste)\n\n        sexo altura\n4  Masculino 1.8796\n7   Feminino 1.6764\n9   Feminino 1.6764\n14 Masculino 1.7272\n16 Masculino 1.6764\n25 Masculino 1.8288\n\nstr(conj_teste)\n\n'data.frame':   211 obs. of  2 variables:\n $ sexo  : Factor w/ 2 levels \"Feminino\",\"Masculino\": 2 1 1 2 2 2 2 2 2 2 ...\n $ altura: num  1.88 1.68 1.68 1.73 1.68 ...\n\nsummary(conj_teste)\n\n        sexo         altura     \n Feminino : 48   Min.   :1.366  \n Masculino:163   1st Qu.:1.676  \n                 Median :1.727  \n                 Mean   :1.735  \n                 3rd Qu.:1.803  \n                 Max.   :2.032  \n\nhead(conj_treino)\n\n       sexo altura\n1 Masculino 1.9050\n2 Masculino 1.7780\n3 Masculino 1.7272\n5 Masculino 1.5494\n6  Feminino 1.6510\n8  Feminino 1.5748\n\nstr(conj_treino)\n\n'data.frame':   839 obs. of  2 variables:\n $ sexo  : Factor w/ 2 levels \"Feminino\",\"Masculino\": 2 2 2 2 1 1 2 2 2 2 ...\n $ altura: num  1.9 1.78 1.73 1.55 1.65 ...\n\nsummary(conj_treino)\n\n        sexo         altura     \n Feminino :190   Min.   :1.270  \n Masculino:649   1st Qu.:1.676  \n                 Median :1.750  \n                 Mean   :1.736  \n                 3rd Qu.:1.803  \n                 Max.   :2.100  \n\n\n\n\ny_chapeu &lt;- sample(c(\"Masculino\", \"Feminino\"), length(indice_teste), replace = TRUE) %&gt;% \n  factor(levels = levels(conj_teste$sexo))\n\n\n\nmean(y_chapeu == conj_teste$sexo)\n\n[1] 0.549763\n\n\n\n\nalturas %&gt;% group_by(sexo) %&gt;% summarize(mean(altura), sd(altura))\n\n# A tibble: 2 × 3\n  sexo      `mean(altura)` `sd(altura)`\n  &lt;fct&gt;              &lt;dbl&gt;        &lt;dbl&gt;\n1 Feminino            1.65       0.0955\n2 Masculino           1.76       0.0917\n\nmedia_Homem &lt;- mean(alturas[alturas$sexo==\"Masculino\",]$altura)\ndesv_pad_Homem &lt;- sd(alturas[alturas$sexo==\"Masculino\",]$altura)\nmedia_Homem\n\n[1] 1.760595\n\ndesv_pad_Homem\n\n[1] 0.09172\n\nmedia_Homem - 2*desv_pad_Homem\n\n[1] 1.577155\n\ny_chapeu &lt;- ifelse(x &gt; media_Homem - 2*desv_pad_Homem , \"Masculino\", \"Feminino\") %&gt;% factor(levels = levels(conj_teste$sexo))\nmean(y == y_chapeu)\n\n[1] 0.7933333\n\n\n\n\npontodecorte &lt;- seq(1.54, 1.78, by = 0.01)\nprecisao &lt;- map_dbl(pontodecorte, function(x){\n  y_chapeu &lt;- ifelse(conj_treino$altura &gt; x, \"Masculino\", \"Feminino\") %&gt;% \n    factor(levels = levels(conj_teste$sexo))\n  mean(y_chapeu == conj_treino$sexo)\n})\nmax(precisao)\n\n[1] 0.8307509\n\nmelhor_ponto &lt;- pontodecorte[which.max(precisao)]\nmelhor_ponto\n\n[1] 1.65\n\nplot(pontodecorte, precisao) \nabline(v=melhor_ponto)\n\n\n\n\n\n\ny_chapeu &lt;- ifelse(conj_teste$altura &gt; melhor_ponto, \"Masculino\", \"Feminino\") %&gt;% \n  factor(levels = levels(conj_teste$sexo))\ny_chapeu &lt;- factor(y_chapeu)\nmean(y_chapeu == conj_teste$sexo)\n\n[1] 0.8530806\n\n\n\n\nlibrary(ggplot2)\ndados &lt;- data.frame(pontodecorte, precisao)\nggplot(data=dados, aes(x=pontodecorte, y=precisao)) + geom_point() +\n       geom_vline(xintercept = melhor_ponto, linetype=\"dashed\", color=\"red\")\n\n\n\n\n\n\n\n\n\ntable(previsto = y_chapeu, real = conj_teste$sexo)\n\n           real\nprevisto    Feminino Masculino\n  Feminino        26         9\n  Masculino       22       154\n\nconj_teste %&gt;% \n  mutate(y_chapeu = y_chapeu) %&gt;%\n  group_by(sexo) %&gt;% \n  summarize(precisao = mean(y_chapeu == sexo))\n\n# A tibble: 2 × 2\n  sexo      precisao\n  &lt;fct&gt;        &lt;dbl&gt;\n1 Feminino     0.542\n2 Masculino    0.945\n\nconfusionMatrix(data = y_chapeu, reference = conj_teste$sexo)\n\nConfusion Matrix and Statistics\n\n           Reference\nPrediction  Feminino Masculino\n  Feminino        26         9\n  Masculino       22       154\n                                         \n               Accuracy : 0.8531         \n                 95% CI : (0.798, 0.8979)\n    No Information Rate : 0.7725         \n    P-Value [Acc &gt; NIR] : 0.002359       \n                                         \n                  Kappa : 0.5378         \n                                         \n Mcnemar's Test P-Value : 0.031141       \n                                         \n            Sensitivity : 0.5417         \n            Specificity : 0.9448         \n         Pos Pred Value : 0.7429         \n         Neg Pred Value : 0.8750         \n             Prevalence : 0.2275         \n         Detection Rate : 0.1232         \n   Detection Prevalence : 0.1659         \n      Balanced Accuracy : 0.7432         \n                                         \n       'Positive' Class : Feminino       \n                                         \n\n\n\n\npontodecorte &lt;- seq(1.54, 1.78, by = 0.01)\nF_1 &lt;- map_dbl(pontodecorte, function(x){\n  y_chapeu &lt;- ifelse(conj_treino$altura &gt; x, \"Masculino\", \"Feminino\") %&gt;% \n    factor(levels = levels(conj_teste$sexo))\n  F_meas(data = y_chapeu, reference = factor(conj_treino$sexo))\n})\nmax(F_1)\n\n[1] 0.616092\n\nmelhor_ponto &lt;- pontodecorte[which.max(F_1)]\nmelhor_ponto\n\n[1] 1.68\n\ndados &lt;- data.frame(pontodecorte, F_1)\nggplot(data=dados, aes(x=pontodecorte, y=F_1)) + geom_point() +\n       geom_vline(xintercept = melhor_ponto, linetype=\"dashed\", color=\"red\")\n\n\n\n\n\n\ny_chapeu &lt;- ifelse(conj_teste$altura &gt; melhor_ponto, \"Masculino\", \"Feminino\") %&gt;% \n  factor(levels = levels(conj_teste$sexo))\nconfusionMatrix(data = y_chapeu, reference = conj_teste$sexo)\n\nConfusion Matrix and Statistics\n\n           Reference\nPrediction  Feminino Masculino\n  Feminino        32        26\n  Masculino       16       137\n                                          \n               Accuracy : 0.8009          \n                 95% CI : (0.7406, 0.8526)\n    No Information Rate : 0.7725          \n    P-Value [Acc &gt; NIR] : 0.1839          \n                                          \n                  Kappa : 0.4724          \n                                          \n Mcnemar's Test P-Value : 0.1649          \n                                          \n            Sensitivity : 0.6667          \n            Specificity : 0.8405          \n         Pos Pred Value : 0.5517          \n         Neg Pred Value : 0.8954          \n             Prevalence : 0.2275          \n         Detection Rate : 0.1517          \n   Detection Prevalence : 0.2749          \n      Balanced Accuracy : 0.7536          \n                                          \n       'Positive' Class : Feminino        \n                                          \n\n\n\n\np &lt;- 0.9\ny_chapeu &lt;- sample(c(\"Masculino\", \"Feminino\"), length(indice_teste), \n                   replace = TRUE, prob=c(p, 1-p)) %&gt;% factor(levels = levels(conj_teste$sexo))\nmean(y_chapeu == conj_teste$sexo)\n\n[1] 0.6919431\n\n\n\n\n     ggplot() +\n     geom_abline(intercept = 0, slope = 1.) + \n     labs(x = \"Taxa Positiva Falsa (1 - Especificidade)\", \n          y = \"Taxa Positiva Verdadeira (Sensibilidade)\", \n          title = \"Curva ROC\") +\n     scale_x_continuous(limits = c(0, 1.0)) + \n     scale_y_continuous(limits = c(0, 1.0))\n\n\n\n\n\n\n\n\n\nprobs &lt;- seq(0, 1, length.out = 10)\nchutando &lt;- map_df(probs, function(p){\n     y_chapeu &lt;- \n          sample(c(\"Masculino\", \"Feminino\"), length(indice_teste), replace = TRUE,                            prob=c(p, 1-p)) %&gt;%  factor(levels = levels(conj_teste$sexo))\n      list(metodo = \"Chutando\",\n           TPF = 1 - specificity(y_chapeu, conj_teste$sexo),\n           TPV = sensitivity(y_chapeu, conj_teste$sexo)) \n})\n     chutando %&gt;%\n     ggplot(aes(TPF, TPV)) +\n     geom_line() +\n     geom_point() +\n     geom_abline(intercept = 0, slope = 1.) + \n     labs(x = \"Taxa Positiva Falsa (1 - Especificidade)\", \n          y = \"Taxa Positiva Verdadeira (Sensibilidade)\", \n          title = \"Curva ROC Método Chutando\") +\n     scale_x_continuous(limits = c(0, 1.0)) + \n     scale_y_continuous(limits = c(0, 1.0))\n\n\n\n\n\n\n\n\n\npontodecorte &lt;- seq(1.27 , 1.92, by = 0.01)\naltura_ptdecorte &lt;- map_df(pontodecorte, function(x){\n     y_chapeu &lt;- ifelse(conj_teste$altura &gt; x, \"Masculino\", \"Feminino\") %&gt;% \n                 factor(levels = levels(conj_teste$sexo))\n     list(metodo = \"Corte por Altura\",\n        TPF = 1 - specificity(y_chapeu, conj_teste$sexo),\n        TPV = sensitivity(y_chapeu, conj_teste$sexo)) \n})\nbind_rows(chutando, altura_ptdecorte) %&gt;%\n     ggplot(aes(TPF, TPV, color = metodo)) +\n     geom_line() +\n     geom_point() +\n     geom_abline(intercept = 0, slope = 1.) + \n     labs(x = \"Taxa Positiva Falsa (1 - Especificidade)\", \n          y = \"Taxa Positiva Verdadeira (Sensibilidade)\", \n          title = \"Curva ROC\") +\n     scale_x_continuous(limits = c(0, 1.0)) + \n     scale_y_continuous(limits = c(0, 1.0))\n\n\n\n\n\n\n\n\n\nmap_df(pontodecorte, function(x){\n     y_chapeu &lt;- ifelse(conj_teste$altura &gt; x, \"Masculino\", \"Feminino\") %&gt;% \n                  factor(levels = c(\"Masculino\", \"Feminino\"))\n     list(metodo = \"Corte por Altura\",\n          corte = round(x, 2), \n          TPF = 1 - specificity(y_chapeu, conj_teste$sexo),\n          TPV = sensitivity(y_chapeu, conj_teste$sexo)) \n}) %&gt;%\n     ggplot(aes(TPF, TPV, label = corte)) +\n     geom_line() +\n     geom_point() +\n     geom_text(nudge_y = - 0.05, size = 3, check_overlap = TRUE) +\n     scale_x_continuous(limits = c(0, 1.0)) + \n     scale_y_continuous(limits = c(0, 1.0))\n\n\n\n\n\n\n\n\n\nparams &lt;- conj_treino %&gt;% \n     group_by(sexo) %&gt;% \n     summarize(media = mean(altura), desvpad = sd(altura))\nparams\n\n# A tibble: 2 × 3\n  sexo      media desvpad\n  &lt;fct&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 Feminino   1.65  0.0947\n2 Masculino  1.76  0.0925\n\npi &lt;- conj_treino %&gt;% summarize(pi=mean(sexo==\"Feminino\")) %&gt;% pull(pi)\npi\n\n[1] 0.2264601\n\nx &lt;- conj_teste$altura\n\nf1 &lt;- dnorm(x, params$media[1], params$desvpad[1])\nf0 &lt;- dnorm(x, params$media[2], params$desvpad[2])\n\np_chapeu_bayes &lt;- f1*pi / (f1*pi + f0*(1 - pi))\ny_chapeu_bayes &lt;- ifelse(p_chapeu_bayes &gt; 0.5, \"Feminino\", \"Masculino\")\n\nconfusionMatrix(factor(y_chapeu_bayes), conj_teste$sexo,  positive=\"Feminino\")\n\nConfusion Matrix and Statistics\n\n           Reference\nPrediction  Feminino Masculino\n  Feminino        20         7\n  Masculino       28       156\n                                          \n               Accuracy : 0.8341          \n                 95% CI : (0.7769, 0.8817)\n    No Information Rate : 0.7725          \n    P-Value [Acc &gt; NIR] : 0.0174801       \n                                          \n                  Kappa : 0.4419          \n                                          \n Mcnemar's Test P-Value : 0.0007232       \n                                          \n            Sensitivity : 0.41667         \n            Specificity : 0.95706         \n         Pos Pred Value : 0.74074         \n         Neg Pred Value : 0.84783         \n             Prevalence : 0.22749         \n         Detection Rate : 0.09479         \n   Detection Prevalence : 0.12796         \n      Balanced Accuracy : 0.68686         \n                                          \n       'Positive' Class : Feminino",
    "crumbs": [
      "Conteúdo Semanal",
      "Classificação 1os passos"
    ]
  },
  {
    "objectID": "semanas/Aula08.0.html#carregando-bibliotecas",
    "href": "semanas/Aula08.0.html#carregando-bibliotecas",
    "title": "Classificação - Primeiros Passos",
    "section": "",
    "text": "library(tidyverse)\nlibrary(caret)\nlibrary(dslabs)\ndata(heights)",
    "crumbs": [
      "Conteúdo Semanal",
      "Classificação 1os passos"
    ]
  },
  {
    "objectID": "semanas/Aula08.0.html#traduzindo",
    "href": "semanas/Aula08.0.html#traduzindo",
    "title": "Classificação - Primeiros Passos",
    "section": "",
    "text": "head(heights)\n\n     sex height\n1   Male     75\n2   Male     70\n3   Male     68\n4   Male     74\n5   Male     61\n6 Female     65\n\nstr(heights)\n\n'data.frame':   1050 obs. of  2 variables:\n $ sex   : Factor w/ 2 levels \"Female\",\"Male\": 2 2 2 2 2 1 1 1 1 2 ...\n $ height: num  75 70 68 74 61 65 66 62 66 67 ...\n\nalturas &lt;- heights %&gt;%\n  # traduz dados para o português e transforma polegada para metro\n  mutate(\n          sex = case_when(\n                           sex == \"Male\" ~ \"Masculino\",\n                           sex == \"Female\" ~ \"Feminino\"\n                          ), \n          altura = height * 2.54 / 100 ) %&gt;%\n  rename( sexo = sex) %&gt;% \n  select(sexo , altura)\nhead(alturas)\n\n       sexo altura\n1 Masculino 1.9050\n2 Masculino 1.7780\n3 Masculino 1.7272\n4 Masculino 1.8796\n5 Masculino 1.5494\n6  Feminino 1.6510\n\nstr(alturas)\n\n'data.frame':   1050 obs. of  2 variables:\n $ sexo  : chr  \"Masculino\" \"Masculino\" \"Masculino\" \"Masculino\" ...\n $ altura: num  1.9 1.78 1.73 1.88 1.55 ...",
    "crumbs": [
      "Conteúdo Semanal",
      "Classificação 1os passos"
    ]
  },
  {
    "objectID": "semanas/Aula08.0.html#alterando",
    "href": "semanas/Aula08.0.html#alterando",
    "title": "Classificação - Primeiros Passos",
    "section": "",
    "text": "alturas &lt;- data.frame(sexo = factor(alturas$sexo), altura = alturas$altura)\nstr(alturas)\n\n'data.frame':   1050 obs. of  2 variables:\n $ sexo  : Factor w/ 2 levels \"Feminino\",\"Masculino\": 2 2 2 2 2 1 1 1 1 2 ...\n $ altura: num  1.9 1.78 1.73 1.88 1.55 ...\n\nhead(alturas)\n\n       sexo altura\n1 Masculino 1.9050\n2 Masculino 1.7780\n3 Masculino 1.7272\n4 Masculino 1.8796\n5 Masculino 1.5494\n6  Feminino 1.6510",
    "crumbs": [
      "Conteúdo Semanal",
      "Classificação 1os passos"
    ]
  },
  {
    "objectID": "semanas/Aula08.0.html#definindo-x-e-y",
    "href": "semanas/Aula08.0.html#definindo-x-e-y",
    "title": "Classificação - Primeiros Passos",
    "section": "",
    "text": "y &lt;- alturas$sexo\nx &lt;- alturas$altura",
    "crumbs": [
      "Conteúdo Semanal",
      "Classificação 1os passos"
    ]
  },
  {
    "objectID": "semanas/Aula08.0.html#treino-e-teste",
    "href": "semanas/Aula08.0.html#treino-e-teste",
    "title": "Classificação - Primeiros Passos",
    "section": "",
    "text": "set.seed(1234)\nindice_teste &lt;- createDataPartition(y, times = 1, p = 0.2, list = FALSE)\nconj_teste &lt;- alturas[indice_teste, ]\nconj_treino &lt;- alturas[-indice_teste, ]\nhead(conj_teste)\n\n        sexo altura\n4  Masculino 1.8796\n7   Feminino 1.6764\n9   Feminino 1.6764\n14 Masculino 1.7272\n16 Masculino 1.6764\n25 Masculino 1.8288\n\nstr(conj_teste)\n\n'data.frame':   211 obs. of  2 variables:\n $ sexo  : Factor w/ 2 levels \"Feminino\",\"Masculino\": 2 1 1 2 2 2 2 2 2 2 ...\n $ altura: num  1.88 1.68 1.68 1.73 1.68 ...\n\nsummary(conj_teste)\n\n        sexo         altura     \n Feminino : 48   Min.   :1.366  \n Masculino:163   1st Qu.:1.676  \n                 Median :1.727  \n                 Mean   :1.735  \n                 3rd Qu.:1.803  \n                 Max.   :2.032  \n\nhead(conj_treino)\n\n       sexo altura\n1 Masculino 1.9050\n2 Masculino 1.7780\n3 Masculino 1.7272\n5 Masculino 1.5494\n6  Feminino 1.6510\n8  Feminino 1.5748\n\nstr(conj_treino)\n\n'data.frame':   839 obs. of  2 variables:\n $ sexo  : Factor w/ 2 levels \"Feminino\",\"Masculino\": 2 2 2 2 1 1 2 2 2 2 ...\n $ altura: num  1.9 1.78 1.73 1.55 1.65 ...\n\nsummary(conj_treino)\n\n        sexo         altura     \n Feminino :190   Min.   :1.270  \n Masculino:649   1st Qu.:1.676  \n                 Median :1.750  \n                 Mean   :1.736  \n                 3rd Qu.:1.803  \n                 Max.   :2.100",
    "crumbs": [
      "Conteúdo Semanal",
      "Classificação 1os passos"
    ]
  },
  {
    "objectID": "semanas/Aula08.0.html#chutar-a-resposta",
    "href": "semanas/Aula08.0.html#chutar-a-resposta",
    "title": "Classificação - Primeiros Passos",
    "section": "",
    "text": "y_chapeu &lt;- sample(c(\"Masculino\", \"Feminino\"), length(indice_teste), replace = TRUE) %&gt;% \n  factor(levels = levels(conj_teste$sexo))",
    "crumbs": [
      "Conteúdo Semanal",
      "Classificação 1os passos"
    ]
  },
  {
    "objectID": "semanas/Aula08.0.html#calcula-precisão",
    "href": "semanas/Aula08.0.html#calcula-precisão",
    "title": "Classificação - Primeiros Passos",
    "section": "",
    "text": "mean(y_chapeu == conj_teste$sexo)\n\n[1] 0.549763",
    "crumbs": [
      "Conteúdo Semanal",
      "Classificação 1os passos"
    ]
  },
  {
    "objectID": "semanas/Aula08.0.html#melhorando",
    "href": "semanas/Aula08.0.html#melhorando",
    "title": "Classificação - Primeiros Passos",
    "section": "",
    "text": "alturas %&gt;% group_by(sexo) %&gt;% summarize(mean(altura), sd(altura))\n\n# A tibble: 2 × 3\n  sexo      `mean(altura)` `sd(altura)`\n  &lt;fct&gt;              &lt;dbl&gt;        &lt;dbl&gt;\n1 Feminino            1.65       0.0955\n2 Masculino           1.76       0.0917\n\nmedia_Homem &lt;- mean(alturas[alturas$sexo==\"Masculino\",]$altura)\ndesv_pad_Homem &lt;- sd(alturas[alturas$sexo==\"Masculino\",]$altura)\nmedia_Homem\n\n[1] 1.760595\n\ndesv_pad_Homem\n\n[1] 0.09172\n\nmedia_Homem - 2*desv_pad_Homem\n\n[1] 1.577155\n\ny_chapeu &lt;- ifelse(x &gt; media_Homem - 2*desv_pad_Homem , \"Masculino\", \"Feminino\") %&gt;% factor(levels = levels(conj_teste$sexo))\nmean(y == y_chapeu)\n\n[1] 0.7933333",
    "crumbs": [
      "Conteúdo Semanal",
      "Classificação 1os passos"
    ]
  },
  {
    "objectID": "semanas/Aula08.0.html#qual-o-melhor-ponto-de-corte",
    "href": "semanas/Aula08.0.html#qual-o-melhor-ponto-de-corte",
    "title": "Classificação - Primeiros Passos",
    "section": "",
    "text": "pontodecorte &lt;- seq(1.54, 1.78, by = 0.01)\nprecisao &lt;- map_dbl(pontodecorte, function(x){\n  y_chapeu &lt;- ifelse(conj_treino$altura &gt; x, \"Masculino\", \"Feminino\") %&gt;% \n    factor(levels = levels(conj_teste$sexo))\n  mean(y_chapeu == conj_treino$sexo)\n})\nmax(precisao)\n\n[1] 0.8307509\n\nmelhor_ponto &lt;- pontodecorte[which.max(precisao)]\nmelhor_ponto\n\n[1] 1.65\n\nplot(pontodecorte, precisao) \nabline(v=melhor_ponto)\n\n\n\n\n\n\ny_chapeu &lt;- ifelse(conj_teste$altura &gt; melhor_ponto, \"Masculino\", \"Feminino\") %&gt;% \n  factor(levels = levels(conj_teste$sexo))\ny_chapeu &lt;- factor(y_chapeu)\nmean(y_chapeu == conj_teste$sexo)\n\n[1] 0.8530806",
    "crumbs": [
      "Conteúdo Semanal",
      "Classificação 1os passos"
    ]
  },
  {
    "objectID": "semanas/Aula08.0.html#melhorando-o-gráfico",
    "href": "semanas/Aula08.0.html#melhorando-o-gráfico",
    "title": "Classificação - Primeiros Passos",
    "section": "",
    "text": "library(ggplot2)\ndados &lt;- data.frame(pontodecorte, precisao)\nggplot(data=dados, aes(x=pontodecorte, y=precisao)) + geom_point() +\n       geom_vline(xintercept = melhor_ponto, linetype=\"dashed\", color=\"red\")",
    "crumbs": [
      "Conteúdo Semanal",
      "Classificação 1os passos"
    ]
  },
  {
    "objectID": "semanas/Aula08.0.html#matriz-de-confusão",
    "href": "semanas/Aula08.0.html#matriz-de-confusão",
    "title": "Classificação - Primeiros Passos",
    "section": "",
    "text": "table(previsto = y_chapeu, real = conj_teste$sexo)\n\n           real\nprevisto    Feminino Masculino\n  Feminino        26         9\n  Masculino       22       154\n\nconj_teste %&gt;% \n  mutate(y_chapeu = y_chapeu) %&gt;%\n  group_by(sexo) %&gt;% \n  summarize(precisao = mean(y_chapeu == sexo))\n\n# A tibble: 2 × 2\n  sexo      precisao\n  &lt;fct&gt;        &lt;dbl&gt;\n1 Feminino     0.542\n2 Masculino    0.945\n\nconfusionMatrix(data = y_chapeu, reference = conj_teste$sexo)\n\nConfusion Matrix and Statistics\n\n           Reference\nPrediction  Feminino Masculino\n  Feminino        26         9\n  Masculino       22       154\n                                         \n               Accuracy : 0.8531         \n                 95% CI : (0.798, 0.8979)\n    No Information Rate : 0.7725         \n    P-Value [Acc &gt; NIR] : 0.002359       \n                                         \n                  Kappa : 0.5378         \n                                         \n Mcnemar's Test P-Value : 0.031141       \n                                         \n            Sensitivity : 0.5417         \n            Specificity : 0.9448         \n         Pos Pred Value : 0.7429         \n         Neg Pred Value : 0.8750         \n             Prevalence : 0.2275         \n         Detection Rate : 0.1232         \n   Detection Prevalence : 0.1659         \n      Balanced Accuracy : 0.7432         \n                                         \n       'Positive' Class : Feminino",
    "crumbs": [
      "Conteúdo Semanal",
      "Classificação 1os passos"
    ]
  },
  {
    "objectID": "semanas/Aula08.0.html#maximizando-a-estatística-f1",
    "href": "semanas/Aula08.0.html#maximizando-a-estatística-f1",
    "title": "Classificação - Primeiros Passos",
    "section": "",
    "text": "pontodecorte &lt;- seq(1.54, 1.78, by = 0.01)\nF_1 &lt;- map_dbl(pontodecorte, function(x){\n  y_chapeu &lt;- ifelse(conj_treino$altura &gt; x, \"Masculino\", \"Feminino\") %&gt;% \n    factor(levels = levels(conj_teste$sexo))\n  F_meas(data = y_chapeu, reference = factor(conj_treino$sexo))\n})\nmax(F_1)\n\n[1] 0.616092\n\nmelhor_ponto &lt;- pontodecorte[which.max(F_1)]\nmelhor_ponto\n\n[1] 1.68\n\ndados &lt;- data.frame(pontodecorte, F_1)\nggplot(data=dados, aes(x=pontodecorte, y=F_1)) + geom_point() +\n       geom_vline(xintercept = melhor_ponto, linetype=\"dashed\", color=\"red\")\n\n\n\n\n\n\ny_chapeu &lt;- ifelse(conj_teste$altura &gt; melhor_ponto, \"Masculino\", \"Feminino\") %&gt;% \n  factor(levels = levels(conj_teste$sexo))\nconfusionMatrix(data = y_chapeu, reference = conj_teste$sexo)\n\nConfusion Matrix and Statistics\n\n           Reference\nPrediction  Feminino Masculino\n  Feminino        32        26\n  Masculino       16       137\n                                          \n               Accuracy : 0.8009          \n                 95% CI : (0.7406, 0.8526)\n    No Information Rate : 0.7725          \n    P-Value [Acc &gt; NIR] : 0.1839          \n                                          \n                  Kappa : 0.4724          \n                                          \n Mcnemar's Test P-Value : 0.1649          \n                                          \n            Sensitivity : 0.6667          \n            Specificity : 0.8405          \n         Pos Pred Value : 0.5517          \n         Neg Pred Value : 0.8954          \n             Prevalence : 0.2275          \n         Detection Rate : 0.1517          \n   Detection Prevalence : 0.2749          \n      Balanced Accuracy : 0.7536          \n                                          \n       'Positive' Class : Feminino",
    "crumbs": [
      "Conteúdo Semanal",
      "Classificação 1os passos"
    ]
  },
  {
    "objectID": "semanas/Aula08.0.html#voltando-a-chutar",
    "href": "semanas/Aula08.0.html#voltando-a-chutar",
    "title": "Classificação - Primeiros Passos",
    "section": "",
    "text": "p &lt;- 0.9\ny_chapeu &lt;- sample(c(\"Masculino\", \"Feminino\"), length(indice_teste), \n                   replace = TRUE, prob=c(p, 1-p)) %&gt;% factor(levels = levels(conj_teste$sexo))\nmean(y_chapeu == conj_teste$sexo)\n\n[1] 0.6919431",
    "crumbs": [
      "Conteúdo Semanal",
      "Classificação 1os passos"
    ]
  },
  {
    "objectID": "semanas/Aula08.0.html#curva-roc",
    "href": "semanas/Aula08.0.html#curva-roc",
    "title": "Classificação - Primeiros Passos",
    "section": "",
    "text": "ggplot() +\n     geom_abline(intercept = 0, slope = 1.) + \n     labs(x = \"Taxa Positiva Falsa (1 - Especificidade)\", \n          y = \"Taxa Positiva Verdadeira (Sensibilidade)\", \n          title = \"Curva ROC\") +\n     scale_x_continuous(limits = c(0, 1.0)) + \n     scale_y_continuous(limits = c(0, 1.0))",
    "crumbs": [
      "Conteúdo Semanal",
      "Classificação 1os passos"
    ]
  },
  {
    "objectID": "semanas/Aula08.0.html#testando-probabilidades",
    "href": "semanas/Aula08.0.html#testando-probabilidades",
    "title": "Classificação - Primeiros Passos",
    "section": "",
    "text": "probs &lt;- seq(0, 1, length.out = 10)\nchutando &lt;- map_df(probs, function(p){\n     y_chapeu &lt;- \n          sample(c(\"Masculino\", \"Feminino\"), length(indice_teste), replace = TRUE,                            prob=c(p, 1-p)) %&gt;%  factor(levels = levels(conj_teste$sexo))\n      list(metodo = \"Chutando\",\n           TPF = 1 - specificity(y_chapeu, conj_teste$sexo),\n           TPV = sensitivity(y_chapeu, conj_teste$sexo)) \n})\n     chutando %&gt;%\n     ggplot(aes(TPF, TPV)) +\n     geom_line() +\n     geom_point() +\n     geom_abline(intercept = 0, slope = 1.) + \n     labs(x = \"Taxa Positiva Falsa (1 - Especificidade)\", \n          y = \"Taxa Positiva Verdadeira (Sensibilidade)\", \n          title = \"Curva ROC Método Chutando\") +\n     scale_x_continuous(limits = c(0, 1.0)) + \n     scale_y_continuous(limits = c(0, 1.0))",
    "crumbs": [
      "Conteúdo Semanal",
      "Classificação 1os passos"
    ]
  },
  {
    "objectID": "semanas/Aula08.0.html#comparando-os-dois-métodos",
    "href": "semanas/Aula08.0.html#comparando-os-dois-métodos",
    "title": "Classificação - Primeiros Passos",
    "section": "",
    "text": "pontodecorte &lt;- seq(1.27 , 1.92, by = 0.01)\naltura_ptdecorte &lt;- map_df(pontodecorte, function(x){\n     y_chapeu &lt;- ifelse(conj_teste$altura &gt; x, \"Masculino\", \"Feminino\") %&gt;% \n                 factor(levels = levels(conj_teste$sexo))\n     list(metodo = \"Corte por Altura\",\n        TPF = 1 - specificity(y_chapeu, conj_teste$sexo),\n        TPV = sensitivity(y_chapeu, conj_teste$sexo)) \n})\nbind_rows(chutando, altura_ptdecorte) %&gt;%\n     ggplot(aes(TPF, TPV, color = metodo)) +\n     geom_line() +\n     geom_point() +\n     geom_abline(intercept = 0, slope = 1.) + \n     labs(x = \"Taxa Positiva Falsa (1 - Especificidade)\", \n          y = \"Taxa Positiva Verdadeira (Sensibilidade)\", \n          title = \"Curva ROC\") +\n     scale_x_continuous(limits = c(0, 1.0)) + \n     scale_y_continuous(limits = c(0, 1.0))",
    "crumbs": [
      "Conteúdo Semanal",
      "Classificação 1os passos"
    ]
  },
  {
    "objectID": "semanas/Aula08.0.html#pontos-da-curva-roc",
    "href": "semanas/Aula08.0.html#pontos-da-curva-roc",
    "title": "Classificação - Primeiros Passos",
    "section": "",
    "text": "map_df(pontodecorte, function(x){\n     y_chapeu &lt;- ifelse(conj_teste$altura &gt; x, \"Masculino\", \"Feminino\") %&gt;% \n                  factor(levels = c(\"Masculino\", \"Feminino\"))\n     list(metodo = \"Corte por Altura\",\n          corte = round(x, 2), \n          TPF = 1 - specificity(y_chapeu, conj_teste$sexo),\n          TPV = sensitivity(y_chapeu, conj_teste$sexo)) \n}) %&gt;%\n     ggplot(aes(TPF, TPV, label = corte)) +\n     geom_line() +\n     geom_point() +\n     geom_text(nudge_y = - 0.05, size = 3, check_overlap = TRUE) +\n     scale_x_continuous(limits = c(0, 1.0)) + \n     scale_y_continuous(limits = c(0, 1.0))",
    "crumbs": [
      "Conteúdo Semanal",
      "Classificação 1os passos"
    ]
  },
  {
    "objectID": "semanas/Aula08.0.html#bayes-ingênuo-naive-bayes",
    "href": "semanas/Aula08.0.html#bayes-ingênuo-naive-bayes",
    "title": "Classificação - Primeiros Passos",
    "section": "",
    "text": "params &lt;- conj_treino %&gt;% \n     group_by(sexo) %&gt;% \n     summarize(media = mean(altura), desvpad = sd(altura))\nparams\n\n# A tibble: 2 × 3\n  sexo      media desvpad\n  &lt;fct&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 Feminino   1.65  0.0947\n2 Masculino  1.76  0.0925\n\npi &lt;- conj_treino %&gt;% summarize(pi=mean(sexo==\"Feminino\")) %&gt;% pull(pi)\npi\n\n[1] 0.2264601\n\nx &lt;- conj_teste$altura\n\nf1 &lt;- dnorm(x, params$media[1], params$desvpad[1])\nf0 &lt;- dnorm(x, params$media[2], params$desvpad[2])\n\np_chapeu_bayes &lt;- f1*pi / (f1*pi + f0*(1 - pi))\ny_chapeu_bayes &lt;- ifelse(p_chapeu_bayes &gt; 0.5, \"Feminino\", \"Masculino\")\n\nconfusionMatrix(factor(y_chapeu_bayes), conj_teste$sexo,  positive=\"Feminino\")\n\nConfusion Matrix and Statistics\n\n           Reference\nPrediction  Feminino Masculino\n  Feminino        20         7\n  Masculino       28       156\n                                          \n               Accuracy : 0.8341          \n                 95% CI : (0.7769, 0.8817)\n    No Information Rate : 0.7725          \n    P-Value [Acc &gt; NIR] : 0.0174801       \n                                          \n                  Kappa : 0.4419          \n                                          \n Mcnemar's Test P-Value : 0.0007232       \n                                          \n            Sensitivity : 0.41667         \n            Specificity : 0.95706         \n         Pos Pred Value : 0.74074         \n         Neg Pred Value : 0.84783         \n             Prevalence : 0.22749         \n         Detection Rate : 0.09479         \n   Detection Prevalence : 0.12796         \n      Balanced Accuracy : 0.68686         \n                                          \n       'Positive' Class : Feminino",
    "crumbs": [
      "Conteúdo Semanal",
      "Classificação 1os passos"
    ]
  },
  {
    "objectID": "semanas/Aula07.3.html",
    "href": "semanas/Aula07.3.html",
    "title": "Seleção de Modelos",
    "section": "",
    "text": "library(tidyverse)\nlibrary(leaps)",
    "crumbs": [
      "Conteúdo Semanal",
      "Seleção de Modelos"
    ]
  },
  {
    "objectID": "semanas/Aula07.3.html#carregando-bibliotecas",
    "href": "semanas/Aula07.3.html#carregando-bibliotecas",
    "title": "Seleção de Modelos",
    "section": "",
    "text": "library(tidyverse)\nlibrary(leaps)",
    "crumbs": [
      "Conteúdo Semanal",
      "Seleção de Modelos"
    ]
  },
  {
    "objectID": "semanas/Aula07.3.html#carregando-os-dados",
    "href": "semanas/Aula07.3.html#carregando-os-dados",
    "title": "Seleção de Modelos",
    "section": "Carregando os dados",
    "text": "Carregando os dados\nVendas de casas em Seattle entre 2015 e 2016\n\nvendas_casa &lt;- readRDS(\"home_sales.rds\")\nhead(vendas_casa)\n\n#&gt; # A tibble: 6 × 8\n#&gt;   selling_price home_age bedrooms bathrooms sqft_living sqft_lot sqft_basement\n#&gt;           &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;         &lt;dbl&gt;\n#&gt; 1        487000       10        4      2.5         2540     5001             0\n#&gt; 2        465000       10        3      2.25        1530     1245           480\n#&gt; 3        411000       18        2      2           1130     1148           330\n#&gt; 4        635000        4        3      2.5         3350     4007           800\n#&gt; 5        380000       24        5      2.5         2130     8428             0\n#&gt; 6        495000       21        3      3.5         1650     1577           550\n#&gt; # ℹ 1 more variable: floors &lt;dbl&gt;\n\nvendas_casa &lt;- vendas_casa %&gt;% rename(preco=selling_price,\n                                      idade=home_age,\n                                      quartos=bedrooms,\n                                      banheiros= bathrooms,\n                                      m2_princ=sqft_living,\n                                      m2_tot=sqft_lot,\n                                      m2_porao=sqft_basement,\n                                      andares=floors\n                                      )\nsummary(vendas_casa)\n\n#&gt;      preco            idade          quartos        banheiros    \n#&gt;  Min.   :350000   Min.   : 0.00   Min.   :1.000   Min.   :0.750  \n#&gt;  1st Qu.:410000   1st Qu.:12.00   1st Qu.:3.000   1st Qu.:2.438  \n#&gt;  Median :470000   Median :19.00   Median :3.000   Median :2.500  \n#&gt;  Mean   :479084   Mean   :18.53   Mean   :3.338   Mean   :2.473  \n#&gt;  3rd Qu.:541625   3rd Qu.:25.00   3rd Qu.:4.000   3rd Qu.:2.500  \n#&gt;  Max.   :650000   Max.   :49.00   Max.   :8.000   Max.   :5.000  \n#&gt;     m2_princ        m2_tot          m2_porao         andares     \n#&gt;  Min.   : 550   Min.   :   600   Min.   :   0.0   Min.   :1.000  \n#&gt;  1st Qu.:1640   1st Qu.:  3200   1st Qu.:   0.0   1st Qu.:2.000  \n#&gt;  Median :2060   Median :  5508   Median :   0.0   Median :2.000  \n#&gt;  Mean   :2087   Mean   : 11891   Mean   : 129.7   Mean   :1.865  \n#&gt;  3rd Qu.:2500   3rd Qu.:  8644   3rd Qu.: 122.5   3rd Qu.:2.000  \n#&gt;  Max.   :3880   Max.   :415126   Max.   :1660.0   Max.   :2.000\n\nvendas_casa &lt;- vendas_casa %&gt;% mutate(preco_m=preco/1000) %&gt;% select(-preco)",
    "crumbs": [
      "Conteúdo Semanal",
      "Seleção de Modelos"
    ]
  },
  {
    "objectID": "semanas/Aula07.3.html#análise-exploratória",
    "href": "semanas/Aula07.3.html#análise-exploratória",
    "title": "Seleção de Modelos",
    "section": "Análise Exploratória",
    "text": "Análise Exploratória\n\nlibrary(summarytools)\ndfSummary(vendas_casa) |&gt; stview(method = \"render\")\n\n\nData Frame Summary\nvendas_casa\nDimensions: 1492 x 8\n  Duplicates: 0\n\n\n\n\n\n\n\n\n\n\n\nNo\nVariable\nStats / Values\nFreqs (% of Valid)\nGraph\nValid\nMissing\n\n\n\n1\nidade [numeric]\n\n\nMean (sd) : 18.5 (9)\n\n\nmin ≤ med ≤ max:\n\n\n0 ≤ 19 ≤ 49\n\n\nIQR (CV) : 13 (0.5)\n\n\n45 distinct values\n\n1492 (100.0%)\n0 (0.0%)\n\n\n2\nquartos [numeric]\n\n\nMean (sd) : 3.3 (0.8)\n\n\nmin ≤ med ≤ max:\n\n\n1 ≤ 3 ≤ 8\n\n\nIQR (CV) : 1 (0.2)\n\n\n\n\n1\n:\n3\n(\n0.2%\n)\n\n\n2\n:\n169\n(\n11.3%\n)\n\n\n3\n:\n753\n(\n50.5%\n)\n\n\n4\n:\n464\n(\n31.1%\n)\n\n\n5\n:\n96\n(\n6.4%\n)\n\n\n6\n:\n5\n(\n0.3%\n)\n\n\n7\n:\n1\n(\n0.1%\n)\n\n\n8\n:\n1\n(\n0.1%\n)\n\n\n\n1492 (100.0%)\n0 (0.0%)\n\n\n3\nbanheiros [numeric]\n\n\nMean (sd) : 2.5 (0.4)\n\n\nmin ≤ med ≤ max:\n\n\n0.8 ≤ 2.5 ≤ 5\n\n\nIQR (CV) : 0.1 (0.2)\n\n\n14 distinct values\n\n1492 (100.0%)\n0 (0.0%)\n\n\n4\nm2_princ [numeric]\n\n\nMean (sd) : 2087 (576.2)\n\n\nmin ≤ med ≤ max:\n\n\n550 ≤ 2060 ≤ 3880\n\n\nIQR (CV) : 860.2 (0.3)\n\n\n290 distinct values\n\n1492 (100.0%)\n0 (0.0%)\n\n\n5\nm2_tot [numeric]\n\n\nMean (sd) : 11891.2 (28195)\n\n\nmin ≤ med ≤ max:\n\n\n600 ≤ 5508.5 ≤ 415126\n\n\nIQR (CV) : 5444.5 (2.4)\n\n\n1310 distinct values\n\n1492 (100.0%)\n0 (0.0%)\n\n\n6\nm2_porao [numeric]\n\n\nMean (sd) : 129.7 (267.8)\n\n\nmin ≤ med ≤ max:\n\n\n0 ≤ 0 ≤ 1660\n\n\nIQR (CV) : 122.5 (2.1)\n\n\n121 distinct values\n\n1492 (100.0%)\n0 (0.0%)\n\n\n7\nandares [numeric]\n\n\nMin : 1\n\n\nMean : 1.9\n\n\nMax : 2\n\n\n\n\n1\n:\n201\n(\n13.5%\n)\n\n\n2\n:\n1291\n(\n86.5%\n)\n\n\n\n1492 (100.0%)\n0 (0.0%)\n\n\n8\npreco_m [numeric]\n\n\nMean (sd) : 479.1 (81)\n\n\nmin ≤ med ≤ max:\n\n\n350 ≤ 470 ≤ 650\n\n\nIQR (CV) : 131.6 (0.2)\n\n\n545 distinct values\n\n1492 (100.0%)\n0 (0.0%)\n\n\n\nGenerated by summarytools 1.1.3 (R version 4.5.0)2025-04-28\n\n\nvendas_casa %&gt;% select(where(is.numeric)) %&gt;% \n          summarize(\n            across(\n              everything(),\n              ~ var(., na.rm = TRUE))) %&gt;% \n  pivot_longer(everything(),\n    names_to = \"variavel\", \n    values_to = \"variancia\")\n\n#&gt; # A tibble: 8 × 2\n#&gt;   variavel      variancia\n#&gt;   &lt;chr&gt;             &lt;dbl&gt;\n#&gt; 1 idade            81.1  \n#&gt; 2 quartos           0.633\n#&gt; 3 banheiros         0.171\n#&gt; 4 m2_princ     332045.   \n#&gt; 5 m2_tot    794957702.   \n#&gt; 6 m2_porao      71699.   \n#&gt; 7 andares           0.117\n#&gt; 8 preco_m        6558.",
    "crumbs": [
      "Conteúdo Semanal",
      "Seleção de Modelos"
    ]
  },
  {
    "objectID": "semanas/Aula07.3.html#conjunto-de-teste-e-treino",
    "href": "semanas/Aula07.3.html#conjunto-de-teste-e-treino",
    "title": "Seleção de Modelos",
    "section": "Conjunto de teste e treino",
    "text": "Conjunto de teste e treino\n\nlibrary(caret)\nset.seed(21)\nnrow(vendas_casa)\n\n#&gt; [1] 1492\n\ny &lt;- vendas_casa$preco_m\nindice_teste &lt;- createDataPartition(y, times = 1, p = 0.2, list = FALSE)\n\nconj_treino &lt;- vendas_casa[-indice_teste,]\nconj_teste &lt;- vendas_casa[indice_teste,]\n\nstr(conj_treino)\n\n#&gt; tibble [1,192 × 8] (S3: tbl_df/tbl/data.frame)\n#&gt;  $ idade    : num [1:1192] 10 18 4 21 19 24 3 16 20 29 ...\n#&gt;  $ quartos  : num [1:1192] 3 2 3 3 3 2 4 3 3 3 ...\n#&gt;  $ banheiros: num [1:1192] 2.25 2 2.5 3.5 2.25 1 2.5 2.75 2.75 2.5 ...\n#&gt;  $ m2_princ : num [1:1192] 1530 1130 3350 1650 1430 1430 2140 2100 2930 1960 ...\n#&gt;  $ m2_tot   : num [1:1192] 1245 1148 4007 1577 4777 ...\n#&gt;  $ m2_porao : num [1:1192] 480 330 800 550 0 420 0 590 1070 0 ...\n#&gt;  $ andares  : num [1:1192] 2 2 2 2 2 1 2 2 1 2 ...\n#&gt;  $ preco_m  : num [1:1192] 465 411 635 495 355 ...\n\nstr(conj_teste)\n\n#&gt; tibble [300 × 8] (S3: tbl_df/tbl/data.frame)\n#&gt;  $ idade    : num [1:300] 10 24 19 11 3 28 9 17 9 19 ...\n#&gt;  $ quartos  : num [1:300] 4 5 3 4 4 5 3 3 3 3 ...\n#&gt;  $ banheiros: num [1:300] 2.5 2.5 2 2.5 2.75 3 2.5 3.5 3.5 2.25 ...\n#&gt;  $ m2_princ : num [1:300] 2540 2130 2190 1920 2360 ...\n#&gt;  $ m2_tot   : num [1:300] 5001 8428 19800 9000 15100 ...\n#&gt;  $ m2_porao : num [1:300] 0 0 0 0 0 0 0 770 770 145 ...\n#&gt;  $ andares  : num [1:300] 2 2 1 2 1 2 2 2 2 2 ...\n#&gt;  $ preco_m  : num [1:300] 487 380 465 425 535 ...\n\ngt::gt(head(conj_treino, 10))\n\n\n\n\n\nidade\nquartos\nbanheiros\nm2_princ\nm2_tot\nm2_porao\nandares\npreco_m\n\n\n\n10\n3\n2.25\n1530\n1245\n480\n2\n465.000\n\n\n18\n2\n2.00\n1130\n1148\n330\n2\n411.000\n\n\n4\n3\n2.50\n3350\n4007\n800\n2\n635.000\n\n\n21\n3\n3.50\n1650\n1577\n550\n2\n495.000\n\n\n19\n3\n2.25\n1430\n4777\n0\n2\n355.000\n\n\n24\n2\n1.00\n1430\n365904\n420\n1\n356.000\n\n\n3\n4\n2.50\n2140\n7245\n0\n2\n495.000\n\n\n16\n3\n2.75\n2100\n10362\n590\n2\n525.000\n\n\n20\n3\n2.75\n2930\n5569\n1070\n1\n559.900\n\n\n29\n3\n2.50\n1960\n8469\n0\n2\n552.321",
    "crumbs": [
      "Conteúdo Semanal",
      "Seleção de Modelos"
    ]
  },
  {
    "objectID": "semanas/Aula07.3.html#matriz-de-correlação",
    "href": "semanas/Aula07.3.html#matriz-de-correlação",
    "title": "Seleção de Modelos",
    "section": "Matriz de correlação",
    "text": "Matriz de correlação\n\nlibrary(corrplot)\nmat_corr &lt;- cor(conj_treino)\ncorrplot(mat_corr)",
    "crumbs": [
      "Conteúdo Semanal",
      "Seleção de Modelos"
    ]
  },
  {
    "objectID": "semanas/Aula07.3.html#matriz-de-dispersão",
    "href": "semanas/Aula07.3.html#matriz-de-dispersão",
    "title": "Seleção de Modelos",
    "section": "Matriz de dispersão",
    "text": "Matriz de dispersão\n\nlibrary(psych)\npairs.panels(conj_treino,\n             method = \"pearson\", # metodo de correlação\n             hist.col = \"#00AFBB\",\n             density = TRUE,  # mostra graficos de densidade\n             ellipses = FALSE # mostra elipses de correlação\n             )",
    "crumbs": [
      "Conteúdo Semanal",
      "Seleção de Modelos"
    ]
  },
  {
    "objectID": "semanas/Aula07.3.html#métodos-de-seleção-de-modelo",
    "href": "semanas/Aula07.3.html#métodos-de-seleção-de-modelo",
    "title": "Seleção de Modelos",
    "section": "Métodos de seleção de modelo",
    "text": "Métodos de seleção de modelo\n\n## Best Subset sem definir o número máx de subsets a ser avaliado\najusreg.comp &lt;- regsubsets(preco_m ~ ., data=conj_treino)\nsummary(ajusreg.comp)\n\n#&gt; Subset selection object\n#&gt; Call: regsubsets.formula(preco_m ~ ., data = conj_treino)\n#&gt; 7 Variables  (and intercept)\n#&gt;           Forced in Forced out\n#&gt; idade         FALSE      FALSE\n#&gt; quartos       FALSE      FALSE\n#&gt; banheiros     FALSE      FALSE\n#&gt; m2_princ      FALSE      FALSE\n#&gt; m2_tot        FALSE      FALSE\n#&gt; m2_porao      FALSE      FALSE\n#&gt; andares       FALSE      FALSE\n#&gt; 1 subsets of each size up to 7\n#&gt; Selection Algorithm: exhaustive\n#&gt;          idade quartos banheiros m2_princ m2_tot m2_porao andares\n#&gt; 1  ( 1 ) \" \"   \" \"     \" \"       \"*\"      \" \"    \" \"      \" \"    \n#&gt; 2  ( 1 ) \" \"   \"*\"     \" \"       \"*\"      \" \"    \" \"      \" \"    \n#&gt; 3  ( 1 ) \" \"   \"*\"     \" \"       \"*\"      \" \"    \" \"      \"*\"    \n#&gt; 4  ( 1 ) \"*\"   \"*\"     \" \"       \"*\"      \" \"    \" \"      \"*\"    \n#&gt; 5  ( 1 ) \"*\"   \"*\"     \"*\"       \"*\"      \" \"    \" \"      \"*\"    \n#&gt; 6  ( 1 ) \"*\"   \"*\"     \"*\"       \"*\"      \"*\"    \" \"      \"*\"    \n#&gt; 7  ( 1 ) \"*\"   \"*\"     \"*\"       \"*\"      \"*\"    \"*\"      \"*\"",
    "crumbs": [
      "Conteúdo Semanal",
      "Seleção de Modelos"
    ]
  },
  {
    "objectID": "semanas/Aula07.3.html#nvmax13",
    "href": "semanas/Aula07.3.html#nvmax13",
    "title": "Seleção de Modelos",
    "section": "nvmax=13",
    "text": "nvmax=13\n\najusreg.comp &lt;- regsubsets(preco_m ~ ., data=conj_treino, nvmax=13)\nsumario.reg &lt;- summary(ajusreg.comp)\nsumario.reg\n\n#&gt; Subset selection object\n#&gt; Call: regsubsets.formula(preco_m ~ ., data = conj_treino, nvmax = 13)\n#&gt; 7 Variables  (and intercept)\n#&gt;           Forced in Forced out\n#&gt; idade         FALSE      FALSE\n#&gt; quartos       FALSE      FALSE\n#&gt; banheiros     FALSE      FALSE\n#&gt; m2_princ      FALSE      FALSE\n#&gt; m2_tot        FALSE      FALSE\n#&gt; m2_porao      FALSE      FALSE\n#&gt; andares       FALSE      FALSE\n#&gt; 1 subsets of each size up to 7\n#&gt; Selection Algorithm: exhaustive\n#&gt;          idade quartos banheiros m2_princ m2_tot m2_porao andares\n#&gt; 1  ( 1 ) \" \"   \" \"     \" \"       \"*\"      \" \"    \" \"      \" \"    \n#&gt; 2  ( 1 ) \" \"   \"*\"     \" \"       \"*\"      \" \"    \" \"      \" \"    \n#&gt; 3  ( 1 ) \" \"   \"*\"     \" \"       \"*\"      \" \"    \" \"      \"*\"    \n#&gt; 4  ( 1 ) \"*\"   \"*\"     \" \"       \"*\"      \" \"    \" \"      \"*\"    \n#&gt; 5  ( 1 ) \"*\"   \"*\"     \"*\"       \"*\"      \" \"    \" \"      \"*\"    \n#&gt; 6  ( 1 ) \"*\"   \"*\"     \"*\"       \"*\"      \"*\"    \" \"      \"*\"    \n#&gt; 7  ( 1 ) \"*\"   \"*\"     \"*\"       \"*\"      \"*\"    \"*\"      \"*\"\n\nnames(sumario.reg)\n\n#&gt; [1] \"which\"  \"rsq\"    \"rss\"    \"adjr2\"  \"cp\"     \"bic\"    \"outmat\" \"obj\"",
    "crumbs": [
      "Conteúdo Semanal",
      "Seleção de Modelos"
    ]
  },
  {
    "objectID": "semanas/Aula07.3.html#avaliando-os-modelos",
    "href": "semanas/Aula07.3.html#avaliando-os-modelos",
    "title": "Seleção de Modelos",
    "section": "Avaliando os modelos",
    "text": "Avaliando os modelos\n\n## Os modelos vão ser escolhidos com base no menor Cp\nplot(sumario.reg$cp,xlab=\"Número de Variáveis\",ylab=\"Cp\")\nwhich.min(sumario.reg$cp)\n\n#&gt; [1] 6\n\npoints(6,sumario.reg$cp[6],pch=20,col=\"red\")",
    "crumbs": [
      "Conteúdo Semanal",
      "Seleção de Modelos"
    ]
  },
  {
    "objectID": "semanas/Aula07.3.html#como-extrair-detalhes-do-ajuste",
    "href": "semanas/Aula07.3.html#como-extrair-detalhes-do-ajuste",
    "title": "Seleção de Modelos",
    "section": "Como extrair detalhes do ajuste",
    "text": "Como extrair detalhes do ajuste\n\ncoef(ajusreg.comp,6)  \n\n#&gt;   (Intercept)         idade       quartos     banheiros      m2_princ \n#&gt;  2.551855e+02 -9.302566e-01 -4.456961e+01  9.461555e+00  1.441541e-01 \n#&gt;        m2_tot       andares \n#&gt;  9.183278e-05  3.486187e+01",
    "crumbs": [
      "Conteúdo Semanal",
      "Seleção de Modelos"
    ]
  },
  {
    "objectID": "semanas/Aula07.3.html#forward-stepwise-passo-a-passo-à-frente",
    "href": "semanas/Aula07.3.html#forward-stepwise-passo-a-passo-à-frente",
    "title": "Seleção de Modelos",
    "section": "Forward Stepwise (passo a passo à frente)",
    "text": "Forward Stepwise (passo a passo à frente)\n\najusreg.fwd &lt;- regsubsets(preco_m ~ . , data=conj_treino,nvmax=13, method=\"forward\")\nsumario.reg.fwd &lt;- summary(ajusreg.fwd)\nsumario.reg.fwd \n\n#&gt; Subset selection object\n#&gt; Call: regsubsets.formula(preco_m ~ ., data = conj_treino, nvmax = 13, \n#&gt;     method = \"forward\")\n#&gt; 7 Variables  (and intercept)\n#&gt;           Forced in Forced out\n#&gt; idade         FALSE      FALSE\n#&gt; quartos       FALSE      FALSE\n#&gt; banheiros     FALSE      FALSE\n#&gt; m2_princ      FALSE      FALSE\n#&gt; m2_tot        FALSE      FALSE\n#&gt; m2_porao      FALSE      FALSE\n#&gt; andares       FALSE      FALSE\n#&gt; 1 subsets of each size up to 7\n#&gt; Selection Algorithm: forward\n#&gt;          idade quartos banheiros m2_princ m2_tot m2_porao andares\n#&gt; 1  ( 1 ) \" \"   \" \"     \" \"       \"*\"      \" \"    \" \"      \" \"    \n#&gt; 2  ( 1 ) \" \"   \"*\"     \" \"       \"*\"      \" \"    \" \"      \" \"    \n#&gt; 3  ( 1 ) \" \"   \"*\"     \" \"       \"*\"      \" \"    \" \"      \"*\"    \n#&gt; 4  ( 1 ) \"*\"   \"*\"     \" \"       \"*\"      \" \"    \" \"      \"*\"    \n#&gt; 5  ( 1 ) \"*\"   \"*\"     \"*\"       \"*\"      \" \"    \" \"      \"*\"    \n#&gt; 6  ( 1 ) \"*\"   \"*\"     \"*\"       \"*\"      \"*\"    \" \"      \"*\"    \n#&gt; 7  ( 1 ) \"*\"   \"*\"     \"*\"       \"*\"      \"*\"    \"*\"      \"*\"\n\nwhich.min(sumario.reg.fwd$cp)\n\n#&gt; [1] 6\n\nplot(sumario.reg.fwd$cp,xlab=\"Número de Variáveis\",ylab=\"Cp\")\npoints(6,sumario.reg.fwd$cp[6],pch=20,col=\"red\")",
    "crumbs": [
      "Conteúdo Semanal",
      "Seleção de Modelos"
    ]
  },
  {
    "objectID": "semanas/Aula07.3.html#como-extrair-detalhes-do-ajuste-1",
    "href": "semanas/Aula07.3.html#como-extrair-detalhes-do-ajuste-1",
    "title": "Seleção de Modelos",
    "section": "Como extrair detalhes do ajuste",
    "text": "Como extrair detalhes do ajuste\n\ncoef(ajusreg.fwd,6)  \n\n#&gt;   (Intercept)         idade       quartos     banheiros      m2_princ \n#&gt;  2.551855e+02 -9.302566e-01 -4.456961e+01  9.461555e+00  1.441541e-01 \n#&gt;        m2_tot       andares \n#&gt;  9.183278e-05  3.486187e+01",
    "crumbs": [
      "Conteúdo Semanal",
      "Seleção de Modelos"
    ]
  },
  {
    "objectID": "semanas/Aula07.3.html#backward-stepwise-passo-a-passo-atrás",
    "href": "semanas/Aula07.3.html#backward-stepwise-passo-a-passo-atrás",
    "title": "Seleção de Modelos",
    "section": "Backward Stepwise (passo a passo atrás)",
    "text": "Backward Stepwise (passo a passo atrás)\n\najusreg.bwd &lt;- regsubsets(preco_m ~ . , data=conj_treino,nvmax=13, method=\"backward\")\nsumario.reg.bwd &lt;- summary(ajusreg.bwd)\nsumario.reg.bwd\n\n#&gt; Subset selection object\n#&gt; Call: regsubsets.formula(preco_m ~ ., data = conj_treino, nvmax = 13, \n#&gt;     method = \"backward\")\n#&gt; 7 Variables  (and intercept)\n#&gt;           Forced in Forced out\n#&gt; idade         FALSE      FALSE\n#&gt; quartos       FALSE      FALSE\n#&gt; banheiros     FALSE      FALSE\n#&gt; m2_princ      FALSE      FALSE\n#&gt; m2_tot        FALSE      FALSE\n#&gt; m2_porao      FALSE      FALSE\n#&gt; andares       FALSE      FALSE\n#&gt; 1 subsets of each size up to 7\n#&gt; Selection Algorithm: backward\n#&gt;          idade quartos banheiros m2_princ m2_tot m2_porao andares\n#&gt; 1  ( 1 ) \" \"   \" \"     \" \"       \"*\"      \" \"    \" \"      \" \"    \n#&gt; 2  ( 1 ) \" \"   \"*\"     \" \"       \"*\"      \" \"    \" \"      \" \"    \n#&gt; 3  ( 1 ) \" \"   \"*\"     \" \"       \"*\"      \" \"    \" \"      \"*\"    \n#&gt; 4  ( 1 ) \"*\"   \"*\"     \" \"       \"*\"      \" \"    \" \"      \"*\"    \n#&gt; 5  ( 1 ) \"*\"   \"*\"     \"*\"       \"*\"      \" \"    \" \"      \"*\"    \n#&gt; 6  ( 1 ) \"*\"   \"*\"     \"*\"       \"*\"      \"*\"    \" \"      \"*\"    \n#&gt; 7  ( 1 ) \"*\"   \"*\"     \"*\"       \"*\"      \"*\"    \"*\"      \"*\"\n\nwhich.min(sumario.reg.bwd$cp)\n\n#&gt; [1] 6\n\nplot(sumario.reg.bwd$cp,xlab=\"Número de Variáveis\",ylab=\"Cp\")\npoints(6,sumario.reg.bwd$cp[6],pch=20,col=\"red\")",
    "crumbs": [
      "Conteúdo Semanal",
      "Seleção de Modelos"
    ]
  },
  {
    "objectID": "semanas/Aula07.3.html#como-extrair-detalhes-do-ajuste-2",
    "href": "semanas/Aula07.3.html#como-extrair-detalhes-do-ajuste-2",
    "title": "Seleção de Modelos",
    "section": "Como extrair detalhes do ajuste",
    "text": "Como extrair detalhes do ajuste\n\ncoef(ajusreg.bwd,6)  \n\n#&gt;   (Intercept)         idade       quartos     banheiros      m2_princ \n#&gt;  2.551855e+02 -9.302566e-01 -4.456961e+01  9.461555e+00  1.441541e-01 \n#&gt;        m2_tot       andares \n#&gt;  9.183278e-05  3.486187e+01",
    "crumbs": [
      "Conteúdo Semanal",
      "Seleção de Modelos"
    ]
  },
  {
    "objectID": "semanas/Aula07.3.html#testando-outra-estatística-de-seleção-de-modelos---bic",
    "href": "semanas/Aula07.3.html#testando-outra-estatística-de-seleção-de-modelos---bic",
    "title": "Seleção de Modelos",
    "section": "Testando outra estatística de seleção de modelos - BIC",
    "text": "Testando outra estatística de seleção de modelos - BIC\n\najusreg.fwd1 &lt;- regsubsets(preco_m ~ . , data=conj_treino,nvmax=13, method=\"forward\")\nsumario.reg.fwd1 &lt;- summary(ajusreg.fwd1)\nnames(sumario.reg.fwd1)\n\n#&gt; [1] \"which\"  \"rsq\"    \"rss\"    \"adjr2\"  \"cp\"     \"bic\"    \"outmat\" \"obj\"\n\nwhich.min(sumario.reg.fwd1$bic)\n\n#&gt; [1] 4\n\nplot(sumario.reg.fwd1$bic,xlab=\"Número de Variáveis\",ylab=\"BIC\")\npoints(4,sumario.reg.fwd1$bic[4],pch=20,col=\"red\")\n\n\n\n\n\n\ncoef(ajusreg.fwd1,4)  \n\n#&gt; (Intercept)       idade     quartos    m2_princ     andares \n#&gt; 269.5362098  -0.9457714 -43.3967857   0.1469077  35.2705008",
    "crumbs": [
      "Conteúdo Semanal",
      "Seleção de Modelos"
    ]
  },
  {
    "objectID": "semanas/Aula07.3.html#usando-o-cp-novamente",
    "href": "semanas/Aula07.3.html#usando-o-cp-novamente",
    "title": "Seleção de Modelos",
    "section": "Usando o Cp novamente",
    "text": "Usando o Cp novamente\n\nwhich.min(sumario.reg.fwd1$cp)\n\n#&gt; [1] 6\n\nplot(sumario.reg.fwd1$cp,xlab=\"Número de Variáveis\",ylab=\"Cp\")\npoints(6,sumario.reg.fwd1$cp[6],pch=20,col=\"red\")\n\n\n\n\n\n\ncoef(ajusreg.fwd1,6)\n\n#&gt;   (Intercept)         idade       quartos     banheiros      m2_princ \n#&gt;  2.551855e+02 -9.302566e-01 -4.456961e+01  9.461555e+00  1.441541e-01 \n#&gt;        m2_tot       andares \n#&gt;  9.183278e-05  3.486187e+01",
    "crumbs": [
      "Conteúdo Semanal",
      "Seleção de Modelos"
    ]
  },
  {
    "objectID": "semanas/Aula07.3.html#comparando-os-dois-modelos-com-o-lm",
    "href": "semanas/Aula07.3.html#comparando-os-dois-modelos-com-o-lm",
    "title": "Seleção de Modelos",
    "section": "Comparando os dois modelos com o lm()",
    "text": "Comparando os dois modelos com o lm()\n\n## Usando o lm para ajustar o modelo com as variáveis selecionadas pelo BIC\nmod_bic &lt;- lm(preco_m ~ idade + quartos + m2_princ + andares, data=conj_treino)\nsummary(mod_bic)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = preco_m ~ idade + quartos + m2_princ + andares, \n#&gt;     data = conj_treino)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -91.569 -31.680  -1.459  34.349  95.522 \n#&gt; \n#&gt; Coefficients:\n#&gt;               Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept) 269.536210   8.833668  30.512  &lt; 2e-16 ***\n#&gt; idade        -0.945771   0.137707  -6.868 1.05e-11 ***\n#&gt; quartos     -43.396786   2.103100 -20.635  &lt; 2e-16 ***\n#&gt; m2_princ      0.146908   0.002991  49.120  &lt; 2e-16 ***\n#&gt; andares      35.270501   3.431146  10.280  &lt; 2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 40.6 on 1187 degrees of freedom\n#&gt; Multiple R-squared:  0.7515, Adjusted R-squared:  0.7507 \n#&gt; F-statistic: 897.4 on 4 and 1187 DF,  p-value: &lt; 2.2e-16\n\nmod_cp &lt;- lm(preco_m ~ idade + quartos + banheiros + m2_princ + m2_tot + andares, data=conj_treino)\nsummary(mod_cp)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = preco_m ~ idade + quartos + banheiros + m2_princ + \n#&gt;     m2_tot + andares, data = conj_treino)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -90.680 -31.815  -0.952  33.701  94.047 \n#&gt; \n#&gt; Coefficients:\n#&gt;               Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  2.552e+02  9.834e+00  25.949  &lt; 2e-16 ***\n#&gt; idade       -9.303e-01  1.373e-01  -6.777 1.93e-11 ***\n#&gt; quartos     -4.457e+01  2.173e+00 -20.513  &lt; 2e-16 ***\n#&gt; banheiros    9.462e+00  3.367e+00   2.810  0.00503 ** \n#&gt; m2_princ     1.442e-01  3.095e-03  46.574  &lt; 2e-16 ***\n#&gt; m2_tot       9.183e-05  4.410e-05   2.083  0.03750 *  \n#&gt; andares      3.486e+01  3.587e+00   9.718  &lt; 2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 40.44 on 1185 degrees of freedom\n#&gt; Multiple R-squared:  0.7538, Adjusted R-squared:  0.7525 \n#&gt; F-statistic: 604.6 on 6 and 1185 DF,  p-value: &lt; 2.2e-16\n\n\nEm termos de ajuste praticamente não há diferença nos resultados, sendo que o modelo obtido usando o BIC é bem mais enxuto. A parcimoniosidade é sempre bem vinda, mas é necessário ver como ficam os resultados com o conjunto de testes.",
    "crumbs": [
      "Conteúdo Semanal",
      "Seleção de Modelos"
    ]
  },
  {
    "objectID": "semanas/Aula07.3.html#avaliando-colinearidade",
    "href": "semanas/Aula07.3.html#avaliando-colinearidade",
    "title": "Seleção de Modelos",
    "section": "Avaliando Colinearidade",
    "text": "Avaliando Colinearidade\nUma investigação minuciosa da multicollinearidade envolverá a análise do valor do \\(R^2\\) que resulta da regressão de cada uma das variáveis explicativas contra todas as outras. A relação entre as variáveis explicativas pode ser julgada examinando uma quantidade chamada fator de inflacionário da variância (FIV) ou Variance Inflation Factor (VIF). Seja \\(Rj~^{2}\\) o quadrado do coeficiente de correlação múltipla que resulta quando a variável explicativa \\(Xj~\\) é ajustada contra todas as outras variáveis explicativas. Então o vif para \\(Xj~\\) é \\(VIFj = 1 / (1-Rj~^{2})\\)\nA regra geral é que vifs superiores a 4 justificam novas investigações, enquanto VIFs superiores a 10 são sinais de multicollinearidade grave que requerem correção.\n\nlibrary(car)\nvif(mod_bic)\n\n#&gt;    idade  quartos m2_princ  andares \n#&gt; 1.121531 2.034438 2.171839 1.025749\n\nvif(mod_cp)\n\n#&gt;     idade   quartos banheiros  m2_princ    m2_tot   andares \n#&gt;  1.122951  2.187800  1.496041  2.343757  1.114370  1.129692",
    "crumbs": [
      "Conteúdo Semanal",
      "Seleção de Modelos"
    ]
  },
  {
    "objectID": "semanas/Aula07.3.html#testando-os-dois-modelos-com-o-conjunto-de-teste",
    "href": "semanas/Aula07.3.html#testando-os-dois-modelos-com-o-conjunto-de-teste",
    "title": "Seleção de Modelos",
    "section": "Testando os dois modelos com o conjunto de teste",
    "text": "Testando os dois modelos com o conjunto de teste\n\n# Modelo com base no Cp\nsummary(mod_cp)$sigma\n\n#&gt; [1] 40.44459\n\nsummary(mod_cp)$adj.r.squared\n\n#&gt; [1] 0.7525384\n\n## Erro com conjunto de teste\nsqrt(mean((conj_teste$preco_m - predict(mod_cp, conj_teste)) ^ 2))\n\n#&gt; [1] 41.50369\n\n# Modelo com base no BIC\nsummary(mod_bic)$sigma\n\n#&gt; [1] 40.59764\n\nsummary(mod_bic)$adj.r.squared\n\n#&gt; [1] 0.750662\n\n## Erro com conjunto de teste\nsqrt(mean((conj_teste$preco_m - predict(mod_bic, conj_teste)) ^ 2))\n\n#&gt; [1] 41.56818\n\n\nAqui vemos que as diferença de resultados entre os dois modelos é muito pequena, mas o modelo com base no BIC é mais parcimonioso, então vamos usá-lo para fazer a previsão.",
    "crumbs": [
      "Conteúdo Semanal",
      "Seleção de Modelos"
    ]
  },
  {
    "objectID": "semanas/Aula07.3.html#comparando-valor-real-vs-previsão-conjunto-de-treino",
    "href": "semanas/Aula07.3.html#comparando-valor-real-vs-previsão-conjunto-de-treino",
    "title": "Seleção de Modelos",
    "section": "Comparando valor real vs previsão (conjunto de treino)",
    "text": "Comparando valor real vs previsão (conjunto de treino)\n\nconj_treino$Previsoes &lt;- predict(mod_bic, conj_treino)\nggplot(conj_treino, aes(x=Previsoes, y=preco_m)) + \n  geom_point() +\n  geom_abline(color = \"darkblue\") +\n  ggtitle(\"Preço da Casa vs. Previsões do modelo linear\")",
    "crumbs": [
      "Conteúdo Semanal",
      "Seleção de Modelos"
    ]
  },
  {
    "objectID": "semanas/Aula07.3.html#comparando-valor-real-vs-previsão-conjunto-de-teste",
    "href": "semanas/Aula07.3.html#comparando-valor-real-vs-previsão-conjunto-de-teste",
    "title": "Seleção de Modelos",
    "section": "Comparando valor real vs previsão (conjunto de teste)",
    "text": "Comparando valor real vs previsão (conjunto de teste)\n\nconj_teste$Previsoes &lt;- predict(mod_bic, conj_teste)\nggplot(conj_teste, aes(x=Previsoes, y=preco_m)) + \n  geom_point() +\n  geom_abline(color = \"darkblue\") +\n  ggtitle(\"Preço da Casa vs. Previsões do modelo linear\")",
    "crumbs": [
      "Conteúdo Semanal",
      "Seleção de Modelos"
    ]
  },
  {
    "objectID": "semanas/Aula07.3.html#avaliando-os-resíduos",
    "href": "semanas/Aula07.3.html#avaliando-os-resíduos",
    "title": "Seleção de Modelos",
    "section": "Avaliando os resíduos",
    "text": "Avaliando os resíduos\n\nresidualPlots(mod_bic)\n\n\n\n\n\n\n\n#&gt;            Test stat Pr(&gt;|Test stat|)\n#&gt; idade        -0.6066           0.5443\n#&gt; quartos       0.6446           0.5193\n#&gt; m2_princ      0.4534           0.6504\n#&gt; andares       0.9874           0.3236\n#&gt; Tukey test    0.6214           0.5343\n\ninfluencePlot(mod_bic)\n\n\n\n\n\n\n\n#&gt;         StudRes         Hat       CookD\n#&gt; 255   2.3601793 0.002345386 0.002609066\n#&gt; 561   0.5579794 0.023405542 0.001493217\n#&gt; 704  -1.7199510 0.021700032 0.013101910\n#&gt; 1115  2.4051510 0.045324181 0.054706842",
    "crumbs": [
      "Conteúdo Semanal",
      "Seleção de Modelos"
    ]
  },
  {
    "objectID": "semanas/Aula07.2A.html",
    "href": "semanas/Aula07.2A.html",
    "title": "Regressão Linear Múltipla - Validação Cruzada",
    "section": "",
    "text": "library(tidyverse)\n\n#&gt; ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n#&gt; ✔ dplyr     1.1.4     ✔ readr     2.1.5\n#&gt; ✔ forcats   1.0.0     ✔ stringr   1.5.1\n#&gt; ✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n#&gt; ✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n#&gt; ✔ purrr     1.0.2     \n#&gt; ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n#&gt; ✖ purrr::%||%()   masks base::%||%()\n#&gt; ✖ dplyr::filter() masks stats::filter()\n#&gt; ✖ dplyr::lag()    masks stats::lag()\n#&gt; ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors"
  },
  {
    "objectID": "semanas/Aula07.2A.html#carregando-bibliotecas",
    "href": "semanas/Aula07.2A.html#carregando-bibliotecas",
    "title": "Regressão Linear Múltipla - Validação Cruzada",
    "section": "",
    "text": "library(tidyverse)\n\n#&gt; ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n#&gt; ✔ dplyr     1.1.4     ✔ readr     2.1.5\n#&gt; ✔ forcats   1.0.0     ✔ stringr   1.5.1\n#&gt; ✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n#&gt; ✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n#&gt; ✔ purrr     1.0.2     \n#&gt; ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n#&gt; ✖ purrr::%||%()   masks base::%||%()\n#&gt; ✖ dplyr::filter() masks stats::filter()\n#&gt; ✖ dplyr::lag()    masks stats::lag()\n#&gt; ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors"
  },
  {
    "objectID": "semanas/Aula07.2A.html#dados-de-propaganda",
    "href": "semanas/Aula07.2A.html#dados-de-propaganda",
    "title": "Regressão Linear Múltipla - Validação Cruzada",
    "section": "Dados de propaganda",
    "text": "Dados de propaganda\nO conjunto de dados contém estatísticas sobre as vendas de um produto em 200 diferentes mercados, juntamente com orçamentos publicitários em cada um desses mercados, para diferentes canais de mídia: TV, rádio e jornal. As vendas estão em milhares de unidades e o orçamento está em milhares de dólares.\n\nlibrary(readxl)\npropaganda &lt;- read_excel(\"Propaganda.xlsx\")\nsummary(propaganda)\n\n#&gt;        TV             Radio          Newspaper          Sales      \n#&gt;  Min.   :  0.70   Min.   : 0.000   Min.   :  0.30   Min.   : 1.60  \n#&gt;  1st Qu.: 74.38   1st Qu.: 9.975   1st Qu.: 12.75   1st Qu.:10.38  \n#&gt;  Median :149.75   Median :22.900   Median : 25.75   Median :12.90  \n#&gt;  Mean   :147.04   Mean   :23.264   Mean   : 30.55   Mean   :14.02  \n#&gt;  3rd Qu.:218.82   3rd Qu.:36.525   3rd Qu.: 45.10   3rd Qu.:17.40  \n#&gt;  Max.   :296.40   Max.   :49.600   Max.   :114.00   Max.   :27.00"
  },
  {
    "objectID": "semanas/Aula07.2A.html#renomeando",
    "href": "semanas/Aula07.2A.html#renomeando",
    "title": "Regressão Linear Múltipla - Validação Cruzada",
    "section": "Renomeando",
    "text": "Renomeando\n\npropaganda &lt;- propaganda %&gt;% rename(Jornal = Newspaper, Vendas = Sales)"
  },
  {
    "objectID": "semanas/Aula07.2A.html#validação-cruzada",
    "href": "semanas/Aula07.2A.html#validação-cruzada",
    "title": "Regressão Linear Múltipla - Validação Cruzada",
    "section": "Validação cruzada",
    "text": "Validação cruzada\nA validação cruzada é útil para termos uma estimativa do erro fora da amostra. O método de validação cruzada é um procedimento utilizado para estimar a qualidade de um modelo de aprendizado de máquina em dados não vistos. A validação cruzada é um procedimento comum para avaliar a capacidade de generalização de um modelo.\n\nlibrary(caret)\n\n#&gt; Carregando pacotes exigidos: lattice\n\n\n#&gt; \n#&gt; Anexando pacote: 'caret'\n\n\n#&gt; O seguinte objeto é mascarado por 'package:purrr':\n#&gt; \n#&gt;     lift\n\nset.seed(21)\nmodelo &lt;- train( Vendas ~ ., propaganda, method = \"lm\",\n                 trControl = trainControl(method = \"cv\", number = 10,\n                                          verboseIter = TRUE))\n\n#&gt; + Fold01: intercept=TRUE \n#&gt; - Fold01: intercept=TRUE \n#&gt; + Fold02: intercept=TRUE \n#&gt; - Fold02: intercept=TRUE \n#&gt; + Fold03: intercept=TRUE \n#&gt; - Fold03: intercept=TRUE \n#&gt; + Fold04: intercept=TRUE \n#&gt; - Fold04: intercept=TRUE \n#&gt; + Fold05: intercept=TRUE \n#&gt; - Fold05: intercept=TRUE \n#&gt; + Fold06: intercept=TRUE \n#&gt; - Fold06: intercept=TRUE \n#&gt; + Fold07: intercept=TRUE \n#&gt; - Fold07: intercept=TRUE \n#&gt; + Fold08: intercept=TRUE \n#&gt; - Fold08: intercept=TRUE \n#&gt; + Fold09: intercept=TRUE \n#&gt; - Fold09: intercept=TRUE \n#&gt; + Fold10: intercept=TRUE \n#&gt; - Fold10: intercept=TRUE \n#&gt; Aggregating results\n#&gt; Fitting final model on full training set\n\n# Mostra o modelo\nmodelo\n\n#&gt; Linear Regression \n#&gt; \n#&gt; 200 samples\n#&gt;   3 predictor\n#&gt; \n#&gt; No pre-processing\n#&gt; Resampling: Cross-Validated (10 fold) \n#&gt; Summary of sample sizes: 180, 180, 181, 180, 180, 180, ... \n#&gt; Resampling results:\n#&gt; \n#&gt;   RMSE      Rsquared   MAE     \n#&gt;   1.662943  0.9033394  1.286409\n#&gt; \n#&gt; Tuning parameter 'intercept' was held constant at a value of TRUE\n\n# Modelo final\nmodelo$finalModel\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = .outcome ~ ., data = dat)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)           TV        Radio       Jornal  \n#&gt;    2.938889     0.045765     0.188530    -0.001037\n\n\nVeja que a validação cruzada não faz seleção de variáveis.\nO modelo final obtido mostra Jornal com um coeficiente negativo!\nEsta variável deveria estar fora do modelo."
  },
  {
    "objectID": "semanas/Aula07.2A.html#novo-modelo",
    "href": "semanas/Aula07.2A.html#novo-modelo",
    "title": "Regressão Linear Múltipla - Validação Cruzada",
    "section": "Novo modelo",
    "text": "Novo modelo\n\nset.seed(21)\nmodelo &lt;- train( Vendas ~ TV + Radio, propaganda, method = \"lm\",\n                 trControl = trainControl(method = \"cv\", number = 10,\n                                          verboseIter = TRUE))\n\n#&gt; + Fold01: intercept=TRUE \n#&gt; - Fold01: intercept=TRUE \n#&gt; + Fold02: intercept=TRUE \n#&gt; - Fold02: intercept=TRUE \n#&gt; + Fold03: intercept=TRUE \n#&gt; - Fold03: intercept=TRUE \n#&gt; + Fold04: intercept=TRUE \n#&gt; - Fold04: intercept=TRUE \n#&gt; + Fold05: intercept=TRUE \n#&gt; - Fold05: intercept=TRUE \n#&gt; + Fold06: intercept=TRUE \n#&gt; - Fold06: intercept=TRUE \n#&gt; + Fold07: intercept=TRUE \n#&gt; - Fold07: intercept=TRUE \n#&gt; + Fold08: intercept=TRUE \n#&gt; - Fold08: intercept=TRUE \n#&gt; + Fold09: intercept=TRUE \n#&gt; - Fold09: intercept=TRUE \n#&gt; + Fold10: intercept=TRUE \n#&gt; - Fold10: intercept=TRUE \n#&gt; Aggregating results\n#&gt; Fitting final model on full training set\n\n# Mostra o modelo\nmodelo\n\n#&gt; Linear Regression \n#&gt; \n#&gt; 200 samples\n#&gt;   2 predictor\n#&gt; \n#&gt; No pre-processing\n#&gt; Resampling: Cross-Validated (10 fold) \n#&gt; Summary of sample sizes: 180, 180, 181, 180, 180, 180, ... \n#&gt; Resampling results:\n#&gt; \n#&gt;   RMSE      Rsquared   MAE     \n#&gt;   1.649103  0.9051774  1.275233\n#&gt; \n#&gt; Tuning parameter 'intercept' was held constant at a value of TRUE\n\n# Modelo final\nmodelo$finalModel\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = .outcome ~ ., data = dat)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)           TV        Radio  \n#&gt;     2.92110      0.04575      0.18799"
  },
  {
    "objectID": "semanas/Aula07.2A.html#modelo-inicial",
    "href": "semanas/Aula07.2A.html#modelo-inicial",
    "title": "Regressão Linear Múltipla - Validação Cruzada",
    "section": "Modelo Inicial",
    "text": "Modelo Inicial\n\nset.seed(21)\ny &lt;- propaganda$Vendas\nindice_teste &lt;- createDataPartition(y, times = 1, p = 0.40, list = FALSE)\nconj_treino &lt;- propaganda[-indice_teste, ]\nconj_teste &lt;- propaganda[indice_teste, ]\n\n## Modelo com conjunto de treino\nmod &lt;- lm( Vendas ~ TV + Radio, data = conj_treino)\nsummary(mod)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = Vendas ~ TV + Radio, data = conj_treino)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -9.4585 -0.6886  0.1687  1.0799  2.5529 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept) 3.441076   0.357985   9.612   &lt;2e-16 ***\n#&gt; TV          0.042575   0.001774  24.005   &lt;2e-16 ***\n#&gt; Radio       0.191607   0.010412  18.402   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 1.607 on 116 degrees of freedom\n#&gt; Multiple R-squared:  0.8992, Adjusted R-squared:  0.8975 \n#&gt; F-statistic: 517.5 on 2 and 116 DF,  p-value: &lt; 2.2e-16\n\n# Erro com conjunto de teste\nsqrt(mean((conj_teste$Vendas - predict(mod, conj_teste)) ^ 2))\n\n#&gt; [1] 1.846764"
  },
  {
    "objectID": "semanas/Aula07.1.html",
    "href": "semanas/Aula07.1.html",
    "title": "Regressão Linear",
    "section": "",
    "text": "library(tidyverse)\n\n#&gt; ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n#&gt; ✔ dplyr     1.1.4     ✔ readr     2.1.5\n#&gt; ✔ forcats   1.0.0     ✔ stringr   1.5.1\n#&gt; ✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n#&gt; ✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n#&gt; ✔ purrr     1.0.4     \n#&gt; ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n#&gt; ✖ dplyr::filter() masks stats::filter()\n#&gt; ✖ dplyr::lag()    masks stats::lag()\n#&gt; ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors",
    "crumbs": [
      "Conteúdo Semanal",
      "Regressão Simples"
    ]
  },
  {
    "objectID": "semanas/Aula07.1.html#carregando-bibliotecas",
    "href": "semanas/Aula07.1.html#carregando-bibliotecas",
    "title": "Regressão Linear",
    "section": "",
    "text": "library(tidyverse)\n\n#&gt; ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n#&gt; ✔ dplyr     1.1.4     ✔ readr     2.1.5\n#&gt; ✔ forcats   1.0.0     ✔ stringr   1.5.1\n#&gt; ✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n#&gt; ✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n#&gt; ✔ purrr     1.0.4     \n#&gt; ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n#&gt; ✖ dplyr::filter() masks stats::filter()\n#&gt; ✖ dplyr::lag()    masks stats::lag()\n#&gt; ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors",
    "crumbs": [
      "Conteúdo Semanal",
      "Regressão Simples"
    ]
  },
  {
    "objectID": "semanas/Aula07.1.html#dados-de-propaganda",
    "href": "semanas/Aula07.1.html#dados-de-propaganda",
    "title": "Regressão Linear",
    "section": "Dados de propaganda",
    "text": "Dados de propaganda\nO conjunto de dados contém estatísticas sobre as vendas de um produto em 200 diferentes mercados, juntamente com orçamentos publicitários em cada um desses mercados, para diferentes canais de mídia: TV, rádio e jornal. As vendas estão em milhares de unidades e o orçamento está em milhares de dólares.\n\nlibrary(readxl)\npropaganda &lt;- read_excel(\"Propaganda.xlsx\")\nsummary(propaganda)\n\n#&gt;        TV             Radio          Newspaper          Sales      \n#&gt;  Min.   :  0.70   Min.   : 0.000   Min.   :  0.30   Min.   : 1.60  \n#&gt;  1st Qu.: 74.38   1st Qu.: 9.975   1st Qu.: 12.75   1st Qu.:10.38  \n#&gt;  Median :149.75   Median :22.900   Median : 25.75   Median :12.90  \n#&gt;  Mean   :147.04   Mean   :23.264   Mean   : 30.55   Mean   :14.02  \n#&gt;  3rd Qu.:218.82   3rd Qu.:36.525   3rd Qu.: 45.10   3rd Qu.:17.40  \n#&gt;  Max.   :296.40   Max.   :49.600   Max.   :114.00   Max.   :27.00",
    "crumbs": [
      "Conteúdo Semanal",
      "Regressão Simples"
    ]
  },
  {
    "objectID": "semanas/Aula07.1.html#renomeando",
    "href": "semanas/Aula07.1.html#renomeando",
    "title": "Regressão Linear",
    "section": "Renomeando",
    "text": "Renomeando\n\npropaganda &lt;- propaganda %&gt;% rename(Jornal = Newspaper, Vendas = Sales)",
    "crumbs": [
      "Conteúdo Semanal",
      "Regressão Simples"
    ]
  },
  {
    "objectID": "semanas/Aula07.1.html#sumario",
    "href": "semanas/Aula07.1.html#sumario",
    "title": "Regressão Linear",
    "section": "Sumario",
    "text": "Sumario\n\nlibrary(summarytools)\n\n#&gt; \n#&gt; Anexando pacote: 'summarytools'\n\n\n#&gt; O seguinte objeto é mascarado por 'package:tibble':\n#&gt; \n#&gt;     view\n\ndfSummary(propaganda) |&gt; stview(method = \"render\")\n\n\nData Frame Summary\npropaganda\nDimensions: 200 x 4\n  Duplicates: 0\n\n\n\n\n\n\n\n\n\n\n\nNo\nVariable\nStats / Values\nFreqs (% of Valid)\nGraph\nValid\nMissing\n\n\n\n1\nTV [numeric]\n\n\nMean (sd) : 147 (85.9)\n\n\nmin ≤ med ≤ max:\n\n\n0.7 ≤ 149.8 ≤ 296.4\n\n\nIQR (CV) : 144.4 (0.6)\n\n\n190 distinct values\n\n200 (100.0%)\n0 (0.0%)\n\n\n2\nRadio [numeric]\n\n\nMean (sd) : 23.3 (14.8)\n\n\nmin ≤ med ≤ max:\n\n\n0 ≤ 22.9 ≤ 49.6\n\n\nIQR (CV) : 26.5 (0.6)\n\n\n167 distinct values\n\n200 (100.0%)\n0 (0.0%)\n\n\n3\nJornal [numeric]\n\n\nMean (sd) : 30.6 (21.8)\n\n\nmin ≤ med ≤ max:\n\n\n0.3 ≤ 25.8 ≤ 114\n\n\nIQR (CV) : 32.4 (0.7)\n\n\n172 distinct values\n\n200 (100.0%)\n0 (0.0%)\n\n\n4\nVendas [numeric]\n\n\nMean (sd) : 14 (5.2)\n\n\nmin ≤ med ≤ max:\n\n\n1.6 ≤ 12.9 ≤ 27\n\n\nIQR (CV) : 7 (0.4)\n\n\n121 distinct values\n\n200 (100.0%)\n0 (0.0%)\n\n\n\nGenerated by summarytools 1.1.3 (R version 4.5.0)2025-04-28",
    "crumbs": [
      "Conteúdo Semanal",
      "Regressão Simples"
    ]
  },
  {
    "objectID": "semanas/Aula07.1.html#linhas-inicias",
    "href": "semanas/Aula07.1.html#linhas-inicias",
    "title": "Regressão Linear",
    "section": "Linhas inicias",
    "text": "Linhas inicias\n\nlibrary(gt)\ngt(head(propaganda, 10)) %&gt;% \n  tab_header(title = \"Propaganda\")\n\n\n\n\n\n\nPropaganda\n\n\nTV\nRadio\nJornal\nVendas\n\n\n\n\n230.1\n37.8\n69.2\n22.1\n\n\n44.5\n39.3\n45.1\n10.4\n\n\n17.2\n45.9\n69.3\n9.3\n\n\n151.5\n41.3\n58.5\n18.5\n\n\n180.8\n10.8\n58.4\n12.9\n\n\n8.7\n48.9\n75.0\n7.2\n\n\n57.5\n32.8\n23.5\n11.8\n\n\n120.2\n19.6\n11.6\n13.2\n\n\n8.6\n2.1\n1.0\n4.8\n\n\n199.8\n2.6\n21.2\n10.6",
    "crumbs": [
      "Conteúdo Semanal",
      "Regressão Simples"
    ]
  },
  {
    "objectID": "semanas/Aula07.1.html#criando-amostra-de-treino-e-teste",
    "href": "semanas/Aula07.1.html#criando-amostra-de-treino-e-teste",
    "title": "Regressão Linear",
    "section": "Criando amostra de treino e teste",
    "text": "Criando amostra de treino e teste\n\nlibrary(caret)\nset.seed(21)\ny &lt;- propaganda$Vendas\nindice_teste &lt;- createDataPartition(y, times = 1, p = 0.40, list = FALSE)\n\nconj_treino &lt;- propaganda[-indice_teste, ]\nconj_teste &lt;- propaganda[indice_teste, ]\n\nstr(conj_treino)\n\n#&gt; tibble [119 × 4] (S3: tbl_df/tbl/data.frame)\n#&gt;  $ TV    : num [1:119] 230.1 151.5 180.8 199.8 66.1 ...\n#&gt;  $ Radio : num [1:119] 37.8 41.3 10.8 2.6 5.8 35.1 7.6 47.7 20.5 23.9 ...\n#&gt;  $ Jornal: num [1:119] 69.2 58.5 58.4 21.2 24.2 65.9 7.2 52.9 18.3 19.1 ...\n#&gt;  $ Vendas: num [1:119] 22.1 18.5 12.9 10.6 8.6 9.2 9.7 22.4 11.3 14.6 ...\n\nstr(conj_teste)\n\n#&gt; tibble [81 × 4] (S3: tbl_df/tbl/data.frame)\n#&gt;  $ TV    : num [1:81] 44.5 17.2 8.7 57.5 120.2 ...\n#&gt;  $ Radio : num [1:81] 39.3 45.9 48.9 32.8 19.6 2.1 24 32.9 36.6 39.6 ...\n#&gt;  $ Jornal: num [1:81] 45.1 69.3 75 23.5 11.6 1 4 46 114 55.8 ...\n#&gt;  $ Vendas: num [1:81] 10.4 9.3 7.2 11.8 13.2 4.8 17.4 19 12.5 24.4 ...\n\ngt::gt(head(conj_treino, 10))   %&gt;% \n  tab_header(title = \"Propaganda\")\n\n\n\n\n\n\nPropaganda\n\n\nTV\nRadio\nJornal\nVendas\n\n\n\n\n230.1\n37.8\n69.2\n22.1\n\n\n151.5\n41.3\n58.5\n18.5\n\n\n180.8\n10.8\n58.4\n12.9\n\n\n199.8\n2.6\n21.2\n10.6\n\n\n66.1\n5.8\n24.2\n8.6\n\n\n23.8\n35.1\n65.9\n9.2\n\n\n97.5\n7.6\n7.2\n9.7\n\n\n195.4\n47.7\n52.9\n22.4\n\n\n69.2\n20.5\n18.3\n11.3\n\n\n147.3\n23.9\n19.1\n14.6",
    "crumbs": [
      "Conteúdo Semanal",
      "Regressão Simples"
    ]
  },
  {
    "objectID": "semanas/Aula07.1.html#primeira-visualização-dos-dados",
    "href": "semanas/Aula07.1.html#primeira-visualização-dos-dados",
    "title": "Regressão Linear",
    "section": "Primeira visualização dos dados",
    "text": "Primeira visualização dos dados\nAqui estou usando uma função do pacote caret que de uma maneira simples apresenta a relação entre a variável resposta e suas possíveis variáveis explicativas\n\nfeaturePlot(x = conj_treino[ , c(\"TV\", \"Radio\", \"Jornal\")], y = conj_treino$Vendas)",
    "crumbs": [
      "Conteúdo Semanal",
      "Regressão Simples"
    ]
  },
  {
    "objectID": "semanas/Aula07.1.html#usando-o-ggplot",
    "href": "semanas/Aula07.1.html#usando-o-ggplot",
    "title": "Regressão Linear",
    "section": "Usando o ggplot",
    "text": "Usando o ggplot\n\ngt(head(conj_treino, 10))\n\n\n\n\n\nTV\nRadio\nJornal\nVendas\n\n\n\n230.1\n37.8\n69.2\n22.1\n\n\n151.5\n41.3\n58.5\n18.5\n\n\n180.8\n10.8\n58.4\n12.9\n\n\n199.8\n2.6\n21.2\n10.6\n\n\n66.1\n5.8\n24.2\n8.6\n\n\n23.8\n35.1\n65.9\n9.2\n\n\n97.5\n7.6\n7.2\n9.7\n\n\n195.4\n47.7\n52.9\n22.4\n\n\n69.2\n20.5\n18.3\n11.3\n\n\n147.3\n23.9\n19.1\n14.6\n\n\n\n\n\nc_treino_pivot &lt;- conj_treino %&gt;% pivot_longer(!Vendas, names_to=\"Tipo\", values_to=\"Orçamento\" ) \ngt(head(c_treino_pivot, 10))\n\n\n\n\n\nVendas\nTipo\nOrçamento\n\n\n\n22.1\nTV\n230.1\n\n\n22.1\nRadio\n37.8\n\n\n22.1\nJornal\n69.2\n\n\n18.5\nTV\n151.5\n\n\n18.5\nRadio\n41.3\n\n\n18.5\nJornal\n58.5\n\n\n12.9\nTV\n180.8\n\n\n12.9\nRadio\n10.8\n\n\n12.9\nJornal\n58.4\n\n\n10.6\nTV\n199.8\n\n\n\n\n\nconj_treino %&gt;% pivot_longer(!Vendas, names_to=\"Tipo\", values_to=\"Orçamento\" ) %&gt;%\n            ggplot() + \n            geom_point(aes(x=Orçamento, y=Vendas)) +\n            facet_wrap( ~ Tipo, scales = \"free_x\") +\n            labs(x = \"Orçamento (1000 US$)\", \n                 y = \"Vendas (em 1000 unidades vendidas)\", \n                 title = \"Vendas vs Propaganda\"\n                 )",
    "crumbs": [
      "Conteúdo Semanal",
      "Regressão Simples"
    ]
  },
  {
    "objectID": "semanas/Aula07.1.html#matriz-de-dispersão",
    "href": "semanas/Aula07.1.html#matriz-de-dispersão",
    "title": "Regressão Linear",
    "section": "Matriz de dispersão",
    "text": "Matriz de dispersão\n\nlibrary(psych)\npairs.panels(conj_treino, \n             method = \"pearson\", # metodo de correlação\n             hist.col = \"#00AFBB\",\n             density = TRUE,  # mostra graficos de densidade\n             ellipses = FALSE # mostra elipses de correlação\n             )",
    "crumbs": [
      "Conteúdo Semanal",
      "Regressão Simples"
    ]
  },
  {
    "objectID": "semanas/Aula07.1.html#matriz-de-correlações",
    "href": "semanas/Aula07.1.html#matriz-de-correlações",
    "title": "Regressão Linear",
    "section": "Matriz de Correlações",
    "text": "Matriz de Correlações\n\nlibrary(corrplot)\n\n#&gt; corrplot 0.95 loaded\n\nmat_corr &lt;- cor(conj_treino)\ncorrplot(mat_corr, method = \"number\", col = \"black\", cl.pos = \"n\")",
    "crumbs": [
      "Conteúdo Semanal",
      "Regressão Simples"
    ]
  },
  {
    "objectID": "semanas/Aula07.1.html#outra-matriz-de-disersão",
    "href": "semanas/Aula07.1.html#outra-matriz-de-disersão",
    "title": "Regressão Linear",
    "section": "Outra Matriz de Disersão",
    "text": "Outra Matriz de Disersão\n\nlibrary(GGally)\n\n#&gt; Registered S3 method overwritten by 'GGally':\n#&gt;   method from   \n#&gt;   +.gg   ggplot2\n\nggpairs(conj_treino)",
    "crumbs": [
      "Conteúdo Semanal",
      "Regressão Simples"
    ]
  },
  {
    "objectID": "semanas/Aula07.1.html#o-mod-regressão",
    "href": "semanas/Aula07.1.html#o-mod-regressão",
    "title": "Regressão Linear",
    "section": "1o Mod Regressão",
    "text": "1o Mod Regressão\n\nmod1 &lt;- lm( Vendas ~ TV, data = conj_treino)\nnames(mod1)\n\n#&gt;  [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n#&gt;  [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n#&gt;  [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"\n\ncoeflinear &lt;- mod1$coefficients[1]\ncoefang &lt;- mod1$coefficients[2]\nsummary(mod1)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = Vendas ~ TV, data = conj_treino)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -7.6383 -1.9426 -0.0565  1.7033  7.5277 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept) 6.929269   0.598642   11.57   &lt;2e-16 ***\n#&gt; TV          0.046471   0.003471   13.39   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 3.167 on 117 degrees of freedom\n#&gt; Multiple R-squared:  0.605,  Adjusted R-squared:  0.6017 \n#&gt; F-statistic: 179.2 on 1 and 117 DF,  p-value: &lt; 2.2e-16\n\nggplot(conj_treino, aes(x=TV, y=Vendas)) +\n    geom_point() +\n    geom_abline(slope = coefang,intercept = coeflinear, color=\"blue\" ) + \n    geom_hline(yintercept=coeflinear, linetype=\"dashed\", color = \"red\")",
    "crumbs": [
      "Conteúdo Semanal",
      "Regressão Simples"
    ]
  },
  {
    "objectID": "semanas/Aula07.1.html#outra-forma-de-representação-do-1o-modelo",
    "href": "semanas/Aula07.1.html#outra-forma-de-representação-do-1o-modelo",
    "title": "Regressão Linear",
    "section": "Outra forma de representação do 1o Modelo",
    "text": "Outra forma de representação do 1o Modelo\n\nlibrary(car)\nscatterplot(Vendas ~ TV, data = conj_treino, smooth=F)",
    "crumbs": [
      "Conteúdo Semanal",
      "Regressão Simples"
    ]
  },
  {
    "objectID": "semanas/Aula07.1.html#outra-forma-de-representação-do-1o-modelo-1",
    "href": "semanas/Aula07.1.html#outra-forma-de-representação-do-1o-modelo-1",
    "title": "Regressão Linear",
    "section": "Outra forma de representação do 1o Modelo",
    "text": "Outra forma de representação do 1o Modelo\n\nlibrary(ggside)\n\n#&gt; Registered S3 method overwritten by 'ggside':\n#&gt;   method from  \n#&gt;   +.gg   GGally\n\nggplot(conj_treino, aes(x=TV, y=Vendas)) +\n    geom_point() +\n    geom_smooth(method = lm, se = FALSE) + \n    geom_xsidehistogram(bins = round(1+3.322*log10(nrow(conj_treino)),0)) + \n    geom_ysidehistogram(bins = round(1+3.322*log10(nrow(conj_treino)),0))  \n\n#&gt; `geom_smooth()` using formula = 'y ~ x'\n\n\n#&gt; Warning: `is.ggproto()` was deprecated in ggplot2 3.5.2.\n#&gt; ℹ Please use `is_ggproto()` instead.",
    "crumbs": [
      "Conteúdo Semanal",
      "Regressão Simples"
    ]
  },
  {
    "objectID": "semanas/Aula07.1.html#extraindo-informações-do-1o-ajuste",
    "href": "semanas/Aula07.1.html#extraindo-informações-do-1o-ajuste",
    "title": "Regressão Linear",
    "section": "Extraindo informações do 1o ajuste",
    "text": "Extraindo informações do 1o ajuste\n\nsummary(mod1)$sigma\n\n#&gt; [1] 3.167319\n\nsummary(mod1)$r.squared\n\n#&gt; [1] 0.605027",
    "crumbs": [
      "Conteúdo Semanal",
      "Regressão Simples"
    ]
  },
  {
    "objectID": "semanas/Aula07.1.html#extraindo-usando-uma-função-do-pacote-car",
    "href": "semanas/Aula07.1.html#extraindo-usando-uma-função-do-pacote-car",
    "title": "Regressão Linear",
    "section": "Extraindo usando uma função do pacote car",
    "text": "Extraindo usando uma função do pacote car\n\nbrief(mod1)\n\n#&gt;            (Intercept)      TV\n#&gt; Estimate         6.929 0.04647\n#&gt; Std. Error       0.599 0.00347\n#&gt; \n#&gt;  Residual SD = 3.17 on 117 df, R-squared = 0.605",
    "crumbs": [
      "Conteúdo Semanal",
      "Regressão Simples"
    ]
  },
  {
    "objectID": "semanas/Aula07.1.html#intervalo-de-confiança",
    "href": "semanas/Aula07.1.html#intervalo-de-confiança",
    "title": "Regressão Linear",
    "section": "Intervalo de Confiança",
    "text": "Intervalo de Confiança\n\nsummary(mod1)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = Vendas ~ TV, data = conj_treino)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -7.6383 -1.9426 -0.0565  1.7033  7.5277 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept) 6.929269   0.598642   11.57   &lt;2e-16 ***\n#&gt; TV          0.046471   0.003471   13.39   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 3.167 on 117 degrees of freedom\n#&gt; Multiple R-squared:  0.605,  Adjusted R-squared:  0.6017 \n#&gt; F-statistic: 179.2 on 1 and 117 DF,  p-value: &lt; 2.2e-16\n\nconfint(mod1)\n\n#&gt;                  2.5 %     97.5 %\n#&gt; (Intercept) 5.74368904 8.11484821\n#&gt; TV          0.03959625 0.05334545",
    "crumbs": [
      "Conteúdo Semanal",
      "Regressão Simples"
    ]
  },
  {
    "objectID": "semanas/Aula07.1.html#anova",
    "href": "semanas/Aula07.1.html#anova",
    "title": "Regressão Linear",
    "section": "Anova",
    "text": "Anova\n\nanova(mod1)\n\n#&gt; Analysis of Variance Table\n#&gt; \n#&gt; Response: Vendas\n#&gt;            Df Sum Sq Mean Sq F value    Pr(&gt;F)    \n#&gt; TV          1 1798.0 1797.95  179.22 &lt; 2.2e-16 ***\n#&gt; Residuals 117 1173.7   10.03                      \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "Conteúdo Semanal",
      "Regressão Simples"
    ]
  },
  {
    "objectID": "semanas/Aula07.1.html#previsões",
    "href": "semanas/Aula07.1.html#previsões",
    "title": "Regressão Linear",
    "section": "Previsões",
    "text": "Previsões\n\n#?predict\npredict(mod1, data.frame(TV=c(50, 150, 250)), interval = \"prediction\")\n\n#&gt;         fit       lwr      upr\n#&gt; 1  9.252811  2.915787 15.58983\n#&gt; 2 13.899896  7.600884 20.19891\n#&gt; 3 18.546982 12.211175 24.88279",
    "crumbs": [
      "Conteúdo Semanal",
      "Regressão Simples"
    ]
  },
  {
    "objectID": "semanas/Aula07.1.html#calculando-o-erro-padrão-do-resíduo-com-amostra-de-teste",
    "href": "semanas/Aula07.1.html#calculando-o-erro-padrão-do-resíduo-com-amostra-de-teste",
    "title": "Regressão Linear",
    "section": "Calculando o erro padrão do resíduo com amostra de teste",
    "text": "Calculando o erro padrão do resíduo com amostra de teste\n\n##Erro com conjunto de teste\nsqrt(mean((conj_teste$Vendas - predict(mod1, conj_teste)) ^ 2)) \n\n#&gt; [1] 3.41382\n\n## Error com conjunto de treino\nsummary(mod1)$sigma\n\n#&gt; [1] 3.167319",
    "crumbs": [
      "Conteúdo Semanal",
      "Regressão Simples"
    ]
  },
  {
    "objectID": "semanas/Aula07.1.html#análise-dos-resíduos-do-1o-modelo",
    "href": "semanas/Aula07.1.html#análise-dos-resíduos-do-1o-modelo",
    "title": "Regressão Linear",
    "section": "Análise dos resíduos do 1o modelo",
    "text": "Análise dos resíduos do 1o modelo\n\nplot(mod1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGráfico de Resíduos vs valor ajustado (fitted): Os resíduos estão igualmente espalhados em torno de uma linha horizontal sem padrões distintos? (isso é uma boa indicação de que você não tem relações não lineares)\nGráfico de quantis normais (Normal QQ). Os resíduos seguem bem a linha reta ou se desviam severamente? (isso é uma indicação que os resíduos se distribuem como a normal)\nGráfico de Dispersão-Localização. Você está vendo uma linha horizontal com pontos espalhados aleatoriamente ao longo dela? (Isto indica que a variância é constante)\n\nGráfico de Resíduos vs influência (leverage). A distância de Cook tem valor alto? ( isto indica que o dado tem influência para os resultados obtidos na regressão)",
    "crumbs": [
      "Conteúdo Semanal",
      "Regressão Simples"
    ]
  },
  {
    "objectID": "semanas/Aula07.1.html#outros-testes",
    "href": "semanas/Aula07.1.html#outros-testes",
    "title": "Regressão Linear",
    "section": "Outros testes",
    "text": "Outros testes\nTeste de normalidade Teste de heterocedasticidade (Bresch-Pagan) Teste de autocorrelação (Durbin-Watson)\n\nlibrary(lmtest)\n\n#&gt; Carregando pacotes exigidos: zoo\n\n\n#&gt; \n#&gt; Anexando pacote: 'zoo'\n\n\n#&gt; Os seguintes objetos são mascarados por 'package:base':\n#&gt; \n#&gt;     as.Date, as.Date.numeric\n\nmod1_sum &lt;- summary(mod1)\n# Teste de normalidade\nshapiro.test(mod1_sum$residuals)\n\n#&gt; \n#&gt;  Shapiro-Wilk normality test\n#&gt; \n#&gt; data:  mod1_sum$residuals\n#&gt; W = 0.98867, p-value = 0.4293\n\n# Teste de hetrocedasticidade\nbptest(mod1)\n\n#&gt; \n#&gt;  studentized Breusch-Pagan test\n#&gt; \n#&gt; data:  mod1\n#&gt; BP = 24.654, df = 1, p-value = 6.86e-07\n\n# Teste de autocorrelação\ndwtest(mod1)\n\n#&gt; \n#&gt;  Durbin-Watson test\n#&gt; \n#&gt; data:  mod1\n#&gt; DW = 2.0232, p-value = 0.5506\n#&gt; alternative hypothesis: true autocorrelation is greater than 0",
    "crumbs": [
      "Conteúdo Semanal",
      "Regressão Simples"
    ]
  },
  {
    "objectID": "semanas/Aula07.1.html#o-modelo-de-regressão",
    "href": "semanas/Aula07.1.html#o-modelo-de-regressão",
    "title": "Regressão Linear",
    "section": "2o Modelo de Regressão",
    "text": "2o Modelo de Regressão\n\nmod2 &lt;- lm( Vendas ~ Radio, data = conj_treino)\ncoeflinear &lt;- mod2$coefficients[1]\ncoefang &lt;- mod2$coefficients[2]\nsummary(mod2)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = Vendas ~ Radio, data = conj_treino)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -16.396  -1.851   0.675   2.522   7.451 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  9.22746    0.64377  14.333  &lt; 2e-16 ***\n#&gt; Radio        0.22144    0.02515   8.806 1.38e-14 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 3.908 on 117 degrees of freedom\n#&gt; Multiple R-squared:  0.3986, Adjusted R-squared:  0.3935 \n#&gt; F-statistic: 77.55 on 1 and 117 DF,  p-value: 1.384e-14\n\nggplot(propaganda, aes(x=Radio, y=Vendas)) +\n    geom_point() +\n    geom_abline(slope = coefang,intercept = coeflinear, color=\"blue\" ) + \n    geom_hline(yintercept=coeflinear, linetype=\"dashed\", color = \"red\")",
    "crumbs": [
      "Conteúdo Semanal",
      "Regressão Simples"
    ]
  },
  {
    "objectID": "semanas/Aula07.1.html#outra-forma-de-representação-do-2o-modelo",
    "href": "semanas/Aula07.1.html#outra-forma-de-representação-do-2o-modelo",
    "title": "Regressão Linear",
    "section": "Outra forma de representação do 2o Modelo",
    "text": "Outra forma de representação do 2o Modelo\n\nscatterplot(Vendas ~ Radio, data = conj_treino, smooth=F)",
    "crumbs": [
      "Conteúdo Semanal",
      "Regressão Simples"
    ]
  },
  {
    "objectID": "semanas/Aula07.1.html#calculando-o-erro-padrão-do-resíduo-com-amostra-de-teste-1",
    "href": "semanas/Aula07.1.html#calculando-o-erro-padrão-do-resíduo-com-amostra-de-teste-1",
    "title": "Regressão Linear",
    "section": "Calculando o erro padrão do resíduo com amostra de teste",
    "text": "Calculando o erro padrão do resíduo com amostra de teste\n\n## Erro com conjunto de teste\nsqrt(mean((conj_teste$Vendas - predict(mod2, conj_teste)) ^ 2)) \n\n#&gt; [1] 4.808117\n\n## Error com conjunto de treino\nsummary(mod2)$sigma\n\n#&gt; [1] 3.908233",
    "crumbs": [
      "Conteúdo Semanal",
      "Regressão Simples"
    ]
  },
  {
    "objectID": "semanas/Aula07.1.html#análise-dos-resíduos-do-2o-modelo",
    "href": "semanas/Aula07.1.html#análise-dos-resíduos-do-2o-modelo",
    "title": "Regressão Linear",
    "section": "Análise dos resíduos do 2o modelo",
    "text": "Análise dos resíduos do 2o modelo\n\nplot(mod2)",
    "crumbs": [
      "Conteúdo Semanal",
      "Regressão Simples"
    ]
  },
  {
    "objectID": "semanas/Aula07.1.html#o-modelo-de-regressão-1",
    "href": "semanas/Aula07.1.html#o-modelo-de-regressão-1",
    "title": "Regressão Linear",
    "section": "3o Modelo de Regressão",
    "text": "3o Modelo de Regressão\n\nmod3 &lt;- lm( Vendas ~ Jornal, data = conj_treino)\ncoeflinear &lt;- mod3$coefficients[1]\ncoefang &lt;- mod3$coefficients[2]\nsummary(mod3)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = Vendas ~ Jornal, data = conj_treino)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -11.0331  -3.2104  -0.8491   3.0389  13.0002 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept) 12.08457    0.80845  14.948  &lt; 2e-16 ***\n#&gt; Jornal       0.06305    0.02290   2.753  0.00685 ** \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 4.884 on 117 degrees of freedom\n#&gt; Multiple R-squared:  0.06084,    Adjusted R-squared:  0.05281 \n#&gt; F-statistic: 7.579 on 1 and 117 DF,  p-value: 0.006847\n\nggplot(propaganda, aes(x=Jornal, y=Vendas)) +\n    geom_point() +\n    geom_abline(slope = coefang,intercept = coeflinear, color=\"blue\" ) + \n    geom_hline(yintercept=coeflinear, linetype=\"dashed\", color = \"red\")",
    "crumbs": [
      "Conteúdo Semanal",
      "Regressão Simples"
    ]
  },
  {
    "objectID": "semanas/Aula07.1.html#outra-forma-de-representação-do-3o-modelo",
    "href": "semanas/Aula07.1.html#outra-forma-de-representação-do-3o-modelo",
    "title": "Regressão Linear",
    "section": "Outra forma de representação do 3o Modelo",
    "text": "Outra forma de representação do 3o Modelo\n\nscatterplot(Vendas ~ Jornal, data = conj_treino, smooth=F)",
    "crumbs": [
      "Conteúdo Semanal",
      "Regressão Simples"
    ]
  },
  {
    "objectID": "semanas/Aula07.1.html#calculando-o-erro-padrão-do-resíduo-com-amostra-de-teste-2",
    "href": "semanas/Aula07.1.html#calculando-o-erro-padrão-do-resíduo-com-amostra-de-teste-2",
    "title": "Regressão Linear",
    "section": "Calculando o erro padrão do resíduo com amostra de teste",
    "text": "Calculando o erro padrão do resíduo com amostra de teste\n\n## Erro com conjunto de teste\nsqrt(mean((conj_teste$Vendas - predict(mod3, conj_teste)) ^ 2)) \n\n#&gt; [1] 5.38693\n\n## Error com conjunto de treino\nsummary(mod3)$sigma\n\n#&gt; [1] 4.884027",
    "crumbs": [
      "Conteúdo Semanal",
      "Regressão Simples"
    ]
  },
  {
    "objectID": "semanas/Aula07.1.html#análise-dos-resíduos-do-3o-modelo",
    "href": "semanas/Aula07.1.html#análise-dos-resíduos-do-3o-modelo",
    "title": "Regressão Linear",
    "section": "Análise dos resíduos do 3o modelo",
    "text": "Análise dos resíduos do 3o modelo\n\nplot(mod3)",
    "crumbs": [
      "Conteúdo Semanal",
      "Regressão Simples"
    ]
  },
  {
    "objectID": "semanas/Aula06.html",
    "href": "semanas/Aula06.html",
    "title": "Eixos - Escalas - Cores",
    "section": "",
    "text": "library(tidyverse)\nlibrary(gapminder)",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 06"
    ]
  },
  {
    "objectID": "semanas/Aula06.html#carregando-bibliotecas",
    "href": "semanas/Aula06.html#carregando-bibliotecas",
    "title": "Eixos - Escalas - Cores",
    "section": "",
    "text": "library(tidyverse)\nlibrary(gapminder)",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 06"
    ]
  },
  {
    "objectID": "semanas/Aula06.html#filtrando-dados-e-fazendo-grafico-de-dispersão-padrão",
    "href": "semanas/Aula06.html#filtrando-dados-e-fazendo-grafico-de-dispersão-padrão",
    "title": "Eixos - Escalas - Cores",
    "section": "Filtrando dados e fazendo grafico de dispersão padrão",
    "text": "Filtrando dados e fazendo grafico de dispersão padrão\n\ngap_07 &lt;- filter(gapminder, year == 2007)\nggplot(gap_07, aes(x = gdpPercap, y = lifeExp)) +\n  geom_point()",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 06"
    ]
  },
  {
    "objectID": "semanas/Aula06.html#transformando-o-eixo-x-para-escala-logarítimica",
    "href": "semanas/Aula06.html#transformando-o-eixo-x-para-escala-logarítimica",
    "title": "Eixos - Escalas - Cores",
    "section": "Transformando o eixo x para escala logarítimica",
    "text": "Transformando o eixo x para escala logarítimica\n\nggplot(gap_07, aes(x = gdpPercap, y = lifeExp)) +\n  geom_point() +\n  scale_x_continuous(trans = \"log10\")",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 06"
    ]
  },
  {
    "objectID": "semanas/Aula06.html#outra-forma-de-transformação-do-eixo-x",
    "href": "semanas/Aula06.html#outra-forma-de-transformação-do-eixo-x",
    "title": "Eixos - Escalas - Cores",
    "section": "Outra forma de transformação do eixo x",
    "text": "Outra forma de transformação do eixo x\n\nggplot(gap_07, aes(x = gdpPercap, y = lifeExp)) +\n  geom_point() +\n  scale_x_log10()",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 06"
    ]
  },
  {
    "objectID": "semanas/Aula06.html#definindo-limites-para-o-eixo-y",
    "href": "semanas/Aula06.html#definindo-limites-para-o-eixo-y",
    "title": "Eixos - Escalas - Cores",
    "section": "Definindo limites para o eixo y",
    "text": "Definindo limites para o eixo y\n\nggplot(gap_07, aes(x = gdpPercap, y = lifeExp)) +\n  geom_point() +\n  scale_x_log10() +\n  scale_y_continuous(limits = c(0, 95))",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 06"
    ]
  },
  {
    "objectID": "semanas/Aula06.html#grafico-com-cores-normais",
    "href": "semanas/Aula06.html#grafico-com-cores-normais",
    "title": "Eixos - Escalas - Cores",
    "section": "Grafico com cores normais",
    "text": "Grafico com cores normais\n\nggplot(gap_07, aes(x = gdpPercap, y = lifeExp, color = continent)) +\n  geom_point() +\n  scale_x_log10()",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 06"
    ]
  },
  {
    "objectID": "semanas/Aula06.html#grafico-usando-uma-paleta-de-cores-diferente",
    "href": "semanas/Aula06.html#grafico-usando-uma-paleta-de-cores-diferente",
    "title": "Eixos - Escalas - Cores",
    "section": "Grafico usando uma paleta de cores diferente",
    "text": "Grafico usando uma paleta de cores diferente\n\nggplot(gap_07, aes(x = gdpPercap, y = lifeExp, color = continent)) +\n  geom_point() +\n  scale_x_log10() +\n  scale_color_brewer(palette = \"Dark2\")",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 06"
    ]
  },
  {
    "objectID": "semanas/Aula06.html#usando-codigos-manuais-para-as-cores",
    "href": "semanas/Aula06.html#usando-codigos-manuais-para-as-cores",
    "title": "Eixos - Escalas - Cores",
    "section": "Usando codigos manuais para as cores",
    "text": "Usando codigos manuais para as cores\n\nggplot(gap_07, aes(x = gdpPercap, y = lifeExp, color = continent)) +\n  geom_point() +\n  scale_x_log10() +\n  scale_color_manual(values = c(\"#FF0000\", \"#00A08A\", \"#F2AD00\",\n                                \"#F98400\", \"#5BBCD6\"))",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 06"
    ]
  },
  {
    "objectID": "semanas/Aula06.html#definindo-as-cores-e-tamanho-dos-pontos",
    "href": "semanas/Aula06.html#definindo-as-cores-e-tamanho-dos-pontos",
    "title": "Eixos - Escalas - Cores",
    "section": "Definindo as cores e tamanho dos pontos",
    "text": "Definindo as cores e tamanho dos pontos\n\nggplot(gap_07, aes(x = gdpPercap, y = lifeExp)) +\n  geom_point(color = \"darkblue\", size = 3) +\n  scale_x_log10()",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 06"
    ]
  },
  {
    "objectID": "semanas/Aula06.html#customizando-títulos-rótulos-de-eixo-e-legendas",
    "href": "semanas/Aula06.html#customizando-títulos-rótulos-de-eixo-e-legendas",
    "title": "Eixos - Escalas - Cores",
    "section": "Customizando títulos, rótulos de eixo e legendas",
    "text": "Customizando títulos, rótulos de eixo e legendas\n\nggplot(gap_07, aes(x = gdpPercap, y = lifeExp, color = continent)) +\n  geom_point(size = 2) +\n  scale_x_log10() +\n  theme_light() +\n  theme(legend.position = \"bottom\")",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 06"
    ]
  },
  {
    "objectID": "semanas/Aula06.html#sem-legenda",
    "href": "semanas/Aula06.html#sem-legenda",
    "title": "Eixos - Escalas - Cores",
    "section": "Sem legenda",
    "text": "Sem legenda\n\nggplot(gap_07, aes(x = gdpPercap, y = lifeExp, color = continent)) +\n  geom_point(size = 2) +\n  scale_x_log10() +\n  theme_light() +\n  theme(legend.position = \"none\")",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 06"
    ]
  },
  {
    "objectID": "semanas/Aula06.html#legenda-dentro-do-gráfico",
    "href": "semanas/Aula06.html#legenda-dentro-do-gráfico",
    "title": "Eixos - Escalas - Cores",
    "section": "Legenda dentro do gráfico",
    "text": "Legenda dentro do gráfico\n\nggplot(gap_07, aes(x = gdpPercap, y = lifeExp, color = continent)) +\n  geom_point(size = 2) +\n  scale_x_log10() +\n  theme_light() +\n  theme(legend.position = c(0.1, 0.85))\n\n#&gt; Warning: A numeric `legend.position` argument in `theme()` was deprecated in ggplot2\n#&gt; 3.5.0.\n#&gt; ℹ Please use the `legend.position.inside` argument of `theme()` instead.",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 06"
    ]
  },
  {
    "objectID": "semanas/Aula06.html#aumentando-o-tamanho-do-texto-e-mudando-para-portugues",
    "href": "semanas/Aula06.html#aumentando-o-tamanho-do-texto-e-mudando-para-portugues",
    "title": "Eixos - Escalas - Cores",
    "section": "Aumentando o tamanho do texto e mudando para portugues",
    "text": "Aumentando o tamanho do texto e mudando para portugues\n\ngraf2 &lt;- ggplot(gap_07, aes(x = gdpPercap, y = lifeExp, color = continent)) +\n  geom_point(size = 2) +\n  scale_x_log10() +\n  theme_light() +\n  theme(legend.position = c(0.1, 0.85),\n        legend.key = element_blank(),\n        axis.text = element_text(size = 12),\n        axis.title = element_text(size = 14)) +\n  labs(x = \"PIB Per capita (US$)\", \n       y = \"Expectativa de Vida (anos)\", \n       title = \"Expectativa de Vida vs PIB em 2007\",\n       color = \"Continente\")\n\ngraf2",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 06"
    ]
  },
  {
    "objectID": "semanas/Aula04.html",
    "href": "semanas/Aula04.html",
    "title": "Melhorando a visualização",
    "section": "",
    "text": "library(\"ggplot2\")\nlibrary(\"dplyr\")\nlibrary(\"gapminder\")",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 04"
    ]
  },
  {
    "objectID": "semanas/Aula04.html#carregando-as-bibliotecas",
    "href": "semanas/Aula04.html#carregando-as-bibliotecas",
    "title": "Melhorando a visualização",
    "section": "",
    "text": "library(\"ggplot2\")\nlibrary(\"dplyr\")\nlibrary(\"gapminder\")",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 04"
    ]
  },
  {
    "objectID": "semanas/Aula04.html#cores-por-continente",
    "href": "semanas/Aula04.html#cores-por-continente",
    "title": "Melhorando a visualização",
    "section": "Cores por continente",
    "text": "Cores por continente\n\ngap_07 &lt;- filter(gapminder, year == 2007)\nggplot(gap_07, aes(x = gdpPercap, y = lifeExp,\n                   color = continent)) +\n  geom_point()",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 04"
    ]
  },
  {
    "objectID": "semanas/Aula04.html#usando-formas-e-cores-diferentes",
    "href": "semanas/Aula04.html#usando-formas-e-cores-diferentes",
    "title": "Melhorando a visualização",
    "section": "Usando formas e cores diferentes",
    "text": "Usando formas e cores diferentes\n\nggplot(gap_07, aes(x = gdpPercap, y = lifeExp,\n                   shape = continent, color = continent)) +\n  geom_point()",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 04"
    ]
  },
  {
    "objectID": "semanas/Aula04.html#cores-e-tamanho",
    "href": "semanas/Aula04.html#cores-e-tamanho",
    "title": "Melhorando a visualização",
    "section": "Cores e tamanho",
    "text": "Cores e tamanho\n\nggplot(gap_07, aes(x = gdpPercap, y = lifeExp,\n                   size = pop, color = continent)) +\n  geom_point()",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 04"
    ]
  },
  {
    "objectID": "semanas/Aula04.html#sumario-dos-dados-para-obter-pop-média-por-continente",
    "href": "semanas/Aula04.html#sumario-dos-dados-para-obter-pop-média-por-continente",
    "title": "Melhorando a visualização",
    "section": "Sumario dos dados para obter pop média por continente",
    "text": "Sumario dos dados para obter pop média por continente\n\ngap_pop &lt;- gapminder %&gt;% \n  group_by(continent, year) %&gt;% \n  summarize(pop = mean(pop))\n\n#&gt; `summarise()` has grouped output by 'continent'. You can override using the\n#&gt; `.groups` argument.\n\nhead(gap_pop)\n\n#&gt; # A tibble: 6 × 3\n#&gt; # Groups:   continent [1]\n#&gt;   continent  year      pop\n#&gt;   &lt;fct&gt;     &lt;int&gt;    &lt;dbl&gt;\n#&gt; 1 Africa     1952 4570010.\n#&gt; 2 Africa     1957 5093033.\n#&gt; 3 Africa     1962 5702247.\n#&gt; 4 Africa     1967 6447875.\n#&gt; 5 Africa     1972 7305376.\n#&gt; 6 Africa     1977 8328097.",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 04"
    ]
  },
  {
    "objectID": "semanas/Aula04.html#grafico-de-linha-com-cores",
    "href": "semanas/Aula04.html#grafico-de-linha-com-cores",
    "title": "Melhorando a visualização",
    "section": "Grafico de linha com cores",
    "text": "Grafico de linha com cores\n\nggplot(gap_pop, aes(x = year, y = pop, color = continent)) +\n  geom_line() + geom_point()",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 04"
    ]
  },
  {
    "objectID": "semanas/Aula04.html#criando-grids-entre-os-anos-de-2002-e-2007",
    "href": "semanas/Aula04.html#criando-grids-entre-os-anos-de-2002-e-2007",
    "title": "Melhorando a visualização",
    "section": "Criando grids entre os anos de 2002 e 2007",
    "text": "Criando grids entre os anos de 2002 e 2007\n\ngap_0207 &lt;- gapminder %&gt;% filter(between(year, 2002, 2007))\nggplot(gap_0207, aes(x = gdpPercap, y = lifeExp)) +\n  geom_point() +\n  facet_grid(continent ~ year)",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 04"
    ]
  },
  {
    "objectID": "semanas/Aula04.html#outro-tipo-de-apresentações-de-grid",
    "href": "semanas/Aula04.html#outro-tipo-de-apresentações-de-grid",
    "title": "Melhorando a visualização",
    "section": "Outro tipo de apresentações de grid",
    "text": "Outro tipo de apresentações de grid\n\ngap_life &lt;- gapminder %&gt;% \n  group_by(continent, year) %&gt;% \n  summarize(lifeExp = mean(lifeExp))\n\n#&gt; `summarise()` has grouped output by 'continent'. You can override using the\n#&gt; `.groups` argument.\n\nggplot(gap_life, aes(x = year, y = lifeExp)) +\n  geom_line() +\n  facet_grid(continent ~ .)",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 04"
    ]
  },
  {
    "objectID": "semanas/Aula04.html#grid-em-outra-direção",
    "href": "semanas/Aula04.html#grid-em-outra-direção",
    "title": "Melhorando a visualização",
    "section": "Grid em outra direção",
    "text": "Grid em outra direção\n\nggplot(gap_life, aes(x = year, y = lifeExp)) +\n  geom_line() +\n  facet_grid(. ~ continent)",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 04"
    ]
  },
  {
    "objectID": "semanas/Aula04.html#usando-o-wrap",
    "href": "semanas/Aula04.html#usando-o-wrap",
    "title": "Melhorando a visualização",
    "section": "Usando o wrap",
    "text": "Usando o wrap\n\nggplot(gap_life, aes(x = year, y = lifeExp)) +\n  geom_line() +\n  facet_wrap( ~ continent)",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 04"
    ]
  },
  {
    "objectID": "semanas/Aula04.html#filtrando-dados-e-fazendo-grafico-de-dispersão-padrão",
    "href": "semanas/Aula04.html#filtrando-dados-e-fazendo-grafico-de-dispersão-padrão",
    "title": "Melhorando a visualização",
    "section": "Filtrando dados e fazendo grafico de dispersão padrão",
    "text": "Filtrando dados e fazendo grafico de dispersão padrão\n\ngap_07 &lt;- filter(gapminder, year == 2007)\nggplot(gap_07, aes(x = gdpPercap, y = lifeExp)) +\n  geom_point()",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 04"
    ]
  },
  {
    "objectID": "semanas/Aula04.html#transformando-o-eixo-x-para-escala-logarítimica",
    "href": "semanas/Aula04.html#transformando-o-eixo-x-para-escala-logarítimica",
    "title": "Melhorando a visualização",
    "section": "Transformando o eixo x para escala logarítimica",
    "text": "Transformando o eixo x para escala logarítimica\n\nggplot(gap_07, aes(x = gdpPercap, y = lifeExp)) +\n  geom_point() +\n  scale_x_continuous(trans = \"log10\")",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 04"
    ]
  },
  {
    "objectID": "semanas/Aula04.html#outra-forma-de-transformação-do-eixo-x",
    "href": "semanas/Aula04.html#outra-forma-de-transformação-do-eixo-x",
    "title": "Melhorando a visualização",
    "section": "Outra forma de transformação do eixo x",
    "text": "Outra forma de transformação do eixo x\n\nggplot(gap_07, aes(x = gdpPercap, y = lifeExp)) +\n  geom_point() +\n  scale_x_log10()",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 04"
    ]
  },
  {
    "objectID": "semanas/Aula04.html#definindo-limites-para-o-eixo-y",
    "href": "semanas/Aula04.html#definindo-limites-para-o-eixo-y",
    "title": "Melhorando a visualização",
    "section": "Definindo limites para o eixo y",
    "text": "Definindo limites para o eixo y\n\nggplot(gap_07, aes(x = gdpPercap, y = lifeExp)) +\n  geom_point() +\n  scale_x_log10() +\n  scale_y_continuous(limits = c(0, 95))",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 04"
    ]
  },
  {
    "objectID": "semanas/Aula04.html#grafico-com-cores-normais",
    "href": "semanas/Aula04.html#grafico-com-cores-normais",
    "title": "Melhorando a visualização",
    "section": "Grafico com cores normais",
    "text": "Grafico com cores normais\n\nggplot(gap_07, aes(x = gdpPercap, y = lifeExp, color = continent)) +\n  geom_point() +\n  scale_x_log10()",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 04"
    ]
  },
  {
    "objectID": "semanas/Aula04.html#grafico-usando-outra-paleta-de-cores",
    "href": "semanas/Aula04.html#grafico-usando-outra-paleta-de-cores",
    "title": "Melhorando a visualização",
    "section": "Grafico usando outra paleta de cores",
    "text": "Grafico usando outra paleta de cores\n\nggplot(gap_07, aes(x = gdpPercap, y = lifeExp, color = continent)) +\n  geom_point() +\n  scale_x_log10() +\n  scale_color_brewer(palette = \"Dark2\")",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 04"
    ]
  },
  {
    "objectID": "semanas/Aula04.html#usando-codigos-manuais-para-as-cores",
    "href": "semanas/Aula04.html#usando-codigos-manuais-para-as-cores",
    "title": "Melhorando a visualização",
    "section": "Usando codigos manuais para as cores",
    "text": "Usando codigos manuais para as cores\n\nggplot(gap_07, aes(x = gdpPercap, y = lifeExp, color = continent)) +\n  geom_point() +\n  scale_x_log10() +\n  scale_color_manual(values = c(\"#FF0000\", \"#00A08A\", \"#F2AD00\",\n                                \"#F98400\", \"#5BBCD6\"))",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 04"
    ]
  },
  {
    "objectID": "semanas/Aula04.html#definindo-as-cores-e-tamanho-dos-pontos",
    "href": "semanas/Aula04.html#definindo-as-cores-e-tamanho-dos-pontos",
    "title": "Melhorando a visualização",
    "section": "Definindo as cores e tamanho dos pontos",
    "text": "Definindo as cores e tamanho dos pontos\n\nggplot(gap_07, aes(x = gdpPercap, y = lifeExp)) +\n  geom_point(color = \"darkblue\", size = 3) +\n  scale_x_log10()",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 04"
    ]
  },
  {
    "objectID": "semanas/Aula04.html#customizando-títulos-rótulos-de-eixo-e-legendas",
    "href": "semanas/Aula04.html#customizando-títulos-rótulos-de-eixo-e-legendas",
    "title": "Melhorando a visualização",
    "section": "Customizando títulos, rótulos de eixo e legendas",
    "text": "Customizando títulos, rótulos de eixo e legendas\n\nggplot(gap_07, aes(x = gdpPercap, y = lifeExp, color = continent)) +\n  geom_point(size = 2) +\n  scale_x_log10() +\n  theme_light() +\n  theme(legend.position = \"bottom\")",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 04"
    ]
  },
  {
    "objectID": "semanas/Aula04.html#sem-legenda",
    "href": "semanas/Aula04.html#sem-legenda",
    "title": "Melhorando a visualização",
    "section": "Sem legenda",
    "text": "Sem legenda\n\nggplot(gap_07, aes(x = gdpPercap, y = lifeExp, color = continent)) +\n  geom_point(size = 2) +\n  scale_x_log10() +\n  theme_light() +\n  theme(legend.position = \"none\")",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 04"
    ]
  },
  {
    "objectID": "semanas/Aula04.html#legenda-dentro-do-gráfico",
    "href": "semanas/Aula04.html#legenda-dentro-do-gráfico",
    "title": "Melhorando a visualização",
    "section": "Legenda dentro do gráfico",
    "text": "Legenda dentro do gráfico\n\nggplot(gap_07, aes(x = gdpPercap, y = lifeExp, color = continent)) +\n  geom_point(size = 2) +\n  scale_x_log10() +\n  theme_light() +\n  theme(legend.position = c(0.1, 0.85))\n\n#&gt; Warning: A numeric `legend.position` argument in `theme()` was deprecated in ggplot2\n#&gt; 3.5.0.\n#&gt; ℹ Please use the `legend.position.inside` argument of `theme()` instead.",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 04"
    ]
  },
  {
    "objectID": "semanas/Aula04.html#salvando-o-gráfico",
    "href": "semanas/Aula04.html#salvando-o-gráfico",
    "title": "Melhorando a visualização",
    "section": "Salvando o gráfico",
    "text": "Salvando o gráfico\n\ngraf1 &lt;- ggplot(gap_07, aes(x = gdpPercap, y = lifeExp, color = continent)) +\n  geom_point(size = 2) +\n  scale_x_log10() +\n  theme_light() +\n  theme(legend.position = c(0.1, 0.85))\n\ngraf1",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 04"
    ]
  },
  {
    "objectID": "semanas/Aula04.html#aumentando-o-tamanho-do-texto-e-mudando-para-portugues",
    "href": "semanas/Aula04.html#aumentando-o-tamanho-do-texto-e-mudando-para-portugues",
    "title": "Melhorando a visualização",
    "section": "Aumentando o tamanho do texto e mudando para portugues",
    "text": "Aumentando o tamanho do texto e mudando para portugues\n\ngraf2 &lt;- ggplot(gap_07, aes(x = gdpPercap, y = lifeExp, color = continent)) +\n  geom_point(size = 2) +\n  scale_x_log10() +\n  theme_light() +\n  theme(legend.position = c(0.1, 0.80),\n        legend.key = element_blank(),\n        axis.text = element_text(size = 12),\n        axis.title = element_text(size = 14)) +\n  labs(x = \"PIB Per capita (US$)\", \n       y = \"Expectativa de Vida (anos)\", \n       title = \"Expectativa de Vida vs PIB em 2007\",\n       color = \"Continente\")\n\ngraf2",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 04"
    ]
  },
  {
    "objectID": "semanas/Aula02.html",
    "href": "semanas/Aula02.html",
    "title": "Manipulação dos dados",
    "section": "",
    "text": "Vamos trabalhar de novo com os dados do gapminder para fazer algumas manipulações de dados que são muito úteis no dia a dia.\nEste material foi em parte adaptado do curso Data Visualization in R with ggplot2 de Kara Woo\n\nlibrary(\"gapminder\")\nlibrary(\"dplyr\")\nlibrary(\"ggplot2\")\nlibrary(\"patchwork\")",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 02"
    ]
  },
  {
    "objectID": "semanas/Aula02.html#carregando-bibliotescas",
    "href": "semanas/Aula02.html#carregando-bibliotescas",
    "title": "Manipulação dos dados",
    "section": "",
    "text": "Vamos trabalhar de novo com os dados do gapminder para fazer algumas manipulações de dados que são muito úteis no dia a dia.\nEste material foi em parte adaptado do curso Data Visualization in R with ggplot2 de Kara Woo\n\nlibrary(\"gapminder\")\nlibrary(\"dplyr\")\nlibrary(\"ggplot2\")\nlibrary(\"patchwork\")",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 02"
    ]
  },
  {
    "objectID": "semanas/Aula02.html#filtrando-2007",
    "href": "semanas/Aula02.html#filtrando-2007",
    "title": "Manipulação dos dados",
    "section": "Filtrando 2007",
    "text": "Filtrando 2007\n\n## Cria um extrato do ano de 2007\ndata(gapminder)\nsummary(gapminder)\n\n#&gt;         country        continent        year         lifeExp     \n#&gt;  Afghanistan:  12   Africa  :624   Min.   :1952   Min.   :23.60  \n#&gt;  Albania    :  12   Americas:300   1st Qu.:1966   1st Qu.:48.20  \n#&gt;  Algeria    :  12   Asia    :396   Median :1980   Median :60.71  \n#&gt;  Angola     :  12   Europe  :360   Mean   :1980   Mean   :59.47  \n#&gt;  Argentina  :  12   Oceania : 24   3rd Qu.:1993   3rd Qu.:70.85  \n#&gt;  Australia  :  12                  Max.   :2007   Max.   :82.60  \n#&gt;  (Other)    :1632                                                \n#&gt;       pop              gdpPercap       \n#&gt;  Min.   :6.001e+04   Min.   :   241.2  \n#&gt;  1st Qu.:2.794e+06   1st Qu.:  1202.1  \n#&gt;  Median :7.024e+06   Median :  3531.8  \n#&gt;  Mean   :2.960e+07   Mean   :  7215.3  \n#&gt;  3rd Qu.:1.959e+07   3rd Qu.:  9325.5  \n#&gt;  Max.   :1.319e+09   Max.   :113523.1  \n#&gt; \n\ngap_07 &lt;- filter(gapminder, year == 2007)",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 02"
    ]
  },
  {
    "objectID": "semanas/Aula02.html#vendo-primeiras-e-últimas-10-linhas",
    "href": "semanas/Aula02.html#vendo-primeiras-e-últimas-10-linhas",
    "title": "Manipulação dos dados",
    "section": "Vendo primeiras e últimas 10 linhas",
    "text": "Vendo primeiras e últimas 10 linhas\n\nhead(gap_07, n=10) %&gt;% knitr::kable(booktabs = TRUE) # primeiros dez paises da base de dados\n\n\n\ncountry\ncontinent\nyear\nlifeExp\npop\ngdpPercap\n\n\n\nAfghanistan\nAsia\n2007\n43.828\n31889923\n974.5803\n\n\nAlbania\nEurope\n2007\n76.423\n3600523\n5937.0295\n\n\nAlgeria\nAfrica\n2007\n72.301\n33333216\n6223.3675\n\n\nAngola\nAfrica\n2007\n42.731\n12420476\n4797.2313\n\n\nArgentina\nAmericas\n2007\n75.320\n40301927\n12779.3796\n\n\nAustralia\nOceania\n2007\n81.235\n20434176\n34435.3674\n\n\nAustria\nEurope\n2007\n79.829\n8199783\n36126.4927\n\n\nBahrain\nAsia\n2007\n75.635\n708573\n29796.0483\n\n\nBangladesh\nAsia\n2007\n64.062\n150448339\n1391.2538\n\n\nBelgium\nEurope\n2007\n79.441\n10392226\n33692.6051\n\n\n\n\ntail(gap_07, n=10) %&gt;% knitr::kable(booktabs = TRUE) # últimos 10 países \n\n\n\ncountry\ncontinent\nyear\nlifeExp\npop\ngdpPercap\n\n\n\nUganda\nAfrica\n2007\n51.542\n29170398\n1056.3801\n\n\nUnited Kingdom\nEurope\n2007\n79.425\n60776238\n33203.2613\n\n\nUnited States\nAmericas\n2007\n78.242\n301139947\n42951.6531\n\n\nUruguay\nAmericas\n2007\n76.384\n3447496\n10611.4630\n\n\nVenezuela\nAmericas\n2007\n73.747\n26084662\n11415.8057\n\n\nVietnam\nAsia\n2007\n74.249\n85262356\n2441.5764\n\n\nWest Bank and Gaza\nAsia\n2007\n73.422\n4018332\n3025.3498\n\n\nYemen, Rep.\nAsia\n2007\n62.698\n22211743\n2280.7699\n\n\nZambia\nAfrica\n2007\n42.384\n11746035\n1271.2116\n\n\nZimbabwe\nAfrica\n2007\n43.487\n12311143\n469.7093",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 02"
    ]
  },
  {
    "objectID": "semanas/Aula02.html#manipulando-1",
    "href": "semanas/Aula02.html#manipulando-1",
    "title": "Manipulação dos dados",
    "section": "Manipulando 1",
    "text": "Manipulando 1\nSelecionando dados por país\n\nfilter(gap_07, country %in% c(\"Brazil\", \"Chile\"))\n\n#&gt; # A tibble: 2 × 6\n#&gt;   country continent  year lifeExp       pop gdpPercap\n#&gt;   &lt;fct&gt;   &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;     &lt;int&gt;     &lt;dbl&gt;\n#&gt; 1 Brazil  Americas   2007    72.4 190010647     9066.\n#&gt; 2 Chile   Americas   2007    78.6  16284741    13172.",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 02"
    ]
  },
  {
    "objectID": "semanas/Aula02.html#manipulando-2",
    "href": "semanas/Aula02.html#manipulando-2",
    "title": "Manipulação dos dados",
    "section": "Manipulando 2",
    "text": "Manipulando 2\nSelecionando dados para 2007 excluindo a Oceania\n\nfilter(gapminder, year == 2007 & continent != \"Oceania\")\n\n#&gt; # A tibble: 140 × 6\n#&gt;    country     continent  year lifeExp       pop gdpPercap\n#&gt;    &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;     &lt;int&gt;     &lt;dbl&gt;\n#&gt;  1 Afghanistan Asia       2007    43.8  31889923      975.\n#&gt;  2 Albania     Europe     2007    76.4   3600523     5937.\n#&gt;  3 Algeria     Africa     2007    72.3  33333216     6223.\n#&gt;  4 Angola      Africa     2007    42.7  12420476     4797.\n#&gt;  5 Argentina   Americas   2007    75.3  40301927    12779.\n#&gt;  6 Austria     Europe     2007    79.8   8199783    36126.\n#&gt;  7 Bahrain     Asia       2007    75.6    708573    29796.\n#&gt;  8 Bangladesh  Asia       2007    64.1 150448339     1391.\n#&gt;  9 Belgium     Europe     2007    79.4  10392226    33693.\n#&gt; 10 Benin       Africa     2007    56.7   8078314     1441.\n#&gt; # ℹ 130 more rows",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 02"
    ]
  },
  {
    "objectID": "semanas/Aula02.html#manipulando-3",
    "href": "semanas/Aula02.html#manipulando-3",
    "title": "Manipulação dos dados",
    "section": "Manipulando 3",
    "text": "Manipulando 3\nSelecionando dados de 2007, agrupando por continente e sumarizando para achar a média da população por continente\n\ngapminder %&gt;%\n  filter(year == 2007) %&gt;% \n  group_by(continent) %&gt;% \n  summarize(mediapop = mean(pop))\n\n#&gt; # A tibble: 5 × 2\n#&gt;   continent   mediapop\n#&gt;   &lt;fct&gt;          &lt;dbl&gt;\n#&gt; 1 Africa     17875763.\n#&gt; 2 Americas   35954847.\n#&gt; 3 Asia      115513752.\n#&gt; 4 Europe     19536618.\n#&gt; 5 Oceania    12274974.",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 02"
    ]
  },
  {
    "objectID": "semanas/Aula02.html#visualizando-1",
    "href": "semanas/Aula02.html#visualizando-1",
    "title": "Manipulação dos dados",
    "section": "Visualizando 1",
    "text": "Visualizando 1\nMostrar linhas e pontos do PIB ao longo do tempo para Brasil e Chile\n\ngap_brachi &lt;- filter(gapminder, country %in% c(\"Brazil\", \"Chile\"))\np &lt;- ggplot(gap_brachi, aes(x = year, y = gdpPercap, color=country))\np1 &lt;- p + geom_point()\np1",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 02"
    ]
  },
  {
    "objectID": "semanas/Aula02.html#visualizando-2",
    "href": "semanas/Aula02.html#visualizando-2",
    "title": "Manipulação dos dados",
    "section": "Visualizando 2",
    "text": "Visualizando 2\n\np2 &lt;- p + geom_line()\np2",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 02"
    ]
  },
  {
    "objectID": "semanas/Aula02.html#visualizando-3",
    "href": "semanas/Aula02.html#visualizando-3",
    "title": "Manipulação dos dados",
    "section": "Visualizando 3",
    "text": "Visualizando 3\n\np3 &lt;- p + geom_point() + geom_line()\np3",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 02"
    ]
  },
  {
    "objectID": "semanas/Aula02.html#visualizando-4---usando-o-pacote-patchwork",
    "href": "semanas/Aula02.html#visualizando-4---usando-o-pacote-patchwork",
    "title": "Manipulação dos dados",
    "section": "Visualizando 4 - Usando o pacote patchwork",
    "text": "Visualizando 4 - Usando o pacote patchwork\n\n(p1 + p2) /\n  p3",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 02"
    ]
  },
  {
    "objectID": "semanas/Aula02.html#manipulando-4",
    "href": "semanas/Aula02.html#manipulando-4",
    "title": "Manipulação dos dados",
    "section": "Manipulando 4",
    "text": "Manipulando 4\nContando número de países e continentes com distinct\n\nnrow(gapminder) ## Esta não é a informação que eu quero\n\n#&gt; [1] 1704\n\nhead(gapminder)\n\n#&gt; # A tibble: 6 × 6\n#&gt;   country     continent  year lifeExp      pop gdpPercap\n#&gt;   &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n#&gt; 1 Afghanistan Asia       1952    28.8  8425333      779.\n#&gt; 2 Afghanistan Asia       1957    30.3  9240934      821.\n#&gt; 3 Afghanistan Asia       1962    32.0 10267083      853.\n#&gt; 4 Afghanistan Asia       1967    34.0 11537966      836.\n#&gt; 5 Afghanistan Asia       1972    36.1 13079460      740.\n#&gt; 6 Afghanistan Asia       1977    38.4 14880372      786.\n\nnrow(distinct(gapminder,country))\n\n#&gt; [1] 142\n\nnrow(distinct(gapminder, continent))\n\n#&gt; [1] 5",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 02"
    ]
  },
  {
    "objectID": "semanas/Aula02.html#fazendo-contagens-de-dados",
    "href": "semanas/Aula02.html#fazendo-contagens-de-dados",
    "title": "Manipulação dos dados",
    "section": "Fazendo contagens de dados",
    "text": "Fazendo contagens de dados\n\ngapminder %&gt;% filter(year == 2007) %&gt;% \n  group_by(continent) %&gt;% summarise(n = n())\n\n#&gt; # A tibble: 5 × 2\n#&gt;   continent     n\n#&gt;   &lt;fct&gt;     &lt;int&gt;\n#&gt; 1 Africa       52\n#&gt; 2 Americas     25\n#&gt; 3 Asia         33\n#&gt; 4 Europe       30\n#&gt; 5 Oceania       2",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 02"
    ]
  },
  {
    "objectID": "semanas/Aula02.html#mudando-orientação-dos-dados",
    "href": "semanas/Aula02.html#mudando-orientação-dos-dados",
    "title": "Manipulação dos dados",
    "section": "Mudando orientação dos dados",
    "text": "Mudando orientação dos dados\n\nlibrary(readxl)\npropaganda &lt;- read_excel(\"Propaganda.xlsx\")\nhead(propaganda)\n\n#&gt; # A tibble: 6 × 4\n#&gt;      TV Radio Newspaper Sales\n#&gt;   &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 230.   37.8      69.2  22.1\n#&gt; 2  44.5  39.3      45.1  10.4\n#&gt; 3  17.2  45.9      69.3   9.3\n#&gt; 4 152.   41.3      58.5  18.5\n#&gt; 5 181.   10.8      58.4  12.9\n#&gt; 6   8.7  48.9      75     7.2\n\npropaganda &lt;- propaganda %&gt;% rename(Jornal = Newspaper, Vendas = Sales)\npropaganda %&gt;% tidyr::pivot_longer(!Vendas, names_to=\"Midia\", values_to=\"Orcamento\")\n\n#&gt; # A tibble: 600 × 3\n#&gt;    Vendas Midia  Orcamento\n#&gt;     &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;\n#&gt;  1   22.1 TV         230. \n#&gt;  2   22.1 Radio       37.8\n#&gt;  3   22.1 Jornal      69.2\n#&gt;  4   10.4 TV          44.5\n#&gt;  5   10.4 Radio       39.3\n#&gt;  6   10.4 Jornal      45.1\n#&gt;  7    9.3 TV          17.2\n#&gt;  8    9.3 Radio       45.9\n#&gt;  9    9.3 Jornal      69.3\n#&gt; 10   18.5 TV         152. \n#&gt; # ℹ 590 more rows\n\n\n\nlibrary(readr)\npesquisa &lt;- read.csv(\"data_joined.csv\", header = T)\nhead(pesquisa)\n\n#&gt;   record_id month day year plot_id species_id sex hindfoot_length weight\n#&gt; 1         1     7  16 1977       2         NL   M              32     NA\n#&gt; 2        72     8  19 1977       2         NL   M              31     NA\n#&gt; 3       224     9  13 1977       2         NL                  NA     NA\n#&gt; 4       266    10  16 1977       2         NL                  NA     NA\n#&gt; 5       349    11  12 1977       2         NL                  NA     NA\n#&gt; 6       363    11  12 1977       2         NL                  NA     NA\n#&gt;     genus  species   taxa plot_type\n#&gt; 1 Neotoma albigula Rodent   Control\n#&gt; 2 Neotoma albigula Rodent   Control\n#&gt; 3 Neotoma albigula Rodent   Control\n#&gt; 4 Neotoma albigula Rodent   Control\n#&gt; 5 Neotoma albigula Rodent   Control\n#&gt; 6 Neotoma albigula Rodent   Control\n\npesquisa_gw &lt;- pesquisa %&gt;% filter(!is.na(weight)) %&gt;%\n  group_by(year, genus) %&gt;%\n  summarize(peso_medio = mean(weight))\n\n#&gt; `summarise()` has grouped output by 'year'. You can override using the\n#&gt; `.groups` argument.\n\nhead(pesquisa_gw)\n\n#&gt; # A tibble: 6 × 3\n#&gt; # Groups:   year [1]\n#&gt;    year genus           peso_medio\n#&gt;   &lt;int&gt; &lt;chr&gt;                &lt;dbl&gt;\n#&gt; 1  1977 Chaetodipus          15.3 \n#&gt; 2  1977 Dipodomys            52.5 \n#&gt; 3  1977 Onychomys            21.4 \n#&gt; 4  1977 Perognathus           7.17\n#&gt; 5  1977 Peromyscus           19.5 \n#&gt; 6  1977 Reithrodontomys      10\n\npesquisa_gw %&gt;% tidyr::pivot_wider(names_from=\"genus\", values_from=\"peso_medio\")\n\n#&gt; # A tibble: 26 × 11\n#&gt; # Groups:   year [26]\n#&gt;     year Chaetodipus Dipodomys Onychomys Perognathus Peromyscus Reithrodontomys\n#&gt;    &lt;int&gt;       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;      &lt;dbl&gt;           &lt;dbl&gt;\n#&gt;  1  1977        15.3      52.5      21.4        7.17       19.5           10   \n#&gt;  2  1978        14.9      73.9      26.5        7.09       20.5            7.5 \n#&gt;  3  1979        15.1      74.9      27.4        7.53       21.3            8.33\n#&gt;  4  1980        14.2      73.1      28.3        7.46       22.4           10.2 \n#&gt;  5  1981        14.0      72.7      28.4        7.15       20.4           11.2 \n#&gt;  6  1982        16.1      66.3      29.9        6.92       21.3           10.5 \n#&gt;  7  1983        15.5      65.0      29.0        6.83       21.5            9.87\n#&gt;  8  1984        15.3      52.8      28.3       16.9        20.0           11.2 \n#&gt;  9  1985        15.8      51.1      28.1       32.7        20.0            8.37\n#&gt; 10  1986        16.8      56.4      27.5       18.3        22.1           10.8 \n#&gt; # ℹ 16 more rows\n#&gt; # ℹ 4 more variables: Neotoma &lt;dbl&gt;, Sigmodon &lt;dbl&gt;, Spermophilus &lt;dbl&gt;,\n#&gt; #   Baiomys &lt;dbl&gt;",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 02"
    ]
  },
  {
    "objectID": "semanas/Aula01.html#lendo-dados-de-arquivos-csv",
    "href": "semanas/Aula01.html#lendo-dados-de-arquivos-csv",
    "title": "Carregando dados, Manipulando e Visualizando",
    "section": "Lendo dados de arquivos csv",
    "text": "Lendo dados de arquivos csv\n\nlibrary(readr)\nurl &lt;- \"https://www.gov.br/anp/pt-br/centrais-de-conteudo/dados-abertos/arquivos-acervo-de-dados-tecnicos/tabela-pocos.csv\"",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 01"
    ]
  },
  {
    "objectID": "semanas/Aula01.html#o-read_delim-permite-que-seja-definido-o-tipo-de-delimitador-dos-dados",
    "href": "semanas/Aula01.html#o-read_delim-permite-que-seja-definido-o-tipo-de-delimitador-dos-dados",
    "title": "Carregando dados, Manipulando e Visualizando",
    "section": "O read_delim permite que seja definido o tipo de delimitador dos dados",
    "text": "O read_delim permite que seja definido o tipo de delimitador dos dados\n\npocos &lt;- read_delim(url, delim = \";\", locale= locale(decimal_mark = \",\"), col_names = TRUE)\n\n#&gt; Rows: 30566 Columns: 60\n#&gt; ── Column specification ────────────────────────────────────────────────────────\n#&gt; Delimiter: \";\"\n#&gt; chr (51): POCO, CADASTRO, OPERADOR, POCO_OPERADOR, ESTADO, BACIA, BLOCO, SIG...\n#&gt; dbl  (8): LATITUDE_BASE_DD, LONGITUDE_BASE_DD, PROFUNDIDADE_VERTICAL_M, PROF...\n#&gt; lgl  (1): UNIDADE_ESTRATIGRAFICA\n#&gt; \n#&gt; ℹ Use `spec()` to retrieve the full column specification for this data.\n#&gt; ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(pocos)\n\n#&gt; # A tibble: 6 × 60\n#&gt;   POCO        CADASTRO OPERADOR POCO_OPERADOR ESTADO BACIA BLOCO SIG_CAMPO CAMPO\n#&gt;   &lt;chr&gt;       &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;         &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;     &lt;chr&gt;\n#&gt; 1 7-PRG-59H-… 7428102… Equinor… BMC7B15       RJ     Camp… &lt;NA&gt;  \"PRG  \"   PERE…\n#&gt; 2 7-GVR-26D-… 5009002… Eneva    7GVR26DMA     MA     Parn… PN-T… \"GVR  \"   GAVI…\n#&gt; 3 6-BRSA-134… 7428102… Petrobr… 6MLS233RJS    RJ     Camp… &lt;NA&gt;  \"MLS  \"   MARL…\n#&gt; 4 7-CP-1505D… 9012002… Carmo    7CP1505DSE    SE     Serg… &lt;NA&gt;  \"CP   \"   CARM…\n#&gt; 5 7-GVP-6-MA  5009002… Eneva    7GVP6MA       MA     Parn… PN-T… \"GVP  \"   GAVI…\n#&gt; 6 7-TIE-1D-BA 2024002… SPE Tiê… 7TIE1DBA      BA     Recô… &lt;NA&gt;  \"TIE  \"   TIÊ  \n#&gt; # ℹ 51 more variables: TERRA_MAR &lt;chr&gt;, POCO_POS_ANP &lt;chr&gt;, TIPO &lt;chr&gt;,\n#&gt; #   CATEGORIA &lt;chr&gt;, RECLASSIFICACAO &lt;chr&gt;, SITUACAO &lt;chr&gt;, INICIO &lt;chr&gt;,\n#&gt; #   TERMINO &lt;chr&gt;, CONCLUSAO &lt;chr&gt;, TITULARIDADE &lt;chr&gt;, LATITUDE_BASE_4C &lt;chr&gt;,\n#&gt; #   LONGITUDE_BASE_4C &lt;chr&gt;, LATITUDE_BASE_DD &lt;dbl&gt;, LONGITUDE_BASE_DD &lt;dbl&gt;,\n#&gt; #   DATUM_HORIZONTAL &lt;chr&gt;, TIPO_DE_COORDENADA_DE_BASE &lt;chr&gt;, DIRECAO &lt;chr&gt;,\n#&gt; #   PROFUNDIDADE_VERTICAL_M &lt;dbl&gt;, PROFUNDIDADE_SONDADOR_M &lt;dbl&gt;,\n#&gt; #   PROFUNDIDADE_MEDIDA_M &lt;dbl&gt;, REFERENCIA_DE_PROFUNDIDADE &lt;chr&gt;, …",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 01"
    ]
  },
  {
    "objectID": "semanas/Aula01.html#bibliotecas",
    "href": "semanas/Aula01.html#bibliotecas",
    "title": "Carregando dados, Manipulando e Visualizando",
    "section": "Bibliotecas",
    "text": "Bibliotecas\nVamos carregar as bibliotecas que serão usadas na manipulação e visualização de dados.\nO pacote tidyverse carrega diversos pacotes muito uteis na manipulação e visualização de dados\n\nlibrary(\"tidyverse\")\n\nVamos primeiro conhecer o que tem na base de dados pocos. A base de dados possui 30566 linhas\n\nclass(pocos)  # Tipo de base de dados\n\n#&gt; [1] \"spec_tbl_df\" \"tbl_df\"      \"tbl\"         \"data.frame\"",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 01"
    ]
  },
  {
    "objectID": "semanas/Aula01.html#dados-de-poços",
    "href": "semanas/Aula01.html#dados-de-poços",
    "title": "Carregando dados, Manipulando e Visualizando",
    "section": "Dados de poços",
    "text": "Dados de poços\nOs dados da tabela poços tem 60 colunas, mas vejam que existem diversas variáveis com dados ausentes “NA”.",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 01"
    ]
  },
  {
    "objectID": "semanas/Aula01.html#variaveis-com-dados-ausentes",
    "href": "semanas/Aula01.html#variaveis-com-dados-ausentes",
    "title": "Carregando dados, Manipulando e Visualizando",
    "section": "Variaveis com dados ausentes",
    "text": "Variaveis com dados ausentes\n\nstr(pocos) # estrutura da base de dados\n\n#&gt; spc_tbl_ [30,566 × 60] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n#&gt;  $ POCO                      : chr [1:30566] \"7-PRG-59H-RJS\" \"7-GVR-26D-MA\" \"6-BRSA-1349-RJS\" \"7-CP-1505D-SE\" ...\n#&gt;  $ CADASTRO                  : chr [1:30566] \"74281028968\" \"50090029009\" \"74281028991\" \"90120029391\" ...\n#&gt;  $ OPERADOR                  : chr [1:30566] \"Equinor Brasil\" \"Eneva\" \"Petrobras\" \"Carmo\" ...\n#&gt;  $ POCO_OPERADOR             : chr [1:30566] \"BMC7B15\" \"7GVR26DMA\" \"6MLS233RJS\" \"7CP1505DSE\" ...\n#&gt;  $ ESTADO                    : chr [1:30566] \"RJ\" \"MA\" \"RJ\" \"SE\" ...\n#&gt;  $ BACIA                     : chr [1:30566] \"Campos\" \"Parnaíba\" \"Campos\" \"Sergipe\" ...\n#&gt;  $ BLOCO                     : chr [1:30566] NA \"PN-T-68\" NA NA ...\n#&gt;  $ SIG_CAMPO                 : chr [1:30566] \"PRG  \" \"GVR  \" \"MLS  \" \"CP   \" ...\n#&gt;  $ CAMPO                     : chr [1:30566] \"PEREGRINO\" \"GAVIÃO REAL\" \"MARLIM SUL\" \"CARMÓPOLIS\" ...\n#&gt;  $ TERRA_MAR                 : chr [1:30566] \"M\" \"T\" \"M\" \"T\" ...\n#&gt;  $ POCO_POS_ANP              : chr [1:30566] \"S\" \"S\" \"S\" \"S\" ...\n#&gt;  $ TIPO                      : chr [1:30566] \"7 \" \"7 \" \"6 \" \"7 \" ...\n#&gt;  $ CATEGORIA                 : chr [1:30566] \"Desenvolvimento\" \"Desenvolvimento\" \"Jazida Mais Profunda\" \"Desenvolvimento\" ...\n#&gt;  $ RECLASSIFICACAO           : chr [1:30566] \"INDEFINIDO\" \"ABANDONADO POR OUTRAS RAZÕES\" \"DESCOBRIDOR DE NOVA JAZIDA PETRÓLEO\" \"PRODUTOR COMERCIAL DE PETRÓLEO\" ...\n#&gt;  $ SITUACAO                  : chr [1:30566] \"ABANDONADO PERMANENTEMENTE\" \"ARRASADO\" \"PRODUZINDO\" \"PRODUZINDO\" ...\n#&gt;  $ INICIO                    : chr [1:30566] \"24/04/2017 00:00\" \"21/06/2017 00:00\" \"28/05/2017 00:00\" \"24/05/2019 00:00\" ...\n#&gt;  $ TERMINO                   : chr [1:30566] \"01/05/2017 00:00\" \"05/07/2017 00:00\" \"16/07/2017 00:00\" \"04/06/2019 00:00\" ...\n#&gt;  $ CONCLUSAO                 : chr [1:30566] \"13/05/2017 00:00\" \"11/07/2017 00:00\" \"30/07/2017 00:00\" \"06/06/2019 00:00\" ...\n#&gt;  $ TITULARIDADE              : chr [1:30566] \"Público\" \"Público\" \"Público\" \"Público\" ...\n#&gt;  $ LATITUDE_BASE_4C          : chr [1:30566] \"-23:17:45,778\" \"-04:51:16,170\" \"-22:30:54,291\" \"-10:38:21,087\" ...\n#&gt;  $ LONGITUDE_BASE_4C         : chr [1:30566] \"-41:12:28,246\" \"-44:22:32,123\" \"-40:01:09,068\" \"-36:58:50,405\" ...\n#&gt;  $ LATITUDE_BASE_DD          : num [1:30566] -23.3 -4.85 -22.52 -10.64 -4.68 ...\n#&gt;  $ LONGITUDE_BASE_DD         : num [1:30566] -41.2 -44.4 -40 -37 -44.7 ...\n#&gt;  $ DATUM_HORIZONTAL          : chr [1:30566] \"SIRGAS2000\" \"SIRGAS2000\" \"SIRGAS2000\" \"SIRGAS2000\" ...\n#&gt;  $ TIPO_DE_COORDENADA_DE_BASE: chr [1:30566] \"Definitiva\" \"Definitiva\" \"Definitiva\" \"Definitiva\" ...\n#&gt;  $ DIRECAO                   : chr [1:30566] \"Horizontal\" \"Direcional\" \"Vertical\" \"Direcional\" ...\n#&gt;  $ PROFUNDIDADE_VERTICAL_M   : num [1:30566] 760 1471 -4572 -835 1344 ...\n#&gt;  $ PROFUNDIDADE_SONDADOR_M   : num [1:30566] 760 2210 4600 935 1430 ...\n#&gt;  $ PROFUNDIDADE_MEDIDA_M     : num [1:30566] 760 2211 4604 934 1430 ...\n#&gt;  $ REFERENCIA_DE_PROFUNDIDADE: chr [1:30566] \"MR\" \"MR\" \"MR\" \"MR\" ...\n#&gt;  $ MESA_ROTATIVA             : num [1:30566] 50 143 32 55.8 86 ...\n#&gt;  $ COTA_ALTIMETRICA_M        : num [1:30566] 0 137 0 50.8 80 ...\n#&gt;  $ LAMINA_D_AGUA_M           : num [1:30566] 122 0 1107 0 0 ...\n#&gt;  $ DATUM_VERTICAL            : chr [1:30566] \"NM\" \"NM\" \"NM\" \"NM\" ...\n#&gt;  $ UNIDADE_ESTRATIGRAFICA    : logi [1:30566] NA NA NA NA NA NA ...\n#&gt;  $ GEOLOGIA_GRUPO_FINAL      : chr [1:30566] \"Campos\" \"Canindé\" \"Lagoa Feia\" \"Coruripe\" ...\n#&gt;  $ GEOLOGIA_FORMACAO_FINAL   : chr [1:30566] \"Carapebus\" \"Poti\" \"Macabu\" \"Barra de Itiuba\" ...\n#&gt;  $ GEOLOGIA_MEMBRO_FINAL     : chr [1:30566] NA NA NA NA ...\n#&gt;  $ CDPE                      : chr [1:30566] \"Existe\" \"Existe\" \"Existe\" \"Existe\" ...\n#&gt;  $ AGP                       : chr [1:30566] NA NA NA NA ...\n#&gt;  $ PC                        : chr [1:30566] NA NA \"Existe\" NA ...\n#&gt;  $ PAG                       : chr [1:30566] NA \"Existe\" \"Existe\" NA ...\n#&gt;  $ PERFIS_CONVENCIONAIS      : chr [1:30566] NA \"Existe\" \"Existe\" \"Existe\" ...\n#&gt;  $ DURANTE_PERFURACAO        : chr [1:30566] NA NA \"Existe\" NA ...\n#&gt;  $ PERFIS_DIGITAIS           : chr [1:30566] NA NA NA NA ...\n#&gt;  $ PERFIS_PROCESSADOS        : chr [1:30566] NA NA \"Existe\" \"Existe\" ...\n#&gt;  $ PERFIS_ESPECIAIS          : chr [1:30566] NA NA NA NA ...\n#&gt;  $ AMOSTRA_LATERAL           : chr [1:30566] NA NA \"Existe\" NA ...\n#&gt;  $ SISMICA                   : chr [1:30566] NA NA \"Existe\" NA ...\n#&gt;  $ TABELA_TEMPO_PROFUNDIDADE : chr [1:30566] NA NA \"Existe\" NA ...\n#&gt;  $ DADOS_DIRECIONAIS         : chr [1:30566] NA \"Existe\" \"Existe\" \"Existe\" ...\n#&gt;  $ TESTE_A_CABO              : chr [1:30566] NA \"Existe\" \"Existe\" \"Existe\" ...\n#&gt;  $ TESTE_DE_FORMACAO         : chr [1:30566] NA NA NA NA ...\n#&gt;  $ CANHONEIO                 : chr [1:30566] NA NA NA \"Existe\" ...\n#&gt;  $ TESTEMUNHO                : chr [1:30566] NA NA NA NA ...\n#&gt;  $ GEOQUIMICA                : chr [1:30566] NA NA NA NA ...\n#&gt;  $ SIG_SONDA                 : chr [1:30566] \"DR-B\" \"GREAT120 (TUS 120)\" \"NS-33\" \"EBS-05\" ...\n#&gt;  $ NOM_SONDA                 : chr [1:30566] \"DRILLINGRIG-B\" \"GREAT - 120\" \"NORBE IX\" \"EBS-05\" ...\n#&gt;  $ ATINGIU_PRESAL            : chr [1:30566] \"I\" \"I\" \"S\" \"I\" ...\n#&gt;  $ DHA_ATUALIZACAO           : chr [1:30566] \"05/11/2023 05:00\" \"05/11/2023 05:00\" \"05/11/2023 05:00\" \"05/11/2023 05:00\" ...\n#&gt;  - attr(*, \"spec\")=\n#&gt;   .. cols(\n#&gt;   ..   POCO = col_character(),\n#&gt;   ..   CADASTRO = col_character(),\n#&gt;   ..   OPERADOR = col_character(),\n#&gt;   ..   POCO_OPERADOR = col_character(),\n#&gt;   ..   ESTADO = col_character(),\n#&gt;   ..   BACIA = col_character(),\n#&gt;   ..   BLOCO = col_character(),\n#&gt;   ..   SIG_CAMPO = col_character(),\n#&gt;   ..   CAMPO = col_character(),\n#&gt;   ..   TERRA_MAR = col_character(),\n#&gt;   ..   POCO_POS_ANP = col_character(),\n#&gt;   ..   TIPO = col_character(),\n#&gt;   ..   CATEGORIA = col_character(),\n#&gt;   ..   RECLASSIFICACAO = col_character(),\n#&gt;   ..   SITUACAO = col_character(),\n#&gt;   ..   INICIO = col_character(),\n#&gt;   ..   TERMINO = col_character(),\n#&gt;   ..   CONCLUSAO = col_character(),\n#&gt;   ..   TITULARIDADE = col_character(),\n#&gt;   ..   LATITUDE_BASE_4C = col_character(),\n#&gt;   ..   LONGITUDE_BASE_4C = col_character(),\n#&gt;   ..   LATITUDE_BASE_DD = col_double(),\n#&gt;   ..   LONGITUDE_BASE_DD = col_double(),\n#&gt;   ..   DATUM_HORIZONTAL = col_character(),\n#&gt;   ..   TIPO_DE_COORDENADA_DE_BASE = col_character(),\n#&gt;   ..   DIRECAO = col_character(),\n#&gt;   ..   PROFUNDIDADE_VERTICAL_M = col_double(),\n#&gt;   ..   PROFUNDIDADE_SONDADOR_M = col_double(),\n#&gt;   ..   PROFUNDIDADE_MEDIDA_M = col_double(),\n#&gt;   ..   REFERENCIA_DE_PROFUNDIDADE = col_character(),\n#&gt;   ..   MESA_ROTATIVA = col_double(),\n#&gt;   ..   COTA_ALTIMETRICA_M = col_double(),\n#&gt;   ..   LAMINA_D_AGUA_M = col_double(),\n#&gt;   ..   DATUM_VERTICAL = col_character(),\n#&gt;   ..   UNIDADE_ESTRATIGRAFICA = col_logical(),\n#&gt;   ..   GEOLOGIA_GRUPO_FINAL = col_character(),\n#&gt;   ..   GEOLOGIA_FORMACAO_FINAL = col_character(),\n#&gt;   ..   GEOLOGIA_MEMBRO_FINAL = col_character(),\n#&gt;   ..   CDPE = col_character(),\n#&gt;   ..   AGP = col_character(),\n#&gt;   ..   PC = col_character(),\n#&gt;   ..   PAG = col_character(),\n#&gt;   ..   PERFIS_CONVENCIONAIS = col_character(),\n#&gt;   ..   DURANTE_PERFURACAO = col_character(),\n#&gt;   ..   PERFIS_DIGITAIS = col_character(),\n#&gt;   ..   PERFIS_PROCESSADOS = col_character(),\n#&gt;   ..   PERFIS_ESPECIAIS = col_character(),\n#&gt;   ..   AMOSTRA_LATERAL = col_character(),\n#&gt;   ..   SISMICA = col_character(),\n#&gt;   ..   TABELA_TEMPO_PROFUNDIDADE = col_character(),\n#&gt;   ..   DADOS_DIRECIONAIS = col_character(),\n#&gt;   ..   TESTE_A_CABO = col_character(),\n#&gt;   ..   TESTE_DE_FORMACAO = col_character(),\n#&gt;   ..   CANHONEIO = col_character(),\n#&gt;   ..   TESTEMUNHO = col_character(),\n#&gt;   ..   GEOQUIMICA = col_character(),\n#&gt;   ..   SIG_SONDA = col_character(),\n#&gt;   ..   NOM_SONDA = col_character(),\n#&gt;   ..   ATINGIU_PRESAL = col_character(),\n#&gt;   ..   DHA_ATUALIZACAO = col_character()\n#&gt;   .. )\n#&gt;  - attr(*, \"problems\")=&lt;externalptr&gt;",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 01"
    ]
  },
  {
    "objectID": "semanas/Aula01.html#criando-um-filtro-de-dados-ausentes",
    "href": "semanas/Aula01.html#criando-um-filtro-de-dados-ausentes",
    "title": "Carregando dados, Manipulando e Visualizando",
    "section": "Criando um filtro de dados ausentes",
    "text": "Criando um filtro de dados ausentes\n\nn &lt;-  nrow(pocos)\n\n## Calculando razão de valores ausentes\nval_ausentes_df &lt;- pocos %&gt;% \n  summarize(across(everything(), ~ sum(is.na(.)))) %&gt;% \n  pivot_longer(everything(), names_to = \"variavel\", values_to = \"num_val_ausentes\") %&gt;% \n  mutate(razao_val_ausente = num_val_ausentes / n)\n\n## Vendo resultado\nknitr::kable(val_ausentes_df, booktabs = TRUE)\n\n\n\nvariavel\nnum_val_ausentes\nrazao_val_ausente\n\n\n\nPOCO\n0\n0.0000000\n\n\nCADASTRO\n0\n0.0000000\n\n\nOPERADOR\n2926\n0.0957273\n\n\nPOCO_OPERADOR\n0\n0.0000000\n\n\nESTADO\n0\n0.0000000\n\n\nBACIA\n0\n0.0000000\n\n\nBLOCO\n3792\n0.1240594\n\n\nSIG_CAMPO\n4107\n0.1343650\n\n\nCAMPO\n4107\n0.1343650\n\n\nTERRA_MAR\n0\n0.0000000\n\n\nPOCO_POS_ANP\n0\n0.0000000\n\n\nTIPO\n0\n0.0000000\n\n\nCATEGORIA\n0\n0.0000000\n\n\nRECLASSIFICACAO\n1263\n0.0413204\n\n\nSITUACAO\n108\n0.0035333\n\n\nINICIO\n0\n0.0000000\n\n\nTERMINO\n1860\n0.0608519\n\n\nCONCLUSAO\n146\n0.0047765\n\n\nTITULARIDADE\n0\n0.0000000\n\n\nLATITUDE_BASE_4C\n0\n0.0000000\n\n\nLONGITUDE_BASE_4C\n0\n0.0000000\n\n\nLATITUDE_BASE_DD\n0\n0.0000000\n\n\nLONGITUDE_BASE_DD\n0\n0.0000000\n\n\nDATUM_HORIZONTAL\n0\n0.0000000\n\n\nTIPO_DE_COORDENADA_DE_BASE\n0\n0.0000000\n\n\nDIRECAO\n0\n0.0000000\n\n\nPROFUNDIDADE_VERTICAL_M\n18066\n0.5910489\n\n\nPROFUNDIDADE_SONDADOR_M\n1004\n0.0328470\n\n\nPROFUNDIDADE_MEDIDA_M\n16550\n0.5414513\n\n\nREFERENCIA_DE_PROFUNDIDADE\n0\n0.0000000\n\n\nMESA_ROTATIVA\n0\n0.0000000\n\n\nCOTA_ALTIMETRICA_M\n15971\n0.5225087\n\n\nLAMINA_D_AGUA_M\n13946\n0.4562586\n\n\nDATUM_VERTICAL\n13437\n0.4396061\n\n\nUNIDADE_ESTRATIGRAFICA\n30566\n1.0000000\n\n\nGEOLOGIA_GRUPO_FINAL\n27388\n0.8960283\n\n\nGEOLOGIA_FORMACAO_FINAL\n27127\n0.8874894\n\n\nGEOLOGIA_MEMBRO_FINAL\n30362\n0.9933259\n\n\nCDPE\n21320\n0.6975070\n\n\nAGP\n23656\n0.7739318\n\n\nPC\n22122\n0.7237453\n\n\nPAG\n29215\n0.9558006\n\n\nPERFIS_CONVENCIONAIS\n21982\n0.7191651\n\n\nDURANTE_PERFURACAO\n27530\n0.9006740\n\n\nPERFIS_DIGITAIS\n9824\n0.3214029\n\n\nPERFIS_PROCESSADOS\n29516\n0.9656481\n\n\nPERFIS_ESPECIAIS\n29860\n0.9769024\n\n\nAMOSTRA_LATERAL\n29604\n0.9685271\n\n\nSISMICA\n28871\n0.9445462\n\n\nTABELA_TEMPO_PROFUNDIDADE\n29676\n0.9708827\n\n\nDADOS_DIRECIONAIS\n22650\n0.7410194\n\n\nTESTE_A_CABO\n26192\n0.8568998\n\n\nTESTE_DE_FORMACAO\n29926\n0.9790617\n\n\nCANHONEIO\n29326\n0.9594320\n\n\nTESTEMUNHO\n30133\n0.9858339\n\n\nGEOQUIMICA\n27420\n0.8970752\n\n\nSIG_SONDA\n86\n0.0028136\n\n\nNOM_SONDA\n4120\n0.1347903\n\n\nATINGIU_PRESAL\n21767\n0.7121311\n\n\nDHA_ATUALIZACAO\n0\n0.0000000\n\n\n\n\n## filtro\nval_ausente_filtro &lt;- val_ausentes_df %&gt;% \n  filter(razao_val_ausente &lt;= 0.5) %&gt;% \n  pull(variavel)\n\n# Aplicando o filtro\npoco_filtrado &lt;- pocos %&gt;% \n  select(all_of(val_ausente_filtro))\n\nVamos inicialmente selecionar algumas colunas para podermos trabalhar com os dados.\nAs colunas que vamos trabalhar são:\n\nPOCO\nOPERADOR\nESTADO\nBACIA\nCAMPO\nTERRA_MAR\nCATEGORIA\nSITUACAO\nINICIO\nTERMINO\nPROFUNDIDADE_SONDADOR_M\n\nPara selecionar colunas usamos a função select\n\n## Vamos selecionar as colunas listadas acima\npocos_01 &lt;- poco_filtrado %&gt;% select(POCO,OPERADOR,ESTADO,BACIA, CAMPO,TERRA_MAR,CATEGORIA,SITUACAO, INICIO, TERMINO, PROFUNDIDADE_SONDADOR_M)\nhead(pocos_01)\n\n#&gt; # A tibble: 6 × 11\n#&gt;   POCO   OPERADOR ESTADO BACIA CAMPO TERRA_MAR CATEGORIA SITUACAO INICIO TERMINO\n#&gt;   &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;    &lt;chr&gt;  &lt;chr&gt;  \n#&gt; 1 7-PRG… Equinor… RJ     Camp… PERE… M         Desenvol… ABANDON… 24/04… 01/05/…\n#&gt; 2 7-GVR… Eneva    MA     Parn… GAVI… T         Desenvol… ARRASADO 21/06… 05/07/…\n#&gt; 3 6-BRS… Petrobr… RJ     Camp… MARL… M         Jazida M… PRODUZI… 28/05… 16/07/…\n#&gt; 4 7-CP-… Carmo    SE     Serg… CARM… T         Desenvol… PRODUZI… 24/05… 04/06/…\n#&gt; 5 7-GVP… Eneva    MA     Parn… GAVI… T         Desenvol… FECHADO  04/02… 10/02/…\n#&gt; 6 7-TIE… SPE Tiê… BA     Recô… TIÊ   T         Desenvol… PRODUZI… 18/02… 10/04/…\n#&gt; # ℹ 1 more variable: PROFUNDIDADE_SONDADOR_M &lt;dbl&gt;\n\nsummary(pocos_01)\n\n#&gt;      POCO             OPERADOR            ESTADO             BACIA          \n#&gt;  Length:30566       Length:30566       Length:30566       Length:30566      \n#&gt;  Class :character   Class :character   Class :character   Class :character  \n#&gt;  Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n#&gt;                                                                             \n#&gt;                                                                             \n#&gt;                                                                             \n#&gt;                                                                             \n#&gt;     CAMPO            TERRA_MAR          CATEGORIA           SITUACAO        \n#&gt;  Length:30566       Length:30566       Length:30566       Length:30566      \n#&gt;  Class :character   Class :character   Class :character   Class :character  \n#&gt;  Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n#&gt;                                                                             \n#&gt;                                                                             \n#&gt;                                                                             \n#&gt;                                                                             \n#&gt;     INICIO            TERMINO          PROFUNDIDADE_SONDADOR_M\n#&gt;  Length:30566       Length:30566       Min.   :   0           \n#&gt;  Class :character   Class :character   1st Qu.: 485           \n#&gt;  Mode  :character   Mode  :character   Median :1000           \n#&gt;                                        Mean   :1506           \n#&gt;                                        3rd Qu.:2239           \n#&gt;                                        Max.   :8613           \n#&gt;                                        NA's   :1004",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 01"
    ]
  },
  {
    "objectID": "semanas/Aula01.html#outra-sumarização",
    "href": "semanas/Aula01.html#outra-sumarização",
    "title": "Carregando dados, Manipulando e Visualizando",
    "section": "Outra sumarização",
    "text": "Outra sumarização\n\nlibrary(summarytools)\n\n#&gt; \n#&gt; Anexando pacote: 'summarytools'\n\n\n#&gt; O seguinte objeto é mascarado por 'package:tibble':\n#&gt; \n#&gt;     view\n\ndfSummary(pocos_01) |&gt; stview(method = \"render\")\n\n\nData Frame Summary\npocos_01\nDimensions: 30566 x 11\n  Duplicates: 0\n\n\n\n\n\n\n\n\n\n\n\nNo\nVariable\nStats / Values\nFreqs (% of Valid)\nGraph\nValid\nMissing\n\n\n\n1\nPOCO [character]\n\n\n1. 1-A-1-BA\n\n\n2. 1-AA-1-RN\n\n\n3. 1-AB-1-BA\n\n\n4. 1-AB-1-PA\n\n\n5. 1-AB-1-SE\n\n\n6. 1-AB-1-SP\n\n\n7. 1-AB-1A-SE\n\n\n8. 1-AB-2-SE\n\n\n9. 1-ABV-1D-RN\n\n\n10. 1-AC-1-PA\n\n\n[ 30556 others ]\n\n\n\n\n1\n(\n0.0%\n)\n\n\n1\n(\n0.0%\n)\n\n\n1\n(\n0.0%\n)\n\n\n1\n(\n0.0%\n)\n\n\n1\n(\n0.0%\n)\n\n\n1\n(\n0.0%\n)\n\n\n1\n(\n0.0%\n)\n\n\n1\n(\n0.0%\n)\n\n\n1\n(\n0.0%\n)\n\n\n1\n(\n0.0%\n)\n\n\n30556\n(\n100.0%\n)\n\n\n\n30566 (100.0%)\n0 (0.0%)\n\n\n2\nOPERADOR [character]\n\n\n1. Petrobras\n\n\n2. 3R Potiguar\n\n\n3. Carmo\n\n\n4. 3R Candeias\n\n\n5. 3R Fazenda Belém\n\n\n6. Potiguar E&P S.A.\n\n\n7. SPE Miranga\n\n\n8. PetroRecôncavo\n\n\n9. Seacrest SPE Cricaré\n\n\n10. Origem Alagoas\n\n\n[ 100 others ]\n\n\n\n\n6689\n(\n24.2%\n)\n\n\n6277\n(\n22.7%\n)\n\n\n3870\n(\n14.0%\n)\n\n\n1431\n(\n5.2%\n)\n\n\n985\n(\n3.6%\n)\n\n\n912\n(\n3.3%\n)\n\n\n765\n(\n2.8%\n)\n\n\n737\n(\n2.7%\n)\n\n\n736\n(\n2.7%\n)\n\n\n609\n(\n2.2%\n)\n\n\n4629\n(\n16.7%\n)\n\n\n\n27640 (90.4%)\n2926 (9.6%)\n\n\n3\nESTADO [character]\n\n\n1. RN\n\n\n2. BA\n\n\n3. SE\n\n\n4. RJ\n\n\n5. ES\n\n\n6. CE\n\n\n7. AL\n\n\n8. AM\n\n\n9. MA\n\n\n10. SP\n\n\n[ 14 others ]\n\n\n\n\n8454\n(\n27.7%\n)\n\n\n7201\n(\n23.6%\n)\n\n\n4706\n(\n15.4%\n)\n\n\n3873\n(\n12.7%\n)\n\n\n2351\n(\n7.7%\n)\n\n\n1273\n(\n4.2%\n)\n\n\n975\n(\n3.2%\n)\n\n\n557\n(\n1.8%\n)\n\n\n398\n(\n1.3%\n)\n\n\n321\n(\n1.1%\n)\n\n\n457\n(\n1.5%\n)\n\n\n\n30566 (100.0%)\n0 (0.0%)\n\n\n4\nBACIA [character]\n\n\n1. Potiguar\n\n\n2. Recôncavo\n\n\n3. Sergipe\n\n\n4. Campos\n\n\n5. Espírito Santo\n\n\n6. Alagoas\n\n\n7. Santos\n\n\n8. Solimões\n\n\n9. Parnaíba\n\n\n10. Amazonas\n\n\n[ 27 others ]\n\n\n\n\n9498\n(\n31.1%\n)\n\n\n6777\n(\n22.2%\n)\n\n\n4741\n(\n15.5%\n)\n\n\n3585\n(\n11.7%\n)\n\n\n2079\n(\n6.8%\n)\n\n\n941\n(\n3.1%\n)\n\n\n906\n(\n3.0%\n)\n\n\n367\n(\n1.2%\n)\n\n\n246\n(\n0.8%\n)\n\n\n239\n(\n0.8%\n)\n\n\n1187\n(\n3.9%\n)\n\n\n\n30566 (100.0%)\n0 (0.0%)\n\n\n5\nCAMPO [character]\n\n\n1. ESTREITO\n\n\n2. CARMÓPOLIS\n\n\n3. CANTO DO AMARO\n\n\n4. FAZENDA BELÉM CE\n\n\n5. DOM JOÃO\n\n\n6. ALTO DO RODRIGUES\n\n\n7. RIACHUELO\n\n\n8. SIRIRIZINHO\n\n\n9. MIRANGA\n\n\n10. FAZENDA POCINHO\n\n\n[ 519 others ]\n\n\n\n\n2053\n(\n7.8%\n)\n\n\n2033\n(\n7.7%\n)\n\n\n1906\n(\n7.2%\n)\n\n\n973\n(\n3.7%\n)\n\n\n927\n(\n3.5%\n)\n\n\n874\n(\n3.3%\n)\n\n\n731\n(\n2.8%\n)\n\n\n640\n(\n2.4%\n)\n\n\n620\n(\n2.3%\n)\n\n\n558\n(\n2.1%\n)\n\n\n15144\n(\n57.2%\n)\n\n\n\n26459 (86.6%)\n4107 (13.4%)\n\n\n6\nTERRA_MAR [character]\n\n\n1. M\n\n\n2. T\n\n\n\n\n7027\n(\n23.0%\n)\n\n\n23539\n(\n77.0%\n)\n\n\n\n30566 (100.0%)\n0 (0.0%)\n\n\n7\nCATEGORIA [character]\n\n\n1. Desenvolvimento\n\n\n2. Especial\n\n\n3. Estratigráfico\n\n\n4. Extensão\n\n\n5. Injeção\n\n\n6. Jazida Mais Profunda\n\n\n7. Jazida Mais Rasa\n\n\n8. Pioneiro\n\n\n9. Pioneiro Adjacente\n\n\n\n\n18949\n(\n62.0%\n)\n\n\n2026\n(\n6.6%\n)\n\n\n328\n(\n1.1%\n)\n\n\n2345\n(\n7.7%\n)\n\n\n1749\n(\n5.7%\n)\n\n\n144\n(\n0.5%\n)\n\n\n35\n(\n0.1%\n)\n\n\n4157\n(\n13.6%\n)\n\n\n833\n(\n2.7%\n)\n\n\n\n30566 (100.0%)\n0 (0.0%)\n\n\n8\nSITUACAO [character]\n\n\n1. FECHADO\n\n\n2. ABANDONADO PERMANENTEMENT\n\n\n3. PRODUZINDO\n\n\n4. ABANDONADO TEMPORARIAMENT\n\n\n5. ARRASADO\n\n\n6. INJETANDO\n\n\n7. DEVOLVIDO\n\n\n8. ABANDONADO TEMPORARIAMENT\n\n\n9. ABANDONADO AGUARDANDO ABA\n\n\n10. CEDIDO PARA A CAPTAÇÃO DE\n\n\n[ 13 others ]\n\n\n\n\n6313\n(\n20.7%\n)\n\n\n5927\n(\n19.5%\n)\n\n\n5761\n(\n18.9%\n)\n\n\n4968\n(\n16.3%\n)\n\n\n2872\n(\n9.4%\n)\n\n\n1616\n(\n5.3%\n)\n\n\n1084\n(\n3.6%\n)\n\n\n835\n(\n2.7%\n)\n\n\n325\n(\n1.1%\n)\n\n\n222\n(\n0.7%\n)\n\n\n535\n(\n1.8%\n)\n\n\n\n30458 (99.6%)\n108 (0.4%)\n\n\n9\nINICIO [character]\n\n\n1. 03/08/1982 00:00\n\n\n2. 22/04/1984 00:00\n\n\n3. 24/07/1986 00:00\n\n\n4. 25/11/1981 00:00\n\n\n5. 29/04/1983 00:00\n\n\n6. 01/03/1985 00:00\n\n\n7. 02/09/1982 00:00\n\n\n8. 05/02/1983 00:00\n\n\n9. 05/11/1982 00:00\n\n\n10. 06/01/1987 00:00\n\n\n[ 16011 others ]\n\n\n\n\n12\n(\n0.0%\n)\n\n\n9\n(\n0.0%\n)\n\n\n9\n(\n0.0%\n)\n\n\n9\n(\n0.0%\n)\n\n\n9\n(\n0.0%\n)\n\n\n8\n(\n0.0%\n)\n\n\n8\n(\n0.0%\n)\n\n\n8\n(\n0.0%\n)\n\n\n8\n(\n0.0%\n)\n\n\n8\n(\n0.0%\n)\n\n\n30478\n(\n99.7%\n)\n\n\n\n30566 (100.0%)\n0 (0.0%)\n\n\n10\nTERMINO [character]\n\n\n1. 01/04/1983 00:00\n\n\n2. 01/11/1982 00:00\n\n\n3. 08/03/1983 00:00\n\n\n4. 11/10/2009 00:00\n\n\n5. 31/12/1983 00:00\n\n\n6. 20/11/1984 00:00\n\n\n7. 22/02/1983 00:00\n\n\n8. 24/03/1982 00:00\n\n\n9. 26/11/1983 00:00\n\n\n10. 02/11/1983 00:00\n\n\n[ 15172 others ]\n\n\n\n\n11\n(\n0.0%\n)\n\n\n11\n(\n0.0%\n)\n\n\n10\n(\n0.0%\n)\n\n\n10\n(\n0.0%\n)\n\n\n10\n(\n0.0%\n)\n\n\n9\n(\n0.0%\n)\n\n\n9\n(\n0.0%\n)\n\n\n9\n(\n0.0%\n)\n\n\n9\n(\n0.0%\n)\n\n\n8\n(\n0.0%\n)\n\n\n28610\n(\n99.7%\n)\n\n\n\n28706 (93.9%)\n1860 (6.1%)\n\n\n11\nPROFUNDIDADE_SONDADOR_M [numeric]\n\n\nMean (sd) : 1505.8 (1351.3)\n\n\nmin ≤ med ≤ max:\n\n\n0 ≤ 1000 ≤ 8613\n\n\nIQR (CV) : 1754 (0.9)\n\n\n6787 distinct values\n\n29562 (96.7%)\n1004 (3.3%)\n\n\n\nGenerated by summarytools 1.1.3 (R version 4.5.0)2025-04-25",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 01"
    ]
  },
  {
    "objectID": "semanas/Aula01.html#manipulação-de-dados",
    "href": "semanas/Aula01.html#manipulação-de-dados",
    "title": "Carregando dados, Manipulando e Visualizando",
    "section": "Manipulação de dados",
    "text": "Manipulação de dados\nVamos eliminar as linhas de dados que apresenta dados ausentes “NA” nas colunas TERMINO e PROFUNDIDADE_SONDADOR_M.\n\nsum(is.na(pocos_01))\n\n#&gt; [1] 10005\n\nsum(is.na(pocos_01$INICIO))\n\n#&gt; [1] 0\n\nsum(is.na(pocos_01$TERMINO))\n\n#&gt; [1] 1860\n\nsum(is.na(pocos_01$PROFUNDIDADE_SONDADOR_M))\n\n#&gt; [1] 1004\n\npocos_01 &lt;- pocos_01 %&gt;% drop_na(any_of(c(\"TERMINO\",\n                            \"PROFUNDIDADE_SONDADOR_M\")))\n# melhorando a visualização dos dados\nknitr::kable(\n  head(pocos_01, 10), booktabs = TRUE,\n  caption = 'Uma tabela mais elegante com as 10 primeras linhas de pocos_01.')\n\n\nUma tabela mais elegante com as 10 primeras linhas de pocos_01.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPOCO\nOPERADOR\nESTADO\nBACIA\nCAMPO\nTERRA_MAR\nCATEGORIA\nSITUACAO\nINICIO\nTERMINO\nPROFUNDIDADE_SONDADOR_M\n\n\n\n7-PRG-59H-RJS\nEquinor Brasil\nRJ\nCampos\nPEREGRINO\nM\nDesenvolvimento\nABANDONADO PERMANENTEMENTE\n24/04/2017 00:00\n01/05/2017 00:00\n760.00\n\n\n7-GVR-26D-MA\nEneva\nMA\nParnaíba\nGAVIÃO REAL\nT\nDesenvolvimento\nARRASADO\n21/06/2017 00:00\n05/07/2017 00:00\n2210.00\n\n\n6-BRSA-1349-RJS\nPetrobras\nRJ\nCampos\nMARLIM SUL\nM\nJazida Mais Profunda\nPRODUZINDO\n28/05/2017 00:00\n16/07/2017 00:00\n4600.00\n\n\n7-CP-1505D-SE\nCarmo\nSE\nSergipe\nCARMÓPOLIS\nT\nDesenvolvimento\nPRODUZINDO\n24/05/2019 00:00\n04/06/2019 00:00\n935.00\n\n\n7-GVP-6-MA\nEneva\nMA\nParnaíba\nGAVIÃO PRETO\nT\nDesenvolvimento\nFECHADO\n04/02/2019 00:00\n10/02/2019 00:00\n1430.00\n\n\n7-TIE-1D-BA\nSPE Tiêta\nBA\nRecôncavo\nTIÊ\nT\nDesenvolvimento\nPRODUZINDO\n18/02/2019 00:00\n10/04/2019 00:00\n2614.42\n\n\n7-CP-1846D-SE\nCarmo\nSE\nSergipe\nCARMÓPOLIS\nT\nDesenvolvimento\nABANDONADO TEMPORARIAMENTE COM MONITORAMENTO\n31/07/2019 00:00\n10/08/2019 00:00\n896.00\n\n\n7-CP-1828D-SE\nCarmo\nSE\nSergipe\nCARMÓPOLIS\nT\nDesenvolvimento\nPRODUZINDO\n06/06/2019 00:00\n19/06/2019 00:00\n900.00\n\n\n7-CP-1827D-SE\nCarmo\nSE\nSergipe\nCARMÓPOLIS\nT\nDesenvolvimento\nPRODUZINDO\n04/07/2019 00:00\n29/07/2019 00:00\n920.00\n\n\n7-CP-1638D-SE\nCarmo\nSE\nSergipe\nCARMÓPOLIS\nT\nDesenvolvimento\nPRODUZINDO\n16/04/2019 00:00\n08/05/2019 00:00\n954.00\n\n\n\n\nsum(is.na(pocos_01))\n\n#&gt; [1] 6096",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 01"
    ]
  },
  {
    "objectID": "semanas/Aula01.html#corrigindo-tipo-de-dados",
    "href": "semanas/Aula01.html#corrigindo-tipo-de-dados",
    "title": "Carregando dados, Manipulando e Visualizando",
    "section": "Corrigindo tipo de dados",
    "text": "Corrigindo tipo de dados\nAs colunas INICIO e TERMINO são datas, mas foram lidas como caracter, vamos corrigir isto!\nPara trabalhar com datas vamos usar o pacote lubridate\n\nlibrary(lubridate)\npocos_01$INICIO &lt;- dmy_hm(pocos_01$INICIO, locale = Sys.getlocale(\"LC_TIME\"))\npocos_01$TERMINO &lt;- dmy_hm(pocos_01$TERMINO, locale = Sys.getlocale(\"LC_TIME\"))\nstr(pocos_01)\n\n#&gt; tibble [27,793 × 11] (S3: tbl_df/tbl/data.frame)\n#&gt;  $ POCO                   : chr [1:27793] \"7-PRG-59H-RJS\" \"7-GVR-26D-MA\" \"6-BRSA-1349-RJS\" \"7-CP-1505D-SE\" ...\n#&gt;  $ OPERADOR               : chr [1:27793] \"Equinor Brasil\" \"Eneva\" \"Petrobras\" \"Carmo\" ...\n#&gt;  $ ESTADO                 : chr [1:27793] \"RJ\" \"MA\" \"RJ\" \"SE\" ...\n#&gt;  $ BACIA                  : chr [1:27793] \"Campos\" \"Parnaíba\" \"Campos\" \"Sergipe\" ...\n#&gt;  $ CAMPO                  : chr [1:27793] \"PEREGRINO\" \"GAVIÃO REAL\" \"MARLIM SUL\" \"CARMÓPOLIS\" ...\n#&gt;  $ TERRA_MAR              : chr [1:27793] \"M\" \"T\" \"M\" \"T\" ...\n#&gt;  $ CATEGORIA              : chr [1:27793] \"Desenvolvimento\" \"Desenvolvimento\" \"Jazida Mais Profunda\" \"Desenvolvimento\" ...\n#&gt;  $ SITUACAO               : chr [1:27793] \"ABANDONADO PERMANENTEMENTE\" \"ARRASADO\" \"PRODUZINDO\" \"PRODUZINDO\" ...\n#&gt;  $ INICIO                 : POSIXct[1:27793], format: \"2017-04-24\" \"2017-06-21\" ...\n#&gt;  $ TERMINO                : POSIXct[1:27793], format: \"2017-05-01\" \"2017-07-05\" ...\n#&gt;  $ PROFUNDIDADE_SONDADOR_M: num [1:27793] 760 2210 4600 935 1430 ...",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 01"
    ]
  },
  {
    "objectID": "semanas/Aula01.html#filtrando-dados",
    "href": "semanas/Aula01.html#filtrando-dados",
    "title": "Carregando dados, Manipulando e Visualizando",
    "section": "Filtrando dados",
    "text": "Filtrando dados\nVamos analisar os poços de uma detreminada região, para isto podemos fltrar os poços de um bloco. Vamos filtrar somente os poços do CAMPO PEREGRINO usando a função filter.\n\npocos_02 &lt;- pocos_01 %&gt;% filter(CAMPO==\"PEREGRINO\") ##  \nsummary(pocos_02)\n\n#&gt;      POCO             OPERADOR            ESTADO             BACIA          \n#&gt;  Length:108         Length:108         Length:108         Length:108        \n#&gt;  Class :character   Class :character   Class :character   Class :character  \n#&gt;  Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n#&gt;                                                                             \n#&gt;                                                                             \n#&gt;                                                                             \n#&gt;     CAMPO            TERRA_MAR          CATEGORIA           SITUACAO        \n#&gt;  Length:108         Length:108         Length:108         Length:108        \n#&gt;  Class :character   Class :character   Class :character   Class :character  \n#&gt;  Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n#&gt;                                                                             \n#&gt;                                                                             \n#&gt;                                                                             \n#&gt;      INICIO                       TERMINO                   \n#&gt;  Min.   :1994-07-07 00:00:00   Min.   :1994-08-17 00:00:00  \n#&gt;  1st Qu.:2012-02-06 06:00:00   1st Qu.:2012-03-30 00:00:00  \n#&gt;  Median :2014-02-08 00:00:00   Median :2014-04-05 12:00:00  \n#&gt;  Mean   :2014-12-06 00:13:20   Mean   :2015-01-09 08:00:00  \n#&gt;  3rd Qu.:2017-06-25 12:00:00   3rd Qu.:2017-06-14 06:00:00  \n#&gt;  Max.   :2023-05-19 00:00:00   Max.   :2023-06-15 00:00:00  \n#&gt;  PROFUNDIDADE_SONDADOR_M\n#&gt;  Min.   : 269           \n#&gt;  1st Qu.:4433           \n#&gt;  Median :4924           \n#&gt;  Mean   :5070           \n#&gt;  3rd Qu.:6138           \n#&gt;  Max.   :8613\n\nknitr::kable(\n  head(pocos_02, 10), booktabs = TRUE,\n  caption = 'Uma tabela mais elegante com as 10 primeras linhas de pocos_02.')\n\n\nUma tabela mais elegante com as 10 primeras linhas de pocos_02.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPOCO\nOPERADOR\nESTADO\nBACIA\nCAMPO\nTERRA_MAR\nCATEGORIA\nSITUACAO\nINICIO\nTERMINO\nPROFUNDIDADE_SONDADOR_M\n\n\n\n7-PRG-59H-RJS\nEquinor Brasil\nRJ\nCampos\nPEREGRINO\nM\nDesenvolvimento\nABANDONADO PERMANENTEMENTE\n2017-04-24\n2017-05-01\n760.0\n\n\n7-PRG-66HP-RJS\nEquinor Brasil\nRJ\nCampos\nPEREGRINO\nM\nDesenvolvimento\nPRODUZINDO\n2019-02-11\n2019-03-07\n6529.0\n\n\n7-PRG-83H-RJS\nEquinor Brasil\nRJ\nCampos\nPEREGRINO\nM\nDesenvolvimento\nPRODUZINDO\n2023-03-18\n2023-04-20\n5854.0\n\n\n7-PRG-8HPA-RJS\nEquinor Brasil\nRJ\nCampos\nPEREGRINO\nM\nDesenvolvimento\nABANDONADO PERMANENTEMENTE\n2011-11-07\n2011-12-02\n4960.0\n\n\n7-PRG-7H-RJS\nEquinor Brasil\nRJ\nCampos\nPEREGRINO\nM\nDesenvolvimento\nFECHADO\n2011-05-14\n2011-06-15\n4510.0\n\n\n7-PRG-58HA-RJS\nEquinor Brasil\nRJ\nCampos\nPEREGRINO\nM\nDesenvolvimento\nABANDONADO PERMANENTEMENTE\n2017-05-26\n2017-06-04\n6015.0\n\n\n7-PRG-2HA-RJS\nEquinor Brasil\nRJ\nCampos\nPEREGRINO\nM\nDesenvolvimento\nABANDONADO PERMANENTEMENTE\n2010-11-17\n2010-12-10\n3969.8\n\n\n7-PRG-59HA-RJS\nEquinor Brasil\nRJ\nCampos\nPEREGRINO\nM\nDesenvolvimento\nABANDONADO PERMANENTEMENTE\n2017-07-15\n2017-07-15\n269.0\n\n\n7-PRG-67H-RJS\nEquinor Brasil\nRJ\nCampos\nPEREGRINO\nM\nDesenvolvimento\nFECHADO\n2019-05-09\n2019-06-06\n4465.0\n\n\n7-PRG-14H-RJS\nEquinor Brasil\nRJ\nCampos\nPEREGRINO\nM\nDesenvolvimento\nABANDONADO PERMANENTEMENTE\n2012-01-26\n2012-02-04\n1167.0\n\n\n\n\n\nAvaliando os dados\nOs poços possuem diversas categorias, vamos ver que categorias existem nestes poços campo de Peregrino.\n\nunique(pocos_02$CATEGORIA)\n\n#&gt; [1] \"Desenvolvimento\" \"Injeção\"         \"Extensão\"        \"Especial\"       \n#&gt; [5] \"Pioneiro\"\n\npocos_02 %&gt;% group_by(CATEGORIA) %&gt;% summarize(total=n())\n\n#&gt; # A tibble: 5 × 2\n#&gt;   CATEGORIA       total\n#&gt;   &lt;chr&gt;           &lt;int&gt;\n#&gt; 1 Desenvolvimento    84\n#&gt; 2 Especial            3\n#&gt; 3 Extensão            7\n#&gt; 4 Injeção            13\n#&gt; 5 Pioneiro            1\n\n\nFiltrando poços de desenvolvimento\n\npocos_03 &lt;- pocos_02 %&gt;% filter(CATEGORIA==\"Desenvolvimento\") ##  \nsummary(pocos_03)\n\n#&gt;      POCO             OPERADOR            ESTADO             BACIA          \n#&gt;  Length:84          Length:84          Length:84          Length:84         \n#&gt;  Class :character   Class :character   Class :character   Class :character  \n#&gt;  Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n#&gt;                                                                             \n#&gt;                                                                             \n#&gt;                                                                             \n#&gt;     CAMPO            TERRA_MAR          CATEGORIA           SITUACAO        \n#&gt;  Length:84          Length:84          Length:84          Length:84         \n#&gt;  Class :character   Class :character   Class :character   Class :character  \n#&gt;  Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n#&gt;                                                                             \n#&gt;                                                                             \n#&gt;                                                                             \n#&gt;      INICIO                       TERMINO                   \n#&gt;  Min.   :2010-11-10 00:00:00   Min.   :2010-11-11 00:00:00  \n#&gt;  1st Qu.:2012-04-27 06:00:00   1st Qu.:2012-06-08 00:00:00  \n#&gt;  Median :2014-11-20 00:00:00   Median :2014-12-05 00:00:00  \n#&gt;  Mean   :2015-06-21 20:00:00   Mean   :2015-07-26 19:25:42  \n#&gt;  3rd Qu.:2017-08-06 00:00:00   3rd Qu.:2017-08-15 12:00:00  \n#&gt;  Max.   :2023-03-18 00:00:00   Max.   :2023-04-20 00:00:00  \n#&gt;  PROFUNDIDADE_SONDADOR_M\n#&gt;  Min.   : 269           \n#&gt;  1st Qu.:4506           \n#&gt;  Median :5081           \n#&gt;  Mean   :5291           \n#&gt;  3rd Qu.:6397           \n#&gt;  Max.   :8613\n\nknitr::kable(\n  head(pocos_03, 10), booktabs = TRUE,\n  caption = 'Uma tabela mais elegante com as 10 primeras linhas de pocos_03.')\n\n\nUma tabela mais elegante com as 10 primeras linhas de pocos_03.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPOCO\nOPERADOR\nESTADO\nBACIA\nCAMPO\nTERRA_MAR\nCATEGORIA\nSITUACAO\nINICIO\nTERMINO\nPROFUNDIDADE_SONDADOR_M\n\n\n\n7-PRG-59H-RJS\nEquinor Brasil\nRJ\nCampos\nPEREGRINO\nM\nDesenvolvimento\nABANDONADO PERMANENTEMENTE\n2017-04-24\n2017-05-01\n760.0\n\n\n7-PRG-66HP-RJS\nEquinor Brasil\nRJ\nCampos\nPEREGRINO\nM\nDesenvolvimento\nPRODUZINDO\n2019-02-11\n2019-03-07\n6529.0\n\n\n7-PRG-83H-RJS\nEquinor Brasil\nRJ\nCampos\nPEREGRINO\nM\nDesenvolvimento\nPRODUZINDO\n2023-03-18\n2023-04-20\n5854.0\n\n\n7-PRG-8HPA-RJS\nEquinor Brasil\nRJ\nCampos\nPEREGRINO\nM\nDesenvolvimento\nABANDONADO PERMANENTEMENTE\n2011-11-07\n2011-12-02\n4960.0\n\n\n7-PRG-7H-RJS\nEquinor Brasil\nRJ\nCampos\nPEREGRINO\nM\nDesenvolvimento\nFECHADO\n2011-05-14\n2011-06-15\n4510.0\n\n\n7-PRG-58HA-RJS\nEquinor Brasil\nRJ\nCampos\nPEREGRINO\nM\nDesenvolvimento\nABANDONADO PERMANENTEMENTE\n2017-05-26\n2017-06-04\n6015.0\n\n\n7-PRG-2HA-RJS\nEquinor Brasil\nRJ\nCampos\nPEREGRINO\nM\nDesenvolvimento\nABANDONADO PERMANENTEMENTE\n2010-11-17\n2010-12-10\n3969.8\n\n\n7-PRG-59HA-RJS\nEquinor Brasil\nRJ\nCampos\nPEREGRINO\nM\nDesenvolvimento\nABANDONADO PERMANENTEMENTE\n2017-07-15\n2017-07-15\n269.0\n\n\n7-PRG-67H-RJS\nEquinor Brasil\nRJ\nCampos\nPEREGRINO\nM\nDesenvolvimento\nFECHADO\n2019-05-09\n2019-06-06\n4465.0\n\n\n7-PRG-14H-RJS\nEquinor Brasil\nRJ\nCampos\nPEREGRINO\nM\nDesenvolvimento\nABANDONADO PERMANENTEMENTE\n2012-01-26\n2012-02-04\n1167.0",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 01"
    ]
  },
  {
    "objectID": "semanas/Aula01.html#criando-uma-coluna-com-mutate",
    "href": "semanas/Aula01.html#criando-uma-coluna-com-mutate",
    "title": "Carregando dados, Manipulando e Visualizando",
    "section": "Criando uma coluna com mutate",
    "text": "Criando uma coluna com mutate\nVamos criar uma coluna que nos dará a duração da perfuração dos poços.\n\npocos_03$INICIO[1]\n\n#&gt; [1] \"2017-04-24 UTC\"\n\npocos_03$TERMINO[1]\n\n#&gt; [1] \"2017-05-01 UTC\"\n\npocos_03$TERMINO[1] - pocos_03$INICIO[1]\n\n#&gt; Time difference of 7 days\n\ndifftime(pocos_03$TERMINO[1], pocos_03$INICIO[1], units = \"days\")\n\n#&gt; Time difference of 7 days\n\ntempo &lt;- difftime(pocos_03$TERMINO[1], pocos_03$INICIO[1], units = \"days\")\nstr(tempo)\n\n#&gt;  'difftime' num 7\n#&gt;  - attr(*, \"units\")= chr \"days\"\n\n(pocos_03$INICIO[1] %--% pocos_03$TERMINO[1])/ddays(1)\n\n#&gt; [1] 7\n\npocos_03 &lt;- pocos_03 %&gt;% mutate(TPERF = (INICIO %--% TERMINO)/ddays(1))\nsummary(pocos_03)\n\n#&gt;      POCO             OPERADOR            ESTADO             BACIA          \n#&gt;  Length:84          Length:84          Length:84          Length:84         \n#&gt;  Class :character   Class :character   Class :character   Class :character  \n#&gt;  Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n#&gt;                                                                             \n#&gt;                                                                             \n#&gt;                                                                             \n#&gt;     CAMPO            TERRA_MAR          CATEGORIA           SITUACAO        \n#&gt;  Length:84          Length:84          Length:84          Length:84         \n#&gt;  Class :character   Class :character   Class :character   Class :character  \n#&gt;  Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n#&gt;                                                                             \n#&gt;                                                                             \n#&gt;                                                                             \n#&gt;      INICIO                       TERMINO                   \n#&gt;  Min.   :2010-11-10 00:00:00   Min.   :2010-11-11 00:00:00  \n#&gt;  1st Qu.:2012-04-27 06:00:00   1st Qu.:2012-06-08 00:00:00  \n#&gt;  Median :2014-11-20 00:00:00   Median :2014-12-05 00:00:00  \n#&gt;  Mean   :2015-06-21 20:00:00   Mean   :2015-07-26 19:25:42  \n#&gt;  3rd Qu.:2017-08-06 00:00:00   3rd Qu.:2017-08-15 12:00:00  \n#&gt;  Max.   :2023-03-18 00:00:00   Max.   :2023-04-20 00:00:00  \n#&gt;  PROFUNDIDADE_SONDADOR_M     TPERF        \n#&gt;  Min.   : 269            Min.   :-347.00  \n#&gt;  1st Qu.:4506            1st Qu.:  14.50  \n#&gt;  Median :5081            Median :  24.50  \n#&gt;  Mean   :5291            Mean   :  34.98  \n#&gt;  3rd Qu.:6397            3rd Qu.:  37.25  \n#&gt;  Max.   :8613            Max.   : 598.00\n\n\nEliminando colunas com tempos negativos\n\npocos_03 %&gt;% filter(TPERF&lt;0) %&gt;% select(POCO, INICIO, TERMINO)\n\n#&gt; # A tibble: 2 × 3\n#&gt;   POCO            INICIO              TERMINO            \n#&gt;   &lt;chr&gt;           &lt;dttm&gt;              &lt;dttm&gt;             \n#&gt; 1 7-PRG-71HPA-RJS 2020-12-18 00:00:00 2020-01-06 00:00:00\n#&gt; 2 7-PRG-58HB-RJS  2017-06-19 00:00:00 2017-06-04 00:00:00\n\npocos_03 &lt;- pocos_03 %&gt;% filter(TPERF &gt; 0)",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 01"
    ]
  },
  {
    "objectID": "semanas/Aula01.html#visualizando-os-dados-de-peregrino",
    "href": "semanas/Aula01.html#visualizando-os-dados-de-peregrino",
    "title": "Carregando dados, Manipulando e Visualizando",
    "section": "Visualizando os dados de PEREGRINO",
    "text": "Visualizando os dados de PEREGRINO\nBox-Plot\n\nggplot(pocos_03, aes(x=CAMPO, y=TPERF)) +\n  geom_boxplot()\n\n\n\n\n\n\n\nVeja que existem alguns tempos bem elevados que estão representados por pontos no box-plot. Eles podem ser considerados pontos afastados (outliers), que neste caso vamos eliminar.\n\npocos_04 &lt;- pocos_03 %&gt;% filter(TPERF &lt; 100)\nggplot(pocos_04, aes(x=CAMPO, y=TPERF)) +\n  geom_boxplot()\n\n\n\n\n\n\n\nHistograma\n\nggplot(pocos_04, aes(x=TPERF)) +\n  geom_histogram()\n\n#&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nVeja que desta forma o ggplot usa o seu padrão de 30 faixas de dados, que geralmente não é o mais adequado.\nVamos usar uma regra adequada para definição de número de faixas.\nCriando um histograma usando a regra de Sturges\nA regra de Sturges indica 7 faixas enquanto que o padrão do ggplot2 é 30.\n\nggplot(pocos_04, aes(x = TPERF)) +\n  geom_histogram(bins = round(1+3.322*log10(nrow(pocos_04)),0))\n\n\n\n\n\n\n\nVeja que agora temos um histograma mais suave.\nGráfico de Dispersão\n\nggplot(pocos_04, aes(x=PROFUNDIDADE_SONDADOR_M, y=TPERF)) + \n  geom_point()\n\n\n\n\n\n\ncor(pocos_04$PROFUNDIDADE_SONDADOR_M, pocos_04$TPERF)\n\n#&gt; [1] 0.5253442\n\n\nPodemos perceber que há uma relação entre o tempo de perfuração e a profundidade do poço.\nTambém é possível se perceber que ainda existem dados com comportamentos estranhos. Poços rasos com profundidade muito diferente dos demais, além disso um poço muito profundo com duração muito pequena.\nSe fossemos construir um modelo certamente terímos que investigar o porque destes comportamentos.\nMelhorando a descrição das colunas\n\nnames(pocos_04)\n\n#&gt;  [1] \"POCO\"                    \"OPERADOR\"               \n#&gt;  [3] \"ESTADO\"                  \"BACIA\"                  \n#&gt;  [5] \"CAMPO\"                   \"TERRA_MAR\"              \n#&gt;  [7] \"CATEGORIA\"               \"SITUACAO\"               \n#&gt;  [9] \"INICIO\"                  \"TERMINO\"                \n#&gt; [11] \"PROFUNDIDADE_SONDADOR_M\" \"TPERF\"\n\npocos_04A &lt;- pocos_04\nnames(pocos_04) &lt;- tolower(names(pocos_04))\nnames(pocos_04)\n\n#&gt;  [1] \"poco\"                    \"operador\"               \n#&gt;  [3] \"estado\"                  \"bacia\"                  \n#&gt;  [5] \"campo\"                   \"terra_mar\"              \n#&gt;  [7] \"categoria\"               \"situacao\"               \n#&gt;  [9] \"inicio\"                  \"termino\"                \n#&gt; [11] \"profundidade_sondador_m\" \"tperf\"\n\n# Outra forma com o pacote janitor\nlibrary(janitor)\n\n#&gt; \n#&gt; Anexando pacote: 'janitor'\n\n\n#&gt; Os seguintes objetos são mascarados por 'package:stats':\n#&gt; \n#&gt;     chisq.test, fisher.test\n\nnames(pocos_04A)\n\n#&gt;  [1] \"POCO\"                    \"OPERADOR\"               \n#&gt;  [3] \"ESTADO\"                  \"BACIA\"                  \n#&gt;  [5] \"CAMPO\"                   \"TERRA_MAR\"              \n#&gt;  [7] \"CATEGORIA\"               \"SITUACAO\"               \n#&gt;  [9] \"INICIO\"                  \"TERMINO\"                \n#&gt; [11] \"PROFUNDIDADE_SONDADOR_M\" \"TPERF\"\n\npocos_04A &lt;- clean_names(pocos_04A)\nnames(pocos_04A)\n\n#&gt;  [1] \"poco\"                    \"operador\"               \n#&gt;  [3] \"estado\"                  \"bacia\"                  \n#&gt;  [5] \"campo\"                   \"terra_mar\"              \n#&gt;  [7] \"categoria\"               \"situacao\"               \n#&gt;  [9] \"inicio\"                  \"termino\"                \n#&gt; [11] \"profundidade_sondador_m\" \"tperf\"",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 01"
    ]
  },
  {
    "objectID": "acesso-rstudio.html",
    "href": "acesso-rstudio.html",
    "title": "RStudio Cloud",
    "section": "",
    "text": "Para acessar as tarefas no RStudio Cloud, abra uma conta gratuita no mesmo e acesse o link abaixo\nRStudio",
    "crumbs": [
      "RStudio Cloud",
      "Acesso"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Sobre",
    "section": "",
    "text": "Bom dia / Boa tarde / Boa Noite,\nSou o Professor Ricardo Accioly e esta é a página principal do site da disciplina Mineração de Dados IME05-12547 / IME05-14149 do Departamento de Estatística da UERJ.\nEste site foi construído usando Quarto e foi baseado em diversas fontes de informação obtidas na internet.\nSeguem algumas referências úteis para o Quarto:\nQuarto\nCurso da Duke University\nComo criar um blog com o Quarto\nWebnar do Quarto",
    "crumbs": [
      "Sobre",
      "Sobre"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "IME05-12547/14149 Mineração de Dados",
    "section": "",
    "text": "Bom dia / Boa tarde / Boa Noite,\nAqui vocês vão encontrar alguns dos exemplos apresentados em aula, com uso do R. O objetivo é mostrar as possibilidades de aplicação do R nesta disciplina.\nOs códigos apresentados em aula serão colocado por aqui também.\n[📖] Explorando 01\n[📖] Explorando 01A\n[📖] Explorando 02\n[📖] Explorando 03\n[📖] Explorando 04\n[📖] Explorando 05\n[📖] Explorando 06\n[📖] Explorando 06A\n[📖] Regressão Simples\n[📖] Regressão Múltipla\n[📖] Problemas na Regressão\n[📖] Seleção de Modelos\n[📖] Métodos de Encolhimento\n[📖] Classificação 1os passos\n[📖] KNN\n[📖] Regressão Logística\n[📖] SMOTE\n[📖] LDA e QDA\n[📖] Arvore de Regressão\n[📖] Arvore de Regressão Random Forest\n[📖] Arvore de Regressão GBM\n[📖] Arvore de Regressão XGBoost\n[📖] Arvore de Classificação Única\n[📖] Arvore de Classificação - GBM\n[📖] Arvore de Classificação - XGBoost\n[📖] Análise de Clusters\n[📖] Análise de Associação",
    "crumbs": [
      "Informação da disciplina",
      "Cronograma"
    ]
  },
  {
    "objectID": "semanas/Aula01A.html#lendo-dados-de-arquivos-xlsx",
    "href": "semanas/Aula01A.html#lendo-dados-de-arquivos-xlsx",
    "title": "Carregando dados, Manipulando e Visualizando",
    "section": "Lendo dados de arquivos xlsx",
    "text": "Lendo dados de arquivos xlsx\n\nlibrary(readxl)\ndados_seg &lt;- read_xlsx(\"indicadoressegurancapublicauf.xlsx\", col_names = TRUE, sheet = \"Ocorrências\")\nhead(dados_seg)\n\n#&gt; # A tibble: 6 × 5\n#&gt;   UF    `Tipo Crime`                      Ano Mês     Ocorrências\n#&gt;   &lt;chr&gt; &lt;chr&gt;                           &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;\n#&gt; 1 Acre  Estupro                          2022 janeiro          31\n#&gt; 2 Acre  Furto de veículo                 2022 janeiro          50\n#&gt; 3 Acre  Homicídio doloso                 2022 janeiro          10\n#&gt; 4 Acre  Lesão corporal seguida de morte  2022 janeiro           0\n#&gt; 5 Acre  Roubo a instituição financeira   2022 janeiro           0\n#&gt; 6 Acre  Roubo de carga                   2022 janeiro           0\n\nstr(dados_seg)\n\n#&gt; tibble [20,686 × 5] (S3: tbl_df/tbl/data.frame)\n#&gt;  $ UF         : chr [1:20686] \"Acre\" \"Acre\" \"Acre\" \"Acre\" ...\n#&gt;  $ Tipo Crime : chr [1:20686] \"Estupro\" \"Furto de veículo\" \"Homicídio doloso\" \"Lesão corporal seguida de morte\" ...\n#&gt;  $ Ano        : num [1:20686] 2022 2022 2022 2022 2022 ...\n#&gt;  $ Mês        : chr [1:20686] \"janeiro\" \"janeiro\" \"janeiro\" \"janeiro\" ...\n#&gt;  $ Ocorrências: num [1:20686] 31 50 10 0 0 0 72 0 22 34 ...",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 01A"
    ]
  },
  {
    "objectID": "semanas/Aula01A.html#bibliotecas",
    "href": "semanas/Aula01A.html#bibliotecas",
    "title": "Carregando dados, Manipulando e Visualizando",
    "section": "Bibliotecas",
    "text": "Bibliotecas\nVamos carregar as bibliotecas que serão usadas na manipulação e visualização de dados.\nO pacote tidyverse carrega diversos pacotes muito uteis na manipulação e visualização de dados\n\nlibrary(\"tidyverse\")\n\n\nglimpse(dados_seg)\n\n#&gt; Rows: 20,686\n#&gt; Columns: 5\n#&gt; $ UF           &lt;chr&gt; \"Acre\", \"Acre\", \"Acre\", \"Acre\", \"Acre\", \"Acre\", \"Acre\", \"…\n#&gt; $ `Tipo Crime` &lt;chr&gt; \"Estupro\", \"Furto de veículo\", \"Homicídio doloso\", \"Lesão…\n#&gt; $ Ano          &lt;dbl&gt; 2022, 2022, 2022, 2022, 2022, 2022, 2022, 2022, 2022, 202…\n#&gt; $ Mês          &lt;chr&gt; \"janeiro\", \"janeiro\", \"janeiro\", \"janeiro\", \"janeiro\", \"j…\n#&gt; $ Ocorrências  &lt;dbl&gt; 31, 50, 10, 0, 0, 0, 72, 0, 22, 34, 55, 10, 0, 0, 0, 48, …\n\nunique(dados_seg$UF)\n\n#&gt;  [1] \"Acre\"                \"Alagoas\"             \"Amapá\"              \n#&gt;  [4] \"Amazonas\"            \"Bahia\"               \"Ceará\"              \n#&gt;  [7] \"Distrito Federal\"    \"Espírito Santo\"      \"Goiás\"              \n#&gt; [10] \"Maranhão\"            \"Mato Grosso\"         \"Mato Grosso do Sul\" \n#&gt; [13] \"Minas Gerais\"        \"Pará\"                \"Paraíba\"            \n#&gt; [16] \"Paraná\"              \"Pernambuco\"          \"Piauí\"              \n#&gt; [19] \"Rio Grande do Norte\" \"Rio Grande do Sul\"   \"Rondônia\"           \n#&gt; [22] \"Roraima\"             \"Santa Catarina\"      \"São Paulo\"          \n#&gt; [25] \"Sergipe\"             \"Tocantins\"           \"Rio de Janeiro\"\n\n\n\nlibrary(summarytools)\ndfSummary(dados_seg) |&gt; stview(method = \"render\")\n\n\nData Frame Summary\ndados_seg\nDimensions: 20686 x 5\n  Duplicates: 0\n\n\n\n\n\n\n\n\n\n\n\nNo\nVariable\nStats / Values\nFreqs (% of Valid)\nGraph\nValid\nMissing\n\n\n\n1\nUF [character]\n\n\n1. Amazonas\n\n\n2. Bahia\n\n\n3. Ceará\n\n\n4. Distrito Federal\n\n\n5. Espírito Santo\n\n\n6. Mato Grosso\n\n\n7. Mato Grosso do Sul\n\n\n8. Minas Gerais\n\n\n9. Pará\n\n\n10. Pernambuco\n\n\n[ 17 others ]\n\n\n\n\n774\n(\n3.7%\n)\n\n\n774\n(\n3.7%\n)\n\n\n774\n(\n3.7%\n)\n\n\n774\n(\n3.7%\n)\n\n\n774\n(\n3.7%\n)\n\n\n774\n(\n3.7%\n)\n\n\n774\n(\n3.7%\n)\n\n\n774\n(\n3.7%\n)\n\n\n774\n(\n3.7%\n)\n\n\n774\n(\n3.7%\n)\n\n\n12946\n(\n62.6%\n)\n\n\n\n20686 (100.0%)\n0 (0.0%)\n\n\n2\nTipo Crime [character]\n\n\n1. Estupro\n\n\n2. Furto de veículo\n\n\n3. Homicídio doloso\n\n\n4. Lesão corporal seguida de\n\n\n5. Roubo a instituição finan\n\n\n6. Roubo de carga\n\n\n7. Roubo de veículo\n\n\n8. Roubo seguido de morte (l\n\n\n9. Tentativa de homicídio\n\n\n\n\n2313\n(\n11.2%\n)\n\n\n2316\n(\n11.2%\n)\n\n\n2319\n(\n11.2%\n)\n\n\n2296\n(\n11.1%\n)\n\n\n2268\n(\n11.0%\n)\n\n\n2229\n(\n10.8%\n)\n\n\n2316\n(\n11.2%\n)\n\n\n2315\n(\n11.2%\n)\n\n\n2314\n(\n11.2%\n)\n\n\n\n20686 (100.0%)\n0 (0.0%)\n\n\n3\nAno [numeric]\n\n\nMean (sd) : 2018.1 (2.1)\n\n\nmin ≤ med ≤ max:\n\n\n2015 ≤ 2018 ≤ 2022\n\n\nIQR (CV) : 4 (0)\n\n\n\n\n2015\n:\n2879\n(\n13.9%\n)\n\n\n2016\n:\n2884\n(\n13.9%\n)\n\n\n2017\n:\n2893\n(\n14.0%\n)\n\n\n2018\n:\n2910\n(\n14.1%\n)\n\n\n2019\n:\n2891\n(\n14.0%\n)\n\n\n2020\n:\n2874\n(\n13.9%\n)\n\n\n2021\n:\n2893\n(\n14.0%\n)\n\n\n2022\n:\n462\n(\n2.2%\n)\n\n\n\n20686 (100.0%)\n0 (0.0%)\n\n\n4\nMês [character]\n\n\n1. fevereiro\n\n\n2. janeiro\n\n\n3. abril\n\n\n4. maio\n\n\n5. outubro\n\n\n6. setembro\n\n\n7. junho\n\n\n8. agosto\n\n\n9. dezembro\n\n\n10. julho\n\n\n[ 2 others ]\n\n\n\n\n1917\n(\n9.3%\n)\n\n\n1908\n(\n9.2%\n)\n\n\n1689\n(\n8.2%\n)\n\n\n1689\n(\n8.2%\n)\n\n\n1688\n(\n8.2%\n)\n\n\n1688\n(\n8.2%\n)\n\n\n1687\n(\n8.2%\n)\n\n\n1686\n(\n8.2%\n)\n\n\n1685\n(\n8.1%\n)\n\n\n1684\n(\n8.1%\n)\n\n\n3365\n(\n16.3%\n)\n\n\n\n20686 (100.0%)\n0 (0.0%)\n\n\n5\nOcorrências [numeric]\n\n\nMean (sd) : 206.5 (678.5)\n\n\nmin ≤ med ≤ max:\n\n\n0 ≤ 34 ≤ 10518\n\n\nIQR (CV) : 166 (3.3)\n\n\n1558 distinct values\n\n20686 (100.0%)\n0 (0.0%)\n\n\n\nGenerated by summarytools 1.1.3 (R version 4.5.0)2025-04-25\n\n\n\n\ndados_segRJ &lt;- dados_seg %&gt;% filter(UF==\"Rio de Janeiro\")\nsummary(dados_segRJ)\n\n#&gt;       UF             Tipo Crime             Ano           Mês           \n#&gt;  Length:756         Length:756         Min.   :2015   Length:756        \n#&gt;  Class :character   Class :character   1st Qu.:2016   Class :character  \n#&gt;  Mode  :character   Mode  :character   Median :2018   Mode  :character  \n#&gt;                                        Mean   :2018                     \n#&gt;                                        3rd Qu.:2020                     \n#&gt;                                        Max.   :2021                     \n#&gt;   Ocorrências    \n#&gt;  Min.   :   0.0  \n#&gt;  1st Qu.:  10.0  \n#&gt;  Median : 335.0  \n#&gt;  Mean   : 683.3  \n#&gt;  3rd Qu.: 744.5  \n#&gt;  Max.   :5358.0\n\n\n\nunique(dados_segRJ$`Tipo Crime`)\n\n#&gt; [1] \"Estupro\"                             \"Furto de veículo\"                   \n#&gt; [3] \"Homicídio doloso\"                    \"Lesão corporal seguida de morte\"    \n#&gt; [5] \"Roubo a instituição financeira\"      \"Roubo de carga\"                     \n#&gt; [7] \"Roubo de veículo\"                    \"Roubo seguido de morte (latrocínio)\"\n#&gt; [9] \"Tentativa de homicídio\"\n\ndados_segRJ$`Tipo Crime` &lt;- as.factor(dados_segRJ$`Tipo Crime`)\ndados_segRJ %&gt;% filter(`Tipo Crime`==\"Homicídio doloso\" ) %&gt;% ggplot(aes(x=as.factor(Ano), y=Ocorrências)) + geom_boxplot() +\n  labs(title=\"Homicídio doloso\", x=\"Ano\", y=\"Ocorrências\")\n\n\n\n\n\n\ndados_segRJ %&gt;% filter(`Tipo Crime`==\"Roubo de veículo\" ) %&gt;% ggplot(aes(x=as.factor(Ano), y=Ocorrências)) + geom_boxplot() +\n  labs(title=\"Roubo de veículo\", x=\"Ano\", y=\"Ocorrências\")\n\n\n\n\n\n\n\n\nsintese_RJ &lt;- dados_segRJ %&gt;% group_by(Ano,`Tipo Crime`) %&gt;% summarise(total = sum(Ocorrências))\nsintese_RJ %&gt;% ggplot(aes(x=Ano, y=total, color=`Tipo Crime`)) + geom_point() + geom_line() +\n  labs(title=\"# de crimes no RJ\", x=\"Ano\", y=\"Ocorrências\")",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 01A"
    ]
  },
  {
    "objectID": "semanas/Aula03.html",
    "href": "semanas/Aula03.html",
    "title": "Visualizando as distribuições",
    "section": "",
    "text": "Vamos trabalhar de novo com os dados do gapminder para fazer algumas visualizações de dados que são muito úteis no dia a dia.\nEste material foi adaptado do curso Data Visualization in R with ggplot2 de Kara Woo\n\nlibrary(\"ggplot2\")\nlibrary(\"dplyr\")\nlibrary(\"gapminder\")",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 03"
    ]
  },
  {
    "objectID": "semanas/Aula03.html#carregando-as-bibliotecas",
    "href": "semanas/Aula03.html#carregando-as-bibliotecas",
    "title": "Visualizando as distribuições",
    "section": "",
    "text": "Vamos trabalhar de novo com os dados do gapminder para fazer algumas visualizações de dados que são muito úteis no dia a dia.\nEste material foi adaptado do curso Data Visualization in R with ggplot2 de Kara Woo\n\nlibrary(\"ggplot2\")\nlibrary(\"dplyr\")\nlibrary(\"gapminder\")",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 03"
    ]
  },
  {
    "objectID": "semanas/Aula03.html#selecionando-dados",
    "href": "semanas/Aula03.html#selecionando-dados",
    "title": "Visualizando as distribuições",
    "section": "Selecionando dados",
    "text": "Selecionando dados\n\ngap_07 &lt;- filter(gapminder, year == 2007)",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 03"
    ]
  },
  {
    "objectID": "semanas/Aula03.html#vendo-a-distribuição",
    "href": "semanas/Aula03.html#vendo-a-distribuição",
    "title": "Visualizando as distribuições",
    "section": "Vendo a distribuição",
    "text": "Vendo a distribuição\n\nggplot(gap_07, aes(x = gdpPercap)) +\n  geom_histogram()\n\n#&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 03"
    ]
  },
  {
    "objectID": "semanas/Aula03.html#criando-um-histograma-usando-a-regra-de-sturges",
    "href": "semanas/Aula03.html#criando-um-histograma-usando-a-regra-de-sturges",
    "title": "Visualizando as distribuições",
    "section": "Criando um histograma usando a regra de Sturges",
    "text": "Criando um histograma usando a regra de Sturges\nA regra de Sturges indica 8 faixas enquanto que o padrão do ggplot2 é 30.\n\nggplot(gap_07, aes(x = gdpPercap)) +\n  geom_histogram(bins = round(1+3.322*log10(nrow(gap_07)),0))",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 03"
    ]
  },
  {
    "objectID": "semanas/Aula03.html#criando-um-gráfico-de-densidade",
    "href": "semanas/Aula03.html#criando-um-gráfico-de-densidade",
    "title": "Visualizando as distribuições",
    "section": "Criando um gráfico de densidade",
    "text": "Criando um gráfico de densidade\n\nggplot(gap_07, aes(x = gdpPercap)) +\n  geom_density()",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 03"
    ]
  },
  {
    "objectID": "semanas/Aula03.html#criando-um-box-plot",
    "href": "semanas/Aula03.html#criando-um-box-plot",
    "title": "Visualizando as distribuições",
    "section": "Criando um box-plot",
    "text": "Criando um box-plot\n\nggplot(gap_07, aes(x = continent, y = lifeExp)) +\n  geom_boxplot()",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 03"
    ]
  },
  {
    "objectID": "semanas/Aula03.html#criando-um-box-plot-com-visão-dos-dados",
    "href": "semanas/Aula03.html#criando-um-box-plot-com-visão-dos-dados",
    "title": "Visualizando as distribuições",
    "section": "Criando um box-plot com visão dos dados",
    "text": "Criando um box-plot com visão dos dados\n\nggplot(gap_07, aes(x = continent, y = lifeExp)) +\n  geom_boxplot() +\n  geom_jitter(width = 0.5, alpha = 0.2)",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 03"
    ]
  },
  {
    "objectID": "semanas/Aula03.html#matriz-de-correlações",
    "href": "semanas/Aula03.html#matriz-de-correlações",
    "title": "Visualizando as distribuições",
    "section": "Matriz de Correlações",
    "text": "Matriz de Correlações\n\nlibrary(corrplot)\n\n#&gt; corrplot 0.92 loaded\n\ngap_07_s &lt;- gap_07 %&gt;% select(lifeExp, pop, gdpPercap)\nmat_corr &lt;- cor(gap_07_s)\ncorrplot(mat_corr, method = \"number\", col = \"black\", cl.pos = \"n\")\n\n\n\n\n\n\ncorrplot(mat_corr, method = \"number\")\n\n\n\n\n\n\ncorrplot(mat_corr)",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 03"
    ]
  },
  {
    "objectID": "semanas/Aula03.html#splom",
    "href": "semanas/Aula03.html#splom",
    "title": "Visualizando as distribuições",
    "section": "SPLOM",
    "text": "SPLOM\n\nlibrary(psych)\n\n#&gt; \n#&gt; Anexando pacote: 'psych'\n\n\n#&gt; Os seguintes objetos são mascarados por 'package:ggplot2':\n#&gt; \n#&gt;     %+%, alpha\n\npairs.panels(gap_07_s)\n\n\n\n\n\n\n\n\nlibrary(GGally)\n\n#&gt; Registered S3 method overwritten by 'GGally':\n#&gt;   method from   \n#&gt;   +.gg   ggplot2\n\nggpairs(gap_07_s)",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 03"
    ]
  },
  {
    "objectID": "semanas/Aula03.html#descrevendo-a-distribuição",
    "href": "semanas/Aula03.html#descrevendo-a-distribuição",
    "title": "Visualizando as distribuições",
    "section": "Descrevendo a distribuição",
    "text": "Descrevendo a distribuição\n\nlibrary(datawizard)\n\n#&gt; \n#&gt; Anexando pacote: 'datawizard'\n\n\n#&gt; O seguinte objeto é mascarado por 'package:psych':\n#&gt; \n#&gt;     rescale\n\ndescribe_distribution(gap_07_s)\n\n#&gt; Variable  |     Mean |       SD |      IQR |                Range | Skewness | Kurtosis |   n | n_Missing\n#&gt; ---------------------------------------------------------------------------------------------------------\n#&gt; lifeExp   |    67.01 |    12.07 |    19.59 |       [39.61, 82.60] |    -0.69 |    -0.83 | 142 |         0\n#&gt; pop       | 4.40e+07 | 1.48e+08 | 2.78e+07 | [2.00e+05, 1.32e+09] |     7.40 |    58.33 | 142 |         0\n#&gt; gdpPercap | 11680.07 | 12859.94 | 16579.19 |   [277.55, 49357.19] |     1.22 |     0.35 | 142 |         0",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 03"
    ]
  },
  {
    "objectID": "semanas/Aula05.html",
    "href": "semanas/Aula05.html",
    "title": "Suavização",
    "section": "",
    "text": "library(\"ggplot2\")\nlibrary(\"gapminder\")\nlibrary(\"dplyr\")",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 05"
    ]
  },
  {
    "objectID": "semanas/Aula05.html#bibliotecas",
    "href": "semanas/Aula05.html#bibliotecas",
    "title": "Suavização",
    "section": "",
    "text": "library(\"ggplot2\")\nlibrary(\"gapminder\")\nlibrary(\"dplyr\")",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 05"
    ]
  },
  {
    "objectID": "semanas/Aula05.html#suavização",
    "href": "semanas/Aula05.html#suavização",
    "title": "Suavização",
    "section": "Suavização",
    "text": "Suavização\n(locally estimated scatterplot smoothing/Local Polynomial Regression Fitting)\n\ngap_07 &lt;- filter(gapminder, year == 2007)\nggplot(gap_07, aes(x = gdpPercap, y = lifeExp)) +\n  geom_point() +\n  geom_smooth()\n\n#&gt; `geom_smooth()` using method = 'loess' and formula = 'y ~ x'",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 05"
    ]
  },
  {
    "objectID": "semanas/Aula05.html#fazendo-o-suavizador-mais-nervoso",
    "href": "semanas/Aula05.html#fazendo-o-suavizador-mais-nervoso",
    "title": "Suavização",
    "section": "Fazendo o suavizador mais nervoso",
    "text": "Fazendo o suavizador mais nervoso\n\nggplot(gap_07, aes(x = gdpPercap, y = lifeExp)) +\n  geom_point() +\n  geom_smooth(span = 0.2)",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 05"
    ]
  },
  {
    "objectID": "semanas/Aula05.html#fazendo-o-suavizador-menos-nervoso",
    "href": "semanas/Aula05.html#fazendo-o-suavizador-menos-nervoso",
    "title": "Suavização",
    "section": "Fazendo o suavizador menos nervoso",
    "text": "Fazendo o suavizador menos nervoso\n\nggplot(gap_07, aes(x = gdpPercap, y = lifeExp)) +\n  geom_point() +\n  geom_smooth(span = 0.9)",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 05"
    ]
  },
  {
    "objectID": "semanas/Aula05.html#removendo-intervalos-de-confiança",
    "href": "semanas/Aula05.html#removendo-intervalos-de-confiança",
    "title": "Suavização",
    "section": "Removendo intervalos de confiança",
    "text": "Removendo intervalos de confiança\n\nggplot(gap_07, aes(x = gdpPercap, y = lifeExp)) +\n  geom_point() +\n  geom_smooth(se = FALSE)",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 05"
    ]
  },
  {
    "objectID": "semanas/Aula05.html#usando-ic-de-90",
    "href": "semanas/Aula05.html#usando-ic-de-90",
    "title": "Suavização",
    "section": "Usando IC de 90%",
    "text": "Usando IC de 90%\n\nggplot(gap_07, aes(x = gdpPercap, y = lifeExp)) +\n  geom_point() +\n  geom_smooth(level = 0.90)",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 05"
    ]
  },
  {
    "objectID": "semanas/Aula05.html#usando-um-modelo-linear-ao-invés-do-loess",
    "href": "semanas/Aula05.html#usando-um-modelo-linear-ao-invés-do-loess",
    "title": "Suavização",
    "section": "Usando um modelo linear ao invés do loess",
    "text": "Usando um modelo linear ao invés do loess\n\nggplot(gap_07, aes(x = gdpPercap, y = lifeExp)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 05"
    ]
  },
  {
    "objectID": "semanas/Aula05.html#usando-basic-splines-para-melhorar-o-ajuste",
    "href": "semanas/Aula05.html#usando-basic-splines-para-melhorar-o-ajuste",
    "title": "Suavização",
    "section": "Usando basic splines para melhorar o ajuste",
    "text": "Usando basic splines para melhorar o ajuste\n\nggplot(gap_07, aes(x = gdpPercap, y = lifeExp)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", formula = y ~ splines::bs(x, df = 3))",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 05"
    ]
  },
  {
    "objectID": "semanas/Aula05.html#usando-o-gam-general-addtive-models-com-regressão-spline",
    "href": "semanas/Aula05.html#usando-o-gam-general-addtive-models-com-regressão-spline",
    "title": "Suavização",
    "section": "Usando o gam (general addtive models) com regressão spline",
    "text": "Usando o gam (general addtive models) com regressão spline\n\nggplot(gap_07, aes(x = gdpPercap, y = lifeExp)) +\n  geom_point() +\n  geom_smooth(method = \"gam\", formula = y ~ s(x))",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 05"
    ]
  },
  {
    "objectID": "semanas/Aula05.html#começando-a-construir-um-gráfico-do-tipo-facet-com-suavizações",
    "href": "semanas/Aula05.html#começando-a-construir-um-gráfico-do-tipo-facet-com-suavizações",
    "title": "Suavização",
    "section": "Começando a construir um gráfico do tipo facet com suavizações",
    "text": "Começando a construir um gráfico do tipo facet com suavizações\n\nggplot(gap_07, aes(x = gdpPercap, y = lifeExp)) +\n  geom_point()",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 05"
    ]
  },
  {
    "objectID": "semanas/Aula05.html#dividindo-por-continente",
    "href": "semanas/Aula05.html#dividindo-por-continente",
    "title": "Suavização",
    "section": "Dividindo por continente",
    "text": "Dividindo por continente\n\nggplot(gap_07, aes(x = gdpPercap, y = lifeExp)) +\n  geom_point() +\n  facet_wrap(~ continent)",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 05"
    ]
  },
  {
    "objectID": "semanas/Aula05.html#adicionando-suavizadores",
    "href": "semanas/Aula05.html#adicionando-suavizadores",
    "title": "Suavização",
    "section": "Adicionando suavizadores",
    "text": "Adicionando suavizadores\n\nggplot(gap_07, aes(x = gdpPercap, y = lifeExp)) +\n  geom_point() +\n  facet_wrap(~ continent) +\n  geom_smooth()",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 05"
    ]
  },
  {
    "objectID": "semanas/Aula05.html#colorindo-por-continente",
    "href": "semanas/Aula05.html#colorindo-por-continente",
    "title": "Suavização",
    "section": "Colorindo por continente",
    "text": "Colorindo por continente\n\nggplot(gap_07, aes(x = gdpPercap, y = lifeExp, color = continent)) +\n  geom_point() +\n  facet_wrap(~ continent) +\n  geom_smooth()",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 05"
    ]
  },
  {
    "objectID": "semanas/Aula05.html#colorindo-somente-a-curva",
    "href": "semanas/Aula05.html#colorindo-somente-a-curva",
    "title": "Suavização",
    "section": "Colorindo somente a curva",
    "text": "Colorindo somente a curva\n\nggplot(gap_07, aes(x = gdpPercap, y = lifeExp)) +\n  geom_point() +\n  facet_wrap(~ continent) +\n  geom_smooth(aes(color = continent))",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 05"
    ]
  },
  {
    "objectID": "semanas/Aula06A.html",
    "href": "semanas/Aula06A.html",
    "title": "Explorando Dados",
    "section": "",
    "text": "library(tidyverse)\ndata(iris)",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 06A"
    ]
  },
  {
    "objectID": "semanas/Aula06A.html#carregando-bibliotecas",
    "href": "semanas/Aula06A.html#carregando-bibliotecas",
    "title": "Explorando Dados",
    "section": "",
    "text": "library(tidyverse)\ndata(iris)",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 06A"
    ]
  },
  {
    "objectID": "semanas/Aula06A.html#o-que-temos-aqui",
    "href": "semanas/Aula06A.html#o-que-temos-aqui",
    "title": "Explorando Dados",
    "section": "O que temos aqui?",
    "text": "O que temos aqui?\n\nhead(iris)\n\n#&gt;   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n#&gt; 1          5.1         3.5          1.4         0.2  setosa\n#&gt; 2          4.9         3.0          1.4         0.2  setosa\n#&gt; 3          4.7         3.2          1.3         0.2  setosa\n#&gt; 4          4.6         3.1          1.5         0.2  setosa\n#&gt; 5          5.0         3.6          1.4         0.2  setosa\n#&gt; 6          5.4         3.9          1.7         0.4  setosa\n\niris %&gt;% count(Species)\n\n#&gt;      Species  n\n#&gt; 1     setosa 50\n#&gt; 2 versicolor 50\n#&gt; 3  virginica 50",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 06A"
    ]
  },
  {
    "objectID": "semanas/Aula06A.html#quais-são-as-médias",
    "href": "semanas/Aula06A.html#quais-são-as-médias",
    "title": "Explorando Dados",
    "section": "Quais são as médias?",
    "text": "Quais são as médias?\n\niris %&gt;% \n  group_by(Species) %&gt;% \n  summarize(across(where(is.numeric), ~ mean(.x, na.rm = TRUE)))\n\n#&gt; # A tibble: 3 × 5\n#&gt;   Species    Sepal.Length Sepal.Width Petal.Length Petal.Width\n#&gt;   &lt;fct&gt;             &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;\n#&gt; 1 setosa             5.01        3.43         1.46       0.246\n#&gt; 2 versicolor         5.94        2.77         4.26       1.33 \n#&gt; 3 virginica          6.59        2.97         5.55       2.03",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 06A"
    ]
  },
  {
    "objectID": "semanas/Aula06A.html#vamos-ver-se-temos-alguma-relação-entre-as-variáveis",
    "href": "semanas/Aula06A.html#vamos-ver-se-temos-alguma-relação-entre-as-variáveis",
    "title": "Explorando Dados",
    "section": "Vamos ver se temos alguma relação entre as variáveis",
    "text": "Vamos ver se temos alguma relação entre as variáveis\n\niris %&gt;% \n  group_by(Species) %&gt;% ggplot(aes(x=Sepal.Width, y=Sepal.Length, \n                                   color=Species)) + geom_point()",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 06A"
    ]
  },
  {
    "objectID": "semanas/Aula06A.html#vamos-ver-se-temos-alguma-relação-entre-as-variáveis-2",
    "href": "semanas/Aula06A.html#vamos-ver-se-temos-alguma-relação-entre-as-variáveis-2",
    "title": "Explorando Dados",
    "section": "Vamos ver se temos alguma relação entre as variáveis 2",
    "text": "Vamos ver se temos alguma relação entre as variáveis 2\n\niris %&gt;% \n  group_by(Species) %&gt;% \n  ggplot(aes(x=Sepal.Width, y=Sepal.Length, color=Species)) +\n  geom_point() + geom_smooth(method = \"lm\", se=FALSE)\n\n#&gt; `geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 06A"
    ]
  },
  {
    "objectID": "semanas/Aula06A.html#vamos-ver-se-temos-alguma-relação-entre-as-variáveis-3",
    "href": "semanas/Aula06A.html#vamos-ver-se-temos-alguma-relação-entre-as-variáveis-3",
    "title": "Explorando Dados",
    "section": "Vamos ver se temos alguma relação entre as variáveis 3",
    "text": "Vamos ver se temos alguma relação entre as variáveis 3\n\niris %&gt;% \n  group_by(Species) %&gt;% ggplot(aes(x=Petal.Width, y=Petal.Length, \n                                   color=Species)) + geom_point()",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 06A"
    ]
  },
  {
    "objectID": "semanas/Aula06A.html#vamos-ver-se-temos-alguma-relação-entre-as-variáveis-4",
    "href": "semanas/Aula06A.html#vamos-ver-se-temos-alguma-relação-entre-as-variáveis-4",
    "title": "Explorando Dados",
    "section": "Vamos ver se temos alguma relação entre as variáveis 4",
    "text": "Vamos ver se temos alguma relação entre as variáveis 4\n\niris %&gt;% \n  group_by(Species) %&gt;% \n  ggplot(aes(x=Petal.Width, y=Petal.Length, color=Species)) +\n  geom_point() + geom_smooth(method = \"lm\")\n\n#&gt; `geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 06A"
    ]
  },
  {
    "objectID": "semanas/Aula06A.html#vamos-ver-como-se-distribui-o-petal.width",
    "href": "semanas/Aula06A.html#vamos-ver-como-se-distribui-o-petal.width",
    "title": "Explorando Dados",
    "section": "Vamos ver como se distribui o Petal.Width",
    "text": "Vamos ver como se distribui o Petal.Width\n\niris %&gt;% \n  group_by(Species) %&gt;% ggplot(aes(x=Petal.Width, \n                                   fill=Species)) + \n                                   geom_histogram()\n\n#&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 06A"
    ]
  },
  {
    "objectID": "semanas/Aula06A.html#vamos-ver-como-se-distribui-o-petal.length",
    "href": "semanas/Aula06A.html#vamos-ver-como-se-distribui-o-petal.length",
    "title": "Explorando Dados",
    "section": "Vamos ver como se distribui o Petal.Length",
    "text": "Vamos ver como se distribui o Petal.Length\n\niris %&gt;% \n  group_by(Species) %&gt;% \n  ggplot(aes(x=Petal.Length, fill=Species)) + \n  geom_histogram()\n\n#&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.",
    "crumbs": [
      "Conteúdo Semanal",
      "Explorando 06A"
    ]
  },
  {
    "objectID": "semanas/Aula07.2.html",
    "href": "semanas/Aula07.2.html",
    "title": "Regressão Linear Múltipla",
    "section": "",
    "text": "library(tidyverse)\n\n#&gt; ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n#&gt; ✔ dplyr     1.1.4     ✔ readr     2.1.5\n#&gt; ✔ forcats   1.0.0     ✔ stringr   1.5.1\n#&gt; ✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n#&gt; ✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n#&gt; ✔ purrr     1.0.2     \n#&gt; ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n#&gt; ✖ purrr::%||%()   masks base::%||%()\n#&gt; ✖ dplyr::filter() masks stats::filter()\n#&gt; ✖ dplyr::lag()    masks stats::lag()\n#&gt; ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors",
    "crumbs": [
      "Conteúdo Semanal",
      "Regressão Múltipla"
    ]
  },
  {
    "objectID": "semanas/Aula07.2.html#carregando-bibliotecas",
    "href": "semanas/Aula07.2.html#carregando-bibliotecas",
    "title": "Regressão Linear Múltipla",
    "section": "",
    "text": "library(tidyverse)\n\n#&gt; ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n#&gt; ✔ dplyr     1.1.4     ✔ readr     2.1.5\n#&gt; ✔ forcats   1.0.0     ✔ stringr   1.5.1\n#&gt; ✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n#&gt; ✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n#&gt; ✔ purrr     1.0.2     \n#&gt; ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n#&gt; ✖ purrr::%||%()   masks base::%||%()\n#&gt; ✖ dplyr::filter() masks stats::filter()\n#&gt; ✖ dplyr::lag()    masks stats::lag()\n#&gt; ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors",
    "crumbs": [
      "Conteúdo Semanal",
      "Regressão Múltipla"
    ]
  },
  {
    "objectID": "semanas/Aula07.2.html#dados-de-propaganda",
    "href": "semanas/Aula07.2.html#dados-de-propaganda",
    "title": "Regressão Linear Múltipla",
    "section": "Dados de propaganda",
    "text": "Dados de propaganda\nO conjunto de dados contém estatísticas sobre as vendas de um produto em 200 diferentes mercados, juntamente com orçamentos publicitários em cada um desses mercados, para diferentes canais de mídia: TV, rádio e jornal. As vendas estão em milhares de unidades e o orçamento está em milhares de dólares.\n\nlibrary(readxl)\npropaganda &lt;- read_excel(\"Propaganda.xlsx\")\nsummary(propaganda)\n\n#&gt;        TV             Radio          Newspaper          Sales      \n#&gt;  Min.   :  0.70   Min.   : 0.000   Min.   :  0.30   Min.   : 1.60  \n#&gt;  1st Qu.: 74.38   1st Qu.: 9.975   1st Qu.: 12.75   1st Qu.:10.38  \n#&gt;  Median :149.75   Median :22.900   Median : 25.75   Median :12.90  \n#&gt;  Mean   :147.04   Mean   :23.264   Mean   : 30.55   Mean   :14.02  \n#&gt;  3rd Qu.:218.82   3rd Qu.:36.525   3rd Qu.: 45.10   3rd Qu.:17.40  \n#&gt;  Max.   :296.40   Max.   :49.600   Max.   :114.00   Max.   :27.00",
    "crumbs": [
      "Conteúdo Semanal",
      "Regressão Múltipla"
    ]
  },
  {
    "objectID": "semanas/Aula07.2.html#renomeando",
    "href": "semanas/Aula07.2.html#renomeando",
    "title": "Regressão Linear Múltipla",
    "section": "Renomeando",
    "text": "Renomeando\n\npropaganda &lt;- propaganda %&gt;% rename(Jornal = Newspaper, Vendas = Sales)",
    "crumbs": [
      "Conteúdo Semanal",
      "Regressão Múltipla"
    ]
  },
  {
    "objectID": "semanas/Aula07.2.html#sumario",
    "href": "semanas/Aula07.2.html#sumario",
    "title": "Regressão Linear Múltipla",
    "section": "Sumario",
    "text": "Sumario\n\nsummary(propaganda)\n\n#&gt;        TV             Radio            Jornal           Vendas     \n#&gt;  Min.   :  0.70   Min.   : 0.000   Min.   :  0.30   Min.   : 1.60  \n#&gt;  1st Qu.: 74.38   1st Qu.: 9.975   1st Qu.: 12.75   1st Qu.:10.38  \n#&gt;  Median :149.75   Median :22.900   Median : 25.75   Median :12.90  \n#&gt;  Mean   :147.04   Mean   :23.264   Mean   : 30.55   Mean   :14.02  \n#&gt;  3rd Qu.:218.82   3rd Qu.:36.525   3rd Qu.: 45.10   3rd Qu.:17.40  \n#&gt;  Max.   :296.40   Max.   :49.600   Max.   :114.00   Max.   :27.00\n\nnrow(propaganda)\n\n#&gt; [1] 200",
    "crumbs": [
      "Conteúdo Semanal",
      "Regressão Múltipla"
    ]
  },
  {
    "objectID": "semanas/Aula07.2.html#criando-amostra-de-treino-e-teste",
    "href": "semanas/Aula07.2.html#criando-amostra-de-treino-e-teste",
    "title": "Regressão Linear Múltipla",
    "section": "Criando amostra de treino e teste",
    "text": "Criando amostra de treino e teste\n\nlibrary(caret)\n\n#&gt; Carregando pacotes exigidos: lattice\n\n\n#&gt; \n#&gt; Anexando pacote: 'caret'\n\n\n#&gt; O seguinte objeto é mascarado por 'package:purrr':\n#&gt; \n#&gt;     lift\n\nset.seed(21)\ny &lt;- propaganda$Vendas\nindice_teste &lt;- createDataPartition(y, times = 1, p = 0.40, list = FALSE)\n\nconj_treino &lt;- propaganda[-indice_teste, ]\nconj_teste &lt;- propaganda[indice_teste, ]\n\nstr(conj_treino)\n\n#&gt; tibble [119 × 4] (S3: tbl_df/tbl/data.frame)\n#&gt;  $ TV    : num [1:119] 230.1 151.5 180.8 199.8 66.1 ...\n#&gt;  $ Radio : num [1:119] 37.8 41.3 10.8 2.6 5.8 35.1 7.6 47.7 20.5 23.9 ...\n#&gt;  $ Jornal: num [1:119] 69.2 58.5 58.4 21.2 24.2 65.9 7.2 52.9 18.3 19.1 ...\n#&gt;  $ Vendas: num [1:119] 22.1 18.5 12.9 10.6 8.6 9.2 9.7 22.4 11.3 14.6 ...\n\nstr(conj_teste)\n\n#&gt; tibble [81 × 4] (S3: tbl_df/tbl/data.frame)\n#&gt;  $ TV    : num [1:81] 44.5 17.2 8.7 57.5 120.2 ...\n#&gt;  $ Radio : num [1:81] 39.3 45.9 48.9 32.8 19.6 2.1 24 32.9 36.6 39.6 ...\n#&gt;  $ Jornal: num [1:81] 45.1 69.3 75 23.5 11.6 1 4 46 114 55.8 ...\n#&gt;  $ Vendas: num [1:81] 10.4 9.3 7.2 11.8 13.2 4.8 17.4 19 12.5 24.4 ...\n\nlibrary(gt)\ngt(head(conj_treino, 6))  %&gt;% \n  tab_header(title = \"Conjunto de Treino\")\n\n\n\n\n\n\nConjunto de Treino\n\n\nTV\nRadio\nJornal\nVendas\n\n\n\n\n230.1\n37.8\n69.2\n22.1\n\n\n151.5\n41.3\n58.5\n18.5\n\n\n180.8\n10.8\n58.4\n12.9\n\n\n199.8\n2.6\n21.2\n10.6\n\n\n66.1\n5.8\n24.2\n8.6\n\n\n23.8\n35.1\n65.9\n9.2",
    "crumbs": [
      "Conteúdo Semanal",
      "Regressão Múltipla"
    ]
  },
  {
    "objectID": "semanas/Aula07.2.html#e-se-eu-usar-uma-outra-semente",
    "href": "semanas/Aula07.2.html#e-se-eu-usar-uma-outra-semente",
    "title": "Regressão Linear Múltipla",
    "section": "E se eu usar uma outra semente?",
    "text": "E se eu usar uma outra semente?\n\nset.seed(1234)\ny2 &lt;- propaganda$Vendas\nindice_teste2 &lt;- createDataPartition(y2, times = 1, p = 0.40, list = FALSE)\n\nconj_treino2 &lt;- propaganda[-indice_teste2, ]\nconj_teste2 &lt;- propaganda[indice_teste2, ]\n\ngt(head(conj_treino2, 6))  %&gt;% \n  tab_header(title = \"2o Conjunto de Treino\")\n\n\n\n\n\n\n2o Conjunto de Treino\n\n\nTV\nRadio\nJornal\nVendas\n\n\n\n\n44.5\n39.3\n45.1\n10.4\n\n\n17.2\n45.9\n69.3\n9.3\n\n\n151.5\n41.3\n58.5\n18.5\n\n\n8.7\n48.9\n75.0\n7.2\n\n\n120.2\n19.6\n11.6\n13.2\n\n\n8.6\n2.1\n1.0\n4.8\n\n\n\n\n\n\n\nlibrary(dlookr)\ndiagnose_numeric(conj_treino)\n\n#&gt; # A tibble: 4 × 10\n#&gt;   variables   min    Q1  mean median    Q3   max  zero minus outlier\n#&gt;   &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt;   &lt;int&gt;\n#&gt; 1 TV          0.7  77.3 151.   150.  225.  294.      0     0       0\n#&gt; 2 Radio       0.3   8.3  21.3   20.5  32.6  49.4     0     0       0\n#&gt; 3 Jornal      1.8  13.6  29.4   24.2  41    89.4     0     0       1\n#&gt; 4 Vendas      1.6  10.4  13.9   12.9  17.2  26.2     0     0       0\n\ndiagnose_numeric(conj_treino2)\n\n#&gt; # A tibble: 4 × 10\n#&gt;   variables   min    Q1  mean median    Q3   max  zero minus outlier\n#&gt;   &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt;   &lt;int&gt;\n#&gt; 1 TV          0.7  67.6 143.   143.  217.  294.      0     0       0\n#&gt; 2 Radio       0.4  12.4  24.3   24    36.3  49.6     0     0       0\n#&gt; 3 Jornal      0.3  12.5  29.7   26.4  43.8  89.4     0     0       0\n#&gt; 4 Vendas      1.6  10.4  14.0   12.8  17.4  27       0     0       0\n\n\nVeja que as amostram ficaram com resultados diferentes, o que levaria a obtermos modelos diferentes num ajuste de regressão!",
    "crumbs": [
      "Conteúdo Semanal",
      "Regressão Múltipla"
    ]
  },
  {
    "objectID": "semanas/Aula07.2.html#regressão-simples",
    "href": "semanas/Aula07.2.html#regressão-simples",
    "title": "Regressão Linear Múltipla",
    "section": "Regressão Simples",
    "text": "Regressão Simples\n\nmod1 &lt;- lm( Vendas ~ TV, data = conj_treino)\nmod2 &lt;- lm( Vendas ~ Radio, data = conj_treino)\nmod3 &lt;- lm( Vendas ~ Jornal, data = conj_treino)",
    "crumbs": [
      "Conteúdo Semanal",
      "Regressão Múltipla"
    ]
  },
  {
    "objectID": "semanas/Aula07.2.html#avaliando-as-correlações",
    "href": "semanas/Aula07.2.html#avaliando-as-correlações",
    "title": "Regressão Linear Múltipla",
    "section": "Avaliando as correlações",
    "text": "Avaliando as correlações\n\nlibrary(corrplot)\n\n#&gt; corrplot 0.92 loaded\n\nmat_corr &lt;- cor(conj_treino)\ncorrplot(mat_corr)",
    "crumbs": [
      "Conteúdo Semanal",
      "Regressão Múltipla"
    ]
  },
  {
    "objectID": "semanas/Aula07.2.html#a-regressão-multipla",
    "href": "semanas/Aula07.2.html#a-regressão-multipla",
    "title": "Regressão Linear Múltipla",
    "section": "1a Regressão Multipla",
    "text": "1a Regressão Multipla\n\nlibrary(car)\n\n#&gt; Carregando pacotes exigidos: carData\n\n\n#&gt; \n#&gt; Anexando pacote: 'car'\n\n\n#&gt; O seguinte objeto é mascarado por 'package:dplyr':\n#&gt; \n#&gt;     recode\n\n\n#&gt; O seguinte objeto é mascarado por 'package:purrr':\n#&gt; \n#&gt;     some\n\nscatterplotMatrix(conj_treino)\n\n\n\n\n\n\nmod4 &lt;- lm( Vendas ~ TV + Radio + Jornal, data = conj_treino)\nsummary(mod4)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = Vendas ~ TV + Radio + Jornal, data = conj_treino)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -9.1595 -0.6961  0.2676  1.0298  2.5871 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept) 3.268910   0.391828   8.343 1.81e-13 ***\n#&gt; TV          0.042705   0.001776  24.039  &lt; 2e-16 ***\n#&gt; Radio       0.186439   0.011458  16.272  &lt; 2e-16 ***\n#&gt; Jornal      0.008931   0.008292   1.077    0.284    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 1.606 on 115 degrees of freedom\n#&gt; Multiple R-squared:  0.9002, Adjusted R-squared:  0.8976 \n#&gt; F-statistic: 345.9 on 3 and 115 DF,  p-value: &lt; 2.2e-16\n\n\nVejam que ao analisarmos a estatística t de Jornal percebemos que não podemos rejeitar a hipótese de que o coeficiente de Jornal possa ser zero.\nVamos refazer o modleo sem Jornal.",
    "crumbs": [
      "Conteúdo Semanal",
      "Regressão Múltipla"
    ]
  },
  {
    "objectID": "semanas/Aula07.2.html#a-regressao-multipla",
    "href": "semanas/Aula07.2.html#a-regressao-multipla",
    "title": "Regressão Linear Múltipla",
    "section": "2a Regressao Multipla",
    "text": "2a Regressao Multipla\n\nmod5 &lt;- lm( Vendas ~ TV + Radio, data = conj_treino)\nsummary(mod5)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = Vendas ~ TV + Radio, data = conj_treino)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -9.4585 -0.6886  0.1687  1.0799  2.5529 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept) 3.441076   0.357985   9.612   &lt;2e-16 ***\n#&gt; TV          0.042575   0.001774  24.005   &lt;2e-16 ***\n#&gt; Radio       0.191607   0.010412  18.402   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 1.607 on 116 degrees of freedom\n#&gt; Multiple R-squared:  0.8992, Adjusted R-squared:  0.8975 \n#&gt; F-statistic: 517.5 on 2 and 116 DF,  p-value: &lt; 2.2e-16\n\n\nAgora todas as variáveis tem indicação de significância estatística.",
    "crumbs": [
      "Conteúdo Semanal",
      "Regressão Múltipla"
    ]
  },
  {
    "objectID": "semanas/Aula07.2.html#confirmando-o-teste-t-com-o-teste-f-anova",
    "href": "semanas/Aula07.2.html#confirmando-o-teste-t-com-o-teste-f-anova",
    "title": "Regressão Linear Múltipla",
    "section": "Confirmando o teste t com o teste F (ANOVA)",
    "text": "Confirmando o teste t com o teste F (ANOVA)\n\nanova(mod5, mod4)\n\n#&gt; Analysis of Variance Table\n#&gt; \n#&gt; Model 1: Vendas ~ TV + Radio\n#&gt; Model 2: Vendas ~ TV + Radio + Jornal\n#&gt;   Res.Df    RSS Df Sum of Sq    F Pr(&gt;F)\n#&gt; 1    116 299.47                         \n#&gt; 2    115 296.48  1    2.9906 1.16 0.2837",
    "crumbs": [
      "Conteúdo Semanal",
      "Regressão Múltipla"
    ]
  },
  {
    "objectID": "semanas/Aula07.2.html#vamos-comparar-com-o-modelo-só-com-tv",
    "href": "semanas/Aula07.2.html#vamos-comparar-com-o-modelo-só-com-tv",
    "title": "Regressão Linear Múltipla",
    "section": "Vamos comparar com o modelo só com TV",
    "text": "Vamos comparar com o modelo só com TV\n\nanova(mod1, mod5)\n\n#&gt; Analysis of Variance Table\n#&gt; \n#&gt; Model 1: Vendas ~ TV\n#&gt; Model 2: Vendas ~ TV + Radio\n#&gt;   Res.Df     RSS Df Sum of Sq      F    Pr(&gt;F)    \n#&gt; 1    117 1173.73                                  \n#&gt; 2    116  299.47  1    874.26 338.65 &lt; 2.2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nVeja que agora a diferença é significativa. O melhor modelo é o com TV e Radio (mod5)",
    "crumbs": [
      "Conteúdo Semanal",
      "Regressão Múltipla"
    ]
  },
  {
    "objectID": "semanas/Aula07.2.html#calculando-o-erro-padrão-do-resíduo-com-amostra-de-teste",
    "href": "semanas/Aula07.2.html#calculando-o-erro-padrão-do-resíduo-com-amostra-de-teste",
    "title": "Regressão Linear Múltipla",
    "section": "Calculando o erro padrão do resíduo com amostra de teste",
    "text": "Calculando o erro padrão do resíduo com amostra de teste\n\nsqrt(mean((conj_teste$Vendas - predict(mod5, conj_teste)) ^ 2)) \n\n#&gt; [1] 1.846764",
    "crumbs": [
      "Conteúdo Semanal",
      "Regressão Múltipla"
    ]
  },
  {
    "objectID": "semanas/Aula07.2.html#comparando-com-a-melhor-regressão-simples",
    "href": "semanas/Aula07.2.html#comparando-com-a-melhor-regressão-simples",
    "title": "Regressão Linear Múltipla",
    "section": "Comparando com a melhor regressão simples",
    "text": "Comparando com a melhor regressão simples\n\n## Modelo com somente TV\nsummary(mod1)$sigma\n\n#&gt; [1] 3.167319\n\nsummary(mod1)$r.squared\n\n#&gt; [1] 0.605027\n\nsqrt(mean((conj_teste$Vendas - predict(mod1, conj_teste)) ^ 2))\n\n#&gt; [1] 3.41382\n\n## Modelo com TV e Jornal\nsummary(mod5)$sigma\n\n#&gt; [1] 1.606744\n\nsummary(mod5)$adj.r.squared\n\n#&gt; [1] 0.8974883\n\nsqrt(mean((conj_teste$Vendas - predict(mod5, conj_teste)) ^ 2))\n\n#&gt; [1] 1.846764",
    "crumbs": [
      "Conteúdo Semanal",
      "Regressão Múltipla"
    ]
  },
  {
    "objectID": "semanas/Aula07.2.html#comparando-valor-real-vs-ajustado",
    "href": "semanas/Aula07.2.html#comparando-valor-real-vs-ajustado",
    "title": "Regressão Linear Múltipla",
    "section": "Comparando valor real vs ajustado",
    "text": "Comparando valor real vs ajustado\n\nconj_treino$Previsoes &lt;- predict(mod5, data=conj_treino)\nggplot(conj_treino, aes(x=Previsoes, y=Vendas)) + \n  geom_point() +\n  geom_abline(color = \"darkblue\") +\n  ggtitle(\"Vendas vs. Previsões do modelo linear\")",
    "crumbs": [
      "Conteúdo Semanal",
      "Regressão Múltipla"
    ]
  },
  {
    "objectID": "semanas/Aula07.2.html#análise-inicial-dos-resíduos",
    "href": "semanas/Aula07.2.html#análise-inicial-dos-resíduos",
    "title": "Regressão Linear Múltipla",
    "section": "Análise Inicial dos Resíduos",
    "text": "Análise Inicial dos Resíduos\n\nplot(mod5)",
    "crumbs": [
      "Conteúdo Semanal",
      "Regressão Múltipla"
    ]
  },
  {
    "objectID": "semanas/Aula07.2.html#análise-do-modelo---parte-2",
    "href": "semanas/Aula07.2.html#análise-do-modelo---parte-2",
    "title": "Regressão Linear Múltipla",
    "section": "Análise do Modelo - Parte 2",
    "text": "Análise do Modelo - Parte 2\nO pacote easystats tem uma função que faz uma análise mais detalhada do modelo. Os gráficos são mais fáceis de interpretar do que os obtidos com o plot do R base.\n\nlibrary(easystats)\ncheck_model(mod5)",
    "crumbs": [
      "Conteúdo Semanal",
      "Regressão Múltipla"
    ]
  },
  {
    "objectID": "semanas/Aula07.2.html#análise-do-modelo---parte-3",
    "href": "semanas/Aula07.2.html#análise-do-modelo---parte-3",
    "title": "Regressão Linear Múltipla",
    "section": "Análise do Modelo - Parte 3",
    "text": "Análise do Modelo - Parte 3\nO pacote car apresenta funções mais avançadas para análise de resíduos.\nPara o gráfico de resíduos versus valores ajustados, podemos usar um teste chamado teste de Tukey de não aditividade (Tukey, 1949), ele é obtido adicionando os quadrados dos valores ajustados ao modelo e reajustando. O valor p para o teste de Tukey é obtido comparando a estatística de teste para a distribuição padrão-normal. O teste confirma a visível impressão de curvatura no gráfico residual, reforçando ainda mais a conclusão que o modelo não é adequado.\n\nlibrary(car)\nresidualPlots(mod5)\n\n\n\n\n\n\n\n#&gt;            Test stat Pr(&gt;|Test stat|)    \n#&gt; TV           -4.8383        4.102e-06 ***\n#&gt; Radio         1.9290           0.0562 .  \n#&gt; Tukey test    6.4114        1.442e-10 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ninfluencePlot(mod5)\n\n\n\n\n\n\n\n#&gt;      StudRes        Hat     CookD\n#&gt; 45 -2.512087 0.05650813 0.1204704\n#&gt; 83 -7.289206 0.05466694 0.7066190",
    "crumbs": [
      "Conteúdo Semanal",
      "Regressão Múltipla"
    ]
  },
  {
    "objectID": "semanas/Aula07.2.html#tentando-avaliar-transformações",
    "href": "semanas/Aula07.2.html#tentando-avaliar-transformações",
    "title": "Regressão Linear Múltipla",
    "section": "Tentando avaliar transformações",
    "text": "Tentando avaliar transformações\nAqui vamos avaliar a necessidade de transformar as variáveis para melhorar o modelo.\n\nsummary(p1 &lt;- powerTransform(Vendas ~ TV + Radio, data=conj_treino))\n\n#&gt; bcPower Transformation to Normality \n#&gt;    Est Power Rounded Pwr Wald Lwr Bnd Wald Upr Bnd\n#&gt; Y1    0.9937           1       0.8018       1.1856\n#&gt; \n#&gt; Likelihood ratio test that transformation parameter is equal to 0\n#&gt;  (log transformation)\n#&gt;                           LRT df       pval\n#&gt; LR test, lambda = (0) 111.042  1 &lt; 2.22e-16\n#&gt; \n#&gt; Likelihood ratio test that no transformation is needed\n#&gt;                               LRT df    pval\n#&gt; LR test, lambda = (1) 0.004149754  1 0.94864\n\nsummary(p2 &lt;- powerTransform(cbind(TV, Radio) ~1 , data=conj_treino))\n\n#&gt; bcPower Transformations to Multinormality \n#&gt;       Est Power Rounded Pwr Wald Lwr Bnd Wald Upr Bnd\n#&gt; TV       0.7692         1.0       0.5311       1.0073\n#&gt; Radio    0.5349         0.5       0.3429       0.7269\n#&gt; \n#&gt; Likelihood ratio test that transformation parameters are equal to 0\n#&gt;  (all log transformations)\n#&gt;                             LRT df       pval\n#&gt; LR test, lambda = (0 0) 102.457  2 &lt; 2.22e-16\n#&gt; \n#&gt; Likelihood ratio test that no transformations are needed\n#&gt;                            LRT df       pval\n#&gt; LR test, lambda = (1 1) 22.189  2 1.5196e-05\n\nboxCox(mod5, lambda = seq(-2, 2, 1/10))",
    "crumbs": [
      "Conteúdo Semanal",
      "Regressão Múltipla"
    ]
  },
  {
    "objectID": "semanas/Aula07.2.html#análise-do-modelo-com-car",
    "href": "semanas/Aula07.2.html#análise-do-modelo-com-car",
    "title": "Regressão Linear Múltipla",
    "section": "Análise do Modelo com car",
    "text": "Análise do Modelo com car\n\n# Fator de inflação da variância\nvif(mod5)\n\n#&gt;       TV    Radio \n#&gt; 1.014454 1.014454",
    "crumbs": [
      "Conteúdo Semanal",
      "Regressão Múltipla"
    ]
  },
  {
    "objectID": "semanas/Aula07.2.html#dado-que-parece-outlier-e-é-um-valor-influente",
    "href": "semanas/Aula07.2.html#dado-que-parece-outlier-e-é-um-valor-influente",
    "title": "Regressão Linear Múltipla",
    "section": "Dado que parece outlier e é um valor influente",
    "text": "Dado que parece outlier e é um valor influente\nA função OutlierTest () no pacote do car localiza o maior resíduo studentizado em valor absoluto e calcula o teste t com correção de Bonferroni. O testes de Outlier utiliza uma distribuição t para testar se o maior valor do residuo studentizado do modelo é estatisticamente diferente das outras observações. Um valor p significativo indica um outlier extremo que merece um exame mais aprofundado.\n\noutlierTest(mod5)\n\n#&gt;     rstudent unadjusted p-value Bonferroni p\n#&gt; 83 -7.289206         4.2486e-11   5.0558e-09\n\nconj_treino[83,]\n\n#&gt; # A tibble: 1 × 5\n#&gt;      TV Radio Jornal Vendas Previsoes\n#&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1   0.7  39.6    8.7    1.6      11.1",
    "crumbs": [
      "Conteúdo Semanal",
      "Regressão Múltipla"
    ]
  },
  {
    "objectID": "semanas/Aula07.2B.html",
    "href": "semanas/Aula07.2B.html",
    "title": "Problemas na Regressão Multipla",
    "section": "",
    "text": "library(tidyverse)\n\n#&gt; ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n#&gt; ✔ dplyr     1.1.4     ✔ readr     2.1.5\n#&gt; ✔ forcats   1.0.0     ✔ stringr   1.5.1\n#&gt; ✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n#&gt; ✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n#&gt; ✔ purrr     1.0.2     \n#&gt; ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n#&gt; ✖ purrr::%||%()   masks base::%||%()\n#&gt; ✖ dplyr::filter() masks stats::filter()\n#&gt; ✖ dplyr::lag()    masks stats::lag()\n#&gt; ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(readxl)\nlibrary(caret)\n\n#&gt; Carregando pacotes exigidos: lattice\n#&gt; \n#&gt; Anexando pacote: 'caret'\n#&gt; \n#&gt; O seguinte objeto é mascarado por 'package:purrr':\n#&gt; \n#&gt;     lift",
    "crumbs": [
      "Conteúdo Semanal",
      "Problemas em Regressão"
    ]
  },
  {
    "objectID": "semanas/Aula07.2B.html#carregando-bibliotecas",
    "href": "semanas/Aula07.2B.html#carregando-bibliotecas",
    "title": "Problemas na Regressão Multipla",
    "section": "",
    "text": "library(tidyverse)\n\n#&gt; ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n#&gt; ✔ dplyr     1.1.4     ✔ readr     2.1.5\n#&gt; ✔ forcats   1.0.0     ✔ stringr   1.5.1\n#&gt; ✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n#&gt; ✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n#&gt; ✔ purrr     1.0.2     \n#&gt; ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n#&gt; ✖ purrr::%||%()   masks base::%||%()\n#&gt; ✖ dplyr::filter() masks stats::filter()\n#&gt; ✖ dplyr::lag()    masks stats::lag()\n#&gt; ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(readxl)\nlibrary(caret)\n\n#&gt; Carregando pacotes exigidos: lattice\n#&gt; \n#&gt; Anexando pacote: 'caret'\n#&gt; \n#&gt; O seguinte objeto é mascarado por 'package:purrr':\n#&gt; \n#&gt;     lift",
    "crumbs": [
      "Conteúdo Semanal",
      "Problemas em Regressão"
    ]
  },
  {
    "objectID": "semanas/Aula07.2B.html#dados-de-pressão-sanguinea",
    "href": "semanas/Aula07.2B.html#dados-de-pressão-sanguinea",
    "title": "Problemas na Regressão Multipla",
    "section": "Dados de pressão sanguinea",
    "text": "Dados de pressão sanguinea\n\nBP = Pressão sanguínea (em mm Hg)\nAge = idade (em anos)\nWeight = peso (em kg)\nBSA = area superficial do corpo (em m2)\nDur = duração da hipertensão (em anos)\nPulse = batimentos (batidas por minuto)\nStress = índice de stress\n\n\npressao_sangue &lt;- read_delim(\"bloodpress.txt\", col_names = TRUE)\n\n#&gt; Rows: 20 Columns: 8\n#&gt; ── Column specification ────────────────────────────────────────────────────────\n#&gt; Delimiter: \"\\t\"\n#&gt; dbl (8): Pt, BP, Age, Weight, BSA, Dur, Pulse, Stress\n#&gt; \n#&gt; ℹ Use `spec()` to retrieve the full column specification for this data.\n#&gt; ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.",
    "crumbs": [
      "Conteúdo Semanal",
      "Problemas em Regressão"
    ]
  },
  {
    "objectID": "semanas/Aula07.2B.html#renomeando",
    "href": "semanas/Aula07.2B.html#renomeando",
    "title": "Problemas na Regressão Multipla",
    "section": "Renomeando",
    "text": "Renomeando\n\npressao_sangue &lt;- pressao_sangue %&gt;% rename(PS = BP, Idade = Age,\n                                            Peso = Weight, Acorp = BSA,\n                                            Pulso = Pulse)",
    "crumbs": [
      "Conteúdo Semanal",
      "Problemas em Regressão"
    ]
  },
  {
    "objectID": "semanas/Aula07.2B.html#sumario",
    "href": "semanas/Aula07.2B.html#sumario",
    "title": "Problemas na Regressão Multipla",
    "section": "Sumario",
    "text": "Sumario\n\nsummary(pressao_sangue)\n\n#&gt;        Pt              PS            Idade            Peso       \n#&gt;  Min.   : 1.00   Min.   :105.0   Min.   :45.00   Min.   : 85.40  \n#&gt;  1st Qu.: 5.75   1st Qu.:110.0   1st Qu.:47.00   1st Qu.: 90.22  \n#&gt;  Median :10.50   Median :114.0   Median :48.50   Median : 94.15  \n#&gt;  Mean   :10.50   Mean   :114.0   Mean   :48.60   Mean   : 93.09  \n#&gt;  3rd Qu.:15.25   3rd Qu.:116.2   3rd Qu.:49.25   3rd Qu.: 94.85  \n#&gt;  Max.   :20.00   Max.   :125.0   Max.   :56.00   Max.   :101.30  \n#&gt;      Acorp            Dur            Pulso           Stress     \n#&gt;  Min.   :1.750   Min.   : 2.50   Min.   :62.00   Min.   : 8.00  \n#&gt;  1st Qu.:1.897   1st Qu.: 5.25   1st Qu.:67.75   1st Qu.:17.00  \n#&gt;  Median :1.980   Median : 6.00   Median :70.00   Median :44.50  \n#&gt;  Mean   :1.998   Mean   : 6.43   Mean   :69.60   Mean   :53.35  \n#&gt;  3rd Qu.:2.075   3rd Qu.: 7.60   3rd Qu.:72.00   3rd Qu.:95.00  \n#&gt;  Max.   :2.250   Max.   :10.20   Max.   :76.00   Max.   :99.00\n\n\n\nlibrary(corrplot)\n\n#&gt; corrplot 0.92 loaded\n\nmat_corr &lt;- cor(cor(pressao_sangue[,-1]))\ncorrplot(mat_corr)\n\n\n\n\n\n\ncor(pressao_sangue[,-1])\n\n#&gt;               PS     Idade       Peso      Acorp       Dur     Pulso     Stress\n#&gt; PS     1.0000000 0.6590930 0.95006765 0.86587887 0.2928336 0.7214132 0.16390139\n#&gt; Idade  0.6590930 1.0000000 0.40734926 0.37845460 0.3437921 0.6187643 0.36822369\n#&gt; Peso   0.9500677 0.4073493 1.00000000 0.87530481 0.2006496 0.6593399 0.03435475\n#&gt; Acorp  0.8658789 0.3784546 0.87530481 1.00000000 0.1305400 0.4648188 0.01844634\n#&gt; Dur    0.2928336 0.3437921 0.20064959 0.13054001 1.0000000 0.4015144 0.31163982\n#&gt; Pulso  0.7214132 0.6187643 0.65933987 0.46481881 0.4015144 1.0000000 0.50631008\n#&gt; Stress 0.1639014 0.3682237 0.03435475 0.01844634 0.3116398 0.5063101 1.00000000\n\n\nAqui vemos que a presssão sanguinea tem uma correlação forte com o peso e também com a área corporal. O peso e a area corporal tem uma correlação forte. Esta correlação alta pode indicar a existencia de multicolinearidade.\n\nlibrary(psych)\npairs.panels(pressao_sangue[,-1])\n\n\n\n\n\n\n\n\ndados &lt;- pressao_sangue[,-1]\nmod1 &lt;- lm(PS ~ ., data=dados)\nsummary(mod1)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = PS ~ ., data = dados)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -0.93213 -0.11314  0.03064  0.21834  0.48454 \n#&gt; \n#&gt; Coefficients:\n#&gt;               Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept) -12.870476   2.556650  -5.034 0.000229 ***\n#&gt; Idade         0.703259   0.049606  14.177 2.76e-09 ***\n#&gt; Peso          0.969920   0.063108  15.369 1.02e-09 ***\n#&gt; Acorp         3.776491   1.580151   2.390 0.032694 *  \n#&gt; Dur           0.068383   0.048441   1.412 0.181534    \n#&gt; Pulso        -0.084485   0.051609  -1.637 0.125594    \n#&gt; Stress        0.005572   0.003412   1.633 0.126491    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 0.4072 on 13 degrees of freedom\n#&gt; Multiple R-squared:  0.9962, Adjusted R-squared:  0.9944 \n#&gt; F-statistic: 560.6 on 6 and 13 DF,  p-value: 6.395e-15\n\nlibrary(car)\nvif(mod1)\n\n#&gt;    Idade     Peso    Acorp      Dur    Pulso   Stress \n#&gt; 1.762807 8.417035 5.328751 1.237309 4.413575 1.834845\n\n\n\nmod2 &lt;- update(mod1,. ~ . -Acorp) \nsummary(mod2)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = PS ~ Idade + Peso + Dur + Pulso + Stress, data = dados)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -1.02600 -0.18526 -0.00077  0.21934  0.72533 \n#&gt; \n#&gt; Coefficients:\n#&gt;               Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept) -15.116781   2.748758  -5.499 7.83e-05 ***\n#&gt; Idade         0.731940   0.055646  13.154 2.85e-09 ***\n#&gt; Peso          1.098958   0.037773  29.093 6.37e-14 ***\n#&gt; Dur           0.064105   0.055965   1.145   0.2712    \n#&gt; Pulso        -0.137444   0.053885  -2.551   0.0231 *  \n#&gt; Stress        0.007429   0.003841   1.934   0.0736 .  \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 0.4708 on 14 degrees of freedom\n#&gt; Multiple R-squared:  0.9945, Adjusted R-squared:  0.9925 \n#&gt; F-statistic: 502.5 on 5 and 14 DF,  p-value: 2.835e-15\n\nvif(mod2)\n\n#&gt;    Idade     Peso      Dur    Pulso   Stress \n#&gt; 1.659637 2.256150 1.235620 3.599913 1.739641\n\n\n\nmod3 &lt;- lm(PS ~ Idade + Peso + Pulso + Stress, data = dados)\nsummary(mod3)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = PS ~ Idade + Peso + Pulso + Stress, data = dados)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -0.89479 -0.15857 -0.04157  0.25593  0.92857 \n#&gt; \n#&gt; Coefficients:\n#&gt;               Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept) -15.683749   2.731808  -5.741 3.90e-05 ***\n#&gt; Idade         0.739878   0.055784  13.263 1.09e-09 ***\n#&gt; Peso          1.097262   0.038135  28.773 1.54e-14 ***\n#&gt; Pulso        -0.126973   0.053654  -2.367   0.0318 *  \n#&gt; Stress        0.007851   0.003863   2.032   0.0602 .  \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 0.4757 on 15 degrees of freedom\n#&gt; Multiple R-squared:  0.9939, Adjusted R-squared:  0.9923 \n#&gt; F-statistic:   615 on 4 and 15 DF,  p-value: &lt; 2.2e-16\n\nresidualPlots(mod3)\n\n\n\n\n\n\n\n#&gt;            Test stat Pr(&gt;|Test stat|)\n#&gt; Idade         0.3403           0.7387\n#&gt; Peso          0.3825           0.7078\n#&gt; Pulso         0.0914           0.9285\n#&gt; Stress       -0.5581           0.5856\n#&gt; Tukey test    0.2340           0.8150",
    "crumbs": [
      "Conteúdo Semanal",
      "Problemas em Regressão"
    ]
  },
  {
    "objectID": "semanas/Aula07.2B.html#teste-dos-resíduos",
    "href": "semanas/Aula07.2B.html#teste-dos-resíduos",
    "title": "Problemas na Regressão Multipla",
    "section": "Teste dos resíduos",
    "text": "Teste dos resíduos\nTeste de normalidade Teste de heterocedasticidade (Bresch-Pagan) Teste de autocorrelação (Durbin-Watson)\n\nlibrary(lmtest)\nmod3_sum &lt;- summary(mod3)\n# Teste de normalidade\nshapiro.test(mod3_sum$residuals)\n\n#&gt; \n#&gt;  Shapiro-Wilk normality test\n#&gt; \n#&gt; data:  mod3_sum$residuals\n#&gt; W = 0.95962, p-value = 0.5364\n\n# Teste de hetrocedasticidade\nbptest(mod3)\n\n#&gt; \n#&gt;  studentized Breusch-Pagan test\n#&gt; \n#&gt; data:  mod3\n#&gt; BP = 1.9471, df = 4, p-value = 0.7455\n\n# Teste de autocorrelação\ndwtest(mod3)\n\n#&gt; \n#&gt;  Durbin-Watson test\n#&gt; \n#&gt; data:  mod3\n#&gt; DW = 1.6389, p-value = 0.2115\n#&gt; alternative hypothesis: true autocorrelation is greater than 0",
    "crumbs": [
      "Conteúdo Semanal",
      "Problemas em Regressão"
    ]
  },
  {
    "objectID": "semanas/Aula07.4.html#carregando-bibliotecas",
    "href": "semanas/Aula07.4.html#carregando-bibliotecas",
    "title": "Regularização de Modelos",
    "section": "Carregando Bibliotecas",
    "text": "Carregando Bibliotecas\n\nlibrary(tidyverse)\nlibrary(glmnet)",
    "crumbs": [
      "Conteúdo Semanal",
      "Métodos de Encolhimento"
    ]
  },
  {
    "objectID": "semanas/Aula07.4.html#carregando-os-dados",
    "href": "semanas/Aula07.4.html#carregando-os-dados",
    "title": "Regularização de Modelos",
    "section": "Carregando os dados",
    "text": "Carregando os dados\nVendas de casas em Seattle entre 2015 e 2016\n\nvendas_casa &lt;- readRDS(\"home_sales.rds\")\nhead(vendas_casa)\n\n#&gt; # A tibble: 6 × 8\n#&gt;   selling_price home_age bedrooms bathrooms sqft_living sqft_lot sqft_basement\n#&gt;           &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;         &lt;dbl&gt;\n#&gt; 1        487000       10        4      2.5         2540     5001             0\n#&gt; 2        465000       10        3      2.25        1530     1245           480\n#&gt; 3        411000       18        2      2           1130     1148           330\n#&gt; 4        635000        4        3      2.5         3350     4007           800\n#&gt; 5        380000       24        5      2.5         2130     8428             0\n#&gt; 6        495000       21        3      3.5         1650     1577           550\n#&gt; # ℹ 1 more variable: floors &lt;dbl&gt;\n\nvendas_casa &lt;- vendas_casa %&gt;% rename(preco=selling_price,\n                                      idade=home_age,\n                                      quartos=bedrooms,\n                                      banheiros= bathrooms,\n                                      m2_princ=sqft_living,\n                                      m2_tot=sqft_lot,\n                                      m2_porao=sqft_basement,\n                                      andares=floors\n                                      )\nsummary(vendas_casa)\n\n#&gt;      preco            idade          quartos        banheiros    \n#&gt;  Min.   :350000   Min.   : 0.00   Min.   :1.000   Min.   :0.750  \n#&gt;  1st Qu.:410000   1st Qu.:12.00   1st Qu.:3.000   1st Qu.:2.438  \n#&gt;  Median :470000   Median :19.00   Median :3.000   Median :2.500  \n#&gt;  Mean   :479085   Mean   :18.53   Mean   :3.338   Mean   :2.473  \n#&gt;  3rd Qu.:541625   3rd Qu.:25.00   3rd Qu.:4.000   3rd Qu.:2.500  \n#&gt;  Max.   :650000   Max.   :49.00   Max.   :8.000   Max.   :5.000  \n#&gt;     m2_princ        m2_tot          m2_porao         andares     \n#&gt;  Min.   : 550   Min.   :   600   Min.   :   0.0   Min.   :1.000  \n#&gt;  1st Qu.:1640   1st Qu.:  3200   1st Qu.:   0.0   1st Qu.:2.000  \n#&gt;  Median :2060   Median :  5508   Median :   0.0   Median :2.000  \n#&gt;  Mean   :2087   Mean   : 11891   Mean   : 129.7   Mean   :1.865  \n#&gt;  3rd Qu.:2500   3rd Qu.:  8644   3rd Qu.: 122.5   3rd Qu.:2.000  \n#&gt;  Max.   :3880   Max.   :415126   Max.   :1660.0   Max.   :2.000\n\nvendas_casa &lt;- vendas_casa %&gt;% mutate(preco_m=preco/1000) %&gt;% select(-preco)\nsummary(vendas_casa)\n\n#&gt;      idade          quartos        banheiros        m2_princ   \n#&gt;  Min.   : 0.00   Min.   :1.000   Min.   :0.750   Min.   : 550  \n#&gt;  1st Qu.:12.00   1st Qu.:3.000   1st Qu.:2.438   1st Qu.:1640  \n#&gt;  Median :19.00   Median :3.000   Median :2.500   Median :2060  \n#&gt;  Mean   :18.53   Mean   :3.338   Mean   :2.473   Mean   :2087  \n#&gt;  3rd Qu.:25.00   3rd Qu.:4.000   3rd Qu.:2.500   3rd Qu.:2500  \n#&gt;  Max.   :49.00   Max.   :8.000   Max.   :5.000   Max.   :3880  \n#&gt;      m2_tot          m2_porao         andares         preco_m     \n#&gt;  Min.   :   600   Min.   :   0.0   Min.   :1.000   Min.   :350.0  \n#&gt;  1st Qu.:  3200   1st Qu.:   0.0   1st Qu.:2.000   1st Qu.:410.0  \n#&gt;  Median :  5508   Median :   0.0   Median :2.000   Median :470.0  \n#&gt;  Mean   : 11891   Mean   : 129.7   Mean   :1.865   Mean   :479.1  \n#&gt;  3rd Qu.:  8644   3rd Qu.: 122.5   3rd Qu.:2.000   3rd Qu.:541.6  \n#&gt;  Max.   :415126   Max.   :1660.0   Max.   :2.000   Max.   :650.0",
    "crumbs": [
      "Conteúdo Semanal",
      "Métodos de Encolhimento"
    ]
  },
  {
    "objectID": "semanas/Aula07.4.html#conjunto-de-treino-e-de-teste",
    "href": "semanas/Aula07.4.html#conjunto-de-treino-e-de-teste",
    "title": "Regularização de Modelos",
    "section": "Conjunto de treino e de teste",
    "text": "Conjunto de treino e de teste\n\nlibrary(caret)\n\n#&gt; Carregando pacotes exigidos: lattice\n\n\n#&gt; \n#&gt; Anexando pacote: 'caret'\n\n\n#&gt; O seguinte objeto é mascarado por 'package:purrr':\n#&gt; \n#&gt;     lift\n\nset.seed(21)\nnrow(vendas_casa)\n\n#&gt; [1] 1492\n\ny &lt;- vendas_casa$preco_m\nindice_teste &lt;- createDataPartition(y, times = 1, p = 0.2, list = FALSE)\n\nconj_treino &lt;- vendas_casa[-indice_teste,]\nconj_teste &lt;- vendas_casa[indice_teste,]\n\nstr(conj_treino)\n\n#&gt; tibble [1,192 × 8] (S3: tbl_df/tbl/data.frame)\n#&gt;  $ idade    : num [1:1192] 10 18 4 21 19 24 3 16 20 29 ...\n#&gt;  $ quartos  : num [1:1192] 3 2 3 3 3 2 4 3 3 3 ...\n#&gt;  $ banheiros: num [1:1192] 2.25 2 2.5 3.5 2.25 1 2.5 2.75 2.75 2.5 ...\n#&gt;  $ m2_princ : num [1:1192] 1530 1130 3350 1650 1430 1430 2140 2100 2930 1960 ...\n#&gt;  $ m2_tot   : num [1:1192] 1245 1148 4007 1577 4777 ...\n#&gt;  $ m2_porao : num [1:1192] 480 330 800 550 0 420 0 590 1070 0 ...\n#&gt;  $ andares  : num [1:1192] 2 2 2 2 2 1 2 2 1 2 ...\n#&gt;  $ preco_m  : num [1:1192] 465 411 635 495 355 ...\n\nstr(conj_teste)\n\n#&gt; tibble [300 × 8] (S3: tbl_df/tbl/data.frame)\n#&gt;  $ idade    : num [1:300] 10 24 19 11 3 28 9 17 9 19 ...\n#&gt;  $ quartos  : num [1:300] 4 5 3 4 4 5 3 3 3 3 ...\n#&gt;  $ banheiros: num [1:300] 2.5 2.5 2 2.5 2.75 3 2.5 3.5 3.5 2.25 ...\n#&gt;  $ m2_princ : num [1:300] 2540 2130 2190 1920 2360 ...\n#&gt;  $ m2_tot   : num [1:300] 5001 8428 19800 9000 15100 ...\n#&gt;  $ m2_porao : num [1:300] 0 0 0 0 0 0 0 770 770 145 ...\n#&gt;  $ andares  : num [1:300] 2 2 1 2 1 2 2 2 2 2 ...\n#&gt;  $ preco_m  : num [1:300] 487 380 465 425 535 ...\n\ngt::gt(head(conj_treino, 10))\n\n\n\n\n\nidade\nquartos\nbanheiros\nm2_princ\nm2_tot\nm2_porao\nandares\npreco_m\n\n\n\n10\n3\n2.25\n1530\n1245\n480\n2\n465.000\n\n\n18\n2\n2.00\n1130\n1148\n330\n2\n411.000\n\n\n4\n3\n2.50\n3350\n4007\n800\n2\n635.000\n\n\n21\n3\n3.50\n1650\n1577\n550\n2\n495.000\n\n\n19\n3\n2.25\n1430\n4777\n0\n2\n355.000\n\n\n24\n2\n1.00\n1430\n365904\n420\n1\n356.000\n\n\n3\n4\n2.50\n2140\n7245\n0\n2\n495.000\n\n\n16\n3\n2.75\n2100\n10362\n590\n2\n525.000\n\n\n20\n3\n2.75\n2930\n5569\n1070\n1\n559.900\n\n\n29\n3\n2.50\n1960\n8469\n0\n2\n552.321",
    "crumbs": [
      "Conteúdo Semanal",
      "Métodos de Encolhimento"
    ]
  },
  {
    "objectID": "semanas/Aula07.4.html#métodos-de-regularização",
    "href": "semanas/Aula07.4.html#métodos-de-regularização",
    "title": "Regularização de Modelos",
    "section": "Métodos de Regularização",
    "text": "Métodos de Regularização\nO pacote glmnet não usa a linguagem de formula, em particular nós devemos passar \\(x\\) como uma matriz e \\(y\\) como um vetor, pois não se usa a sintaxe \\(y \\sim x\\). Com isso será necessário ajustar x e y. A função model.matrix() é particularmente útil para criar x; não só produz uma matriz correspondente as variáveis explicativas, mas também transforma automaticamente quaisquer variáveis qualitativas em variáveis dummy. Esta última propriedade é importante porque o glmnet() só pode tomar insumos numéricos e quantitativos.\nO pacote glmnet também por default padroniza as variáveis, o que é importante para a regressão Ridge e também para o LASSO. Ele posteriormente retorna os coeficientes para a escala inicial.\n\nx_treino &lt;- model.matrix(preco_m ~ . , data = conj_treino)[, -1]\ny_treino &lt;- conj_treino$preco_m\n\nx_teste &lt;- model.matrix(preco_m ~ . , data = conj_teste)[, -1]\ny_teste = conj_teste$preco_m",
    "crumbs": [
      "Conteúdo Semanal",
      "Métodos de Encolhimento"
    ]
  },
  {
    "objectID": "semanas/Aula07.4.html#regressão-ridge",
    "href": "semanas/Aula07.4.html#regressão-ridge",
    "title": "Regularização de Modelos",
    "section": "Regressão Ridge",
    "text": "Regressão Ridge\nPrimeiro vamos ajustar um modelo de regressão Ridge. Isso é conseguido chamando glmnet() com alpha=0, se alpha=1 então glmnet() ajusta um lasso.(veja o arquivo de ajuda).\n\n## Estabelecendo um grid de valores para lambda\ngrid &lt;- 10^seq(-2, 10, length = 100)\najusreg.ridge &lt;- glmnet(x_treino, y_treino, alpha=0, lambda = grid)\n\nPor padrão, a função glmnet() executa a regressão ridge automaticamente selecionando a faixa de valores de \\(\\lambda\\). No entanto, aqui nós escolhemos implementar usando uma grade de valores que variam de \\(\\lambda = 10^{-2}\\) a \\(\\lambda = 10^{10}\\), cobrindo toda a gama de cenários do modelo nulo contendo apenas o coeficiente linear até o ajuste dos mínimos quadrados.\nTambém podemos calcular o modelo para um valor particular de \\(\\lambda\\) que não é um dos valores de grade. Observe que, por padrão, a função glmnet() padroniza as variáveis para que elas estejam na mesma escala. Esta padronização é muito importante no caso da regressão Ridge, pois ela é afetada pela mudança de escala das variáveis explicativas.\nAssociado a cada valor de \\(\\lambda\\) existe um vetor de coeficientes de regressão de ridge, que é armazenado em uma matriz que pode ser acessada por ‘coef()’. Neste caso, é uma matriz \\(13 \\times 100\\), com 13 linhas (uma para cada preditor, mais uma para o coeficiente linear) e 100 colunas (uma para cada valor de \\(\\lambda\\)).\n\ndim(coef(ajusreg.ridge))\n\n#&gt; [1]   8 100\n\nplot(ajusreg.ridge, xvar=\"lambda\", label=TRUE) # Representando os coeficientes\n\n\n\n\n\n\n\nQuando \\(\\lambda\\) é grande o esperado é que os coeficentes sejam pequenos e quando \\(\\lambda\\) é pequeno os coeficientes assumem valores maiores.\n\najusreg.ridge$lambda[1] # Mostra primeiro valor de lambda\n\n#&gt; [1] 1e+10\n\ncoef(ajusreg.ridge)[,1] # Mostra os coeficientes associados com o primeiro valor\n\n#&gt;   (Intercept)         idade       quartos     banheiros      m2_princ \n#&gt;  4.793155e+02 -2.734498e-08  2.861996e-07  5.797974e-07  9.012053e-10 \n#&gt;        m2_tot      m2_porao       andares \n#&gt;  3.183259e-12 -3.419035e-11  3.936601e-07\n\najusreg.ridge$lambda[100] # Mostra centésimo valor de lambda\n\n#&gt; [1] 0.01\n\ncoef(ajusreg.ridge)[,100] # Mostra os coeficientes associados com o centésimo valor\n\n#&gt;   (Intercept)         idade       quartos     banheiros      m2_princ \n#&gt;  2.558886e+02 -9.299049e-01 -4.458647e+01  1.038811e+01  1.439939e-01 \n#&gt;        m2_tot      m2_porao       andares \n#&gt;  9.087628e-05 -3.568248e-03  3.371000e+01\n\n\n\nlibrary(plotmo)\n\n#&gt; Carregando pacotes exigidos: Formula\n\n\n#&gt; Carregando pacotes exigidos: plotrix\n\nplot_glmnet(ajusreg.ridge)",
    "crumbs": [
      "Conteúdo Semanal",
      "Métodos de Encolhimento"
    ]
  },
  {
    "objectID": "semanas/Aula07.4.html#cross-validation-no-ridge",
    "href": "semanas/Aula07.4.html#cross-validation-no-ridge",
    "title": "Regularização de Modelos",
    "section": "Cross-Validation no Ridge",
    "text": "Cross-Validation no Ridge\nNós podemos usar o k-fold cross validation para identificar o melhor valor de \\(\\lambda\\)\nA biblioteca glmnet já tem internamente uma função para uso do crosss validation. O default são 10 envelopes de dados nfold=10.\n\nset.seed(21)\nridge_cv &lt;- cv.glmnet(x_treino,y_treino, alpha=0) ## por padrão k=10\nplot(ridge_cv)\n\n\n\n\n\n\nm_lamb &lt;- ridge_cv$lambda.min  # Seleciona o lambda que minimiza o MSE (EQM) de treino\nm_lamb\n\n#&gt; [1] 6.425292\n\nlog(m_lamb)\n\n#&gt; [1] 1.860242\n\ncoef(ridge_cv, s=m_lamb)\n\n#&gt; 8 x 1 sparse Matrix of class \"dgCMatrix\"\n#&gt;                        s1\n#&gt; (Intercept)  2.642227e+02\n#&gt; idade       -1.136747e+00\n#&gt; quartos     -2.993272e+01\n#&gt; banheiros    1.272875e+01\n#&gt; m2_princ     1.185102e-01\n#&gt; m2_tot       1.373143e-04\n#&gt; m2_porao    -5.244311e-03\n#&gt; andares      3.030654e+01",
    "crumbs": [
      "Conteúdo Semanal",
      "Métodos de Encolhimento"
    ]
  },
  {
    "objectID": "semanas/Aula07.4.html#avaliando-com-conjunto-de-teste",
    "href": "semanas/Aula07.4.html#avaliando-com-conjunto-de-teste",
    "title": "Regularização de Modelos",
    "section": "Avaliando com conjunto de teste",
    "text": "Avaliando com conjunto de teste\nEm seguida avaliamos seu MSE no conjunto de teste, usando \\(\\lambda\\) = m_lamb. Observe o uso da função ‘predict()’: desta vez temos previsões para um conjunto de teste, com o argumento newx.\n\najusreg.ridge2 &lt;- glmnet(x_treino, y_treino, alpha=0, lambda = m_lamb)\ny_prev &lt;- predict(ajusreg.ridge2, s = m_lamb, newx = x_teste)\n# Metricas de desempenho\nsqrt(mean((y_prev - y_teste)^2))\n\n#&gt; [1] 42.19339",
    "crumbs": [
      "Conteúdo Semanal",
      "Métodos de Encolhimento"
    ]
  },
  {
    "objectID": "semanas/Aula07.4.html#comparando-real-vs-previsão-no-conjunto-de-teste",
    "href": "semanas/Aula07.4.html#comparando-real-vs-previsão-no-conjunto-de-teste",
    "title": "Regularização de Modelos",
    "section": "Comparando real vs previsão no conjunto de teste",
    "text": "Comparando real vs previsão no conjunto de teste\n\nv_teste &lt;- data.frame(y_teste, y_prev)\nggplot(v_teste, aes(x=y_prev, y=y_teste)) +geom_point() +\n  geom_abline(color = \"darkblue\") +\n  ggtitle(\"Preço da Casa vs. Previsões do modelo Ridge\")",
    "crumbs": [
      "Conteúdo Semanal",
      "Métodos de Encolhimento"
    ]
  },
  {
    "objectID": "semanas/Aula07.4.html#sintetizando-o-ajuste",
    "href": "semanas/Aula07.4.html#sintetizando-o-ajuste",
    "title": "Regularização de Modelos",
    "section": "Sintetizando o ajuste",
    "text": "Sintetizando o ajuste\n\nlibrary(broom)\nglance(ridge_cv)\n\n#&gt; # A tibble: 1 × 3\n#&gt;   lambda.min lambda.1se  nobs\n#&gt;        &lt;dbl&gt;      &lt;dbl&gt; &lt;int&gt;\n#&gt; 1       6.43       8.49  1192",
    "crumbs": [
      "Conteúdo Semanal",
      "Métodos de Encolhimento"
    ]
  },
  {
    "objectID": "semanas/Aula07.4.html#lasso",
    "href": "semanas/Aula07.4.html#lasso",
    "title": "Regularização de Modelos",
    "section": "LASSO",
    "text": "LASSO\nPrimeiro ajustamos com todos os dados como no caso do Ridge\n\najusreg.lasso &lt;- glmnet(x_treino,y_treino, alpha = 1)\nplot(ajusreg.lasso, xvar=\"lambda\", label=TRUE) # Representando os coeficientes\n\n\n\n\n\n\nplot_glmnet(ajusreg.lasso)",
    "crumbs": [
      "Conteúdo Semanal",
      "Métodos de Encolhimento"
    ]
  },
  {
    "objectID": "semanas/Aula07.4.html#validação-cruzada-no-lasso",
    "href": "semanas/Aula07.4.html#validação-cruzada-no-lasso",
    "title": "Regularização de Modelos",
    "section": "Validação Cruzada no LASSO",
    "text": "Validação Cruzada no LASSO\n\nlasso_cv &lt;- cv.glmnet(x_treino,y_treino, alpha = 1)\nplot(lasso_cv)\n\n\n\n\n\n\nm_lamb1 &lt;- lasso_cv$lambda.min  # Seleciona o lambda que minimiza o MSE de treino\nm_lamb1\n\n#&gt; [1] 0.1384287\n\nlog(m_lamb1)\n\n#&gt; [1] -1.9774\n\ncoef(lasso_cv, s=m_lamb1)\n\n#&gt; 8 x 1 sparse Matrix of class \"dgCMatrix\"\n#&gt;                        s1\n#&gt; (Intercept)  2.567612e+02\n#&gt; idade       -9.248281e-01\n#&gt; quartos     -4.393777e+01\n#&gt; banheiros    9.756224e+00\n#&gt; m2_princ     1.434165e-01\n#&gt; m2_tot       8.570819e-05\n#&gt; m2_porao    -2.919595e-03\n#&gt; andares      3.350271e+01",
    "crumbs": [
      "Conteúdo Semanal",
      "Métodos de Encolhimento"
    ]
  },
  {
    "objectID": "semanas/Aula07.4.html#avaliando-com-conjunto-de-teste-1",
    "href": "semanas/Aula07.4.html#avaliando-com-conjunto-de-teste-1",
    "title": "Regularização de Modelos",
    "section": "Avaliando com conjunto de teste",
    "text": "Avaliando com conjunto de teste\n\najusreg.lasso2 &lt;- glmnet(x_treino, y_treino, alpha=1, lambda = m_lamb1)\ny_prev &lt;- predict(ajusreg.lasso2, s = m_lamb1, newx = x_teste)\n# Metricas de desempenho\nsqrt(mean((y_prev - y_teste)^2))\n\n#&gt; [1] 41.48019",
    "crumbs": [
      "Conteúdo Semanal",
      "Métodos de Encolhimento"
    ]
  },
  {
    "objectID": "semanas/Aula07.4.html#comparando-real-vs-previsão-no-conjunto-de-teste-1",
    "href": "semanas/Aula07.4.html#comparando-real-vs-previsão-no-conjunto-de-teste-1",
    "title": "Regularização de Modelos",
    "section": "Comparando real vs previsão no conjunto de teste",
    "text": "Comparando real vs previsão no conjunto de teste\n\nv_teste &lt;- data.frame(y_teste, y_prev)\nggplot(v_teste, aes(x=y_prev, y=y_teste)) +geom_point() +\n  geom_abline(color = \"darkblue\") +\n  ggtitle(\"Preço da Casa vs. Previsões do modelo Ridge\")",
    "crumbs": [
      "Conteúdo Semanal",
      "Métodos de Encolhimento"
    ]
  },
  {
    "objectID": "semanas/Aula08.html",
    "href": "semanas/Aula08.html",
    "title": "KNN",
    "section": "",
    "text": "O KNN é um algoritmo muito simples no qual cada observação é prevista com base em sua “semelhança” com outras observações. Ao contrário da maioria dos métodos, KNN é um algoritmo baseado na memória e não pode ser resumido por um modelo de forma fechada. Isso significa que as amostras de treinamento são necessárias no tempo de execução e as previsões são feitas diretamente das relações amostrais. Consequentemente, os KNNs também são conhecidos como aprendizes preguiçosos",
    "crumbs": [
      "Conteúdo Semanal",
      "KNN"
    ]
  },
  {
    "objectID": "semanas/Aula08.html#knn",
    "href": "semanas/Aula08.html#knn",
    "title": "KNN",
    "section": "",
    "text": "O KNN é um algoritmo muito simples no qual cada observação é prevista com base em sua “semelhança” com outras observações. Ao contrário da maioria dos métodos, KNN é um algoritmo baseado na memória e não pode ser resumido por um modelo de forma fechada. Isso significa que as amostras de treinamento são necessárias no tempo de execução e as previsões são feitas diretamente das relações amostrais. Consequentemente, os KNNs também são conhecidos como aprendizes preguiçosos",
    "crumbs": [
      "Conteúdo Semanal",
      "KNN"
    ]
  },
  {
    "objectID": "semanas/Aula08.html#carregando-bibliotecas",
    "href": "semanas/Aula08.html#carregando-bibliotecas",
    "title": "KNN",
    "section": "Carregando Bibliotecas",
    "text": "Carregando Bibliotecas\n\nlibrary(tidyverse)\nlibrary(ISLR)\ndata(Default)\nsummary(Default)\n\n#&gt;  default    student       balance           income     \n#&gt;  No :9667   No :7056   Min.   :   0.0   Min.   :  772  \n#&gt;  Yes: 333   Yes:2944   1st Qu.: 481.7   1st Qu.:21340  \n#&gt;                        Median : 823.6   Median :34553  \n#&gt;                        Mean   : 835.4   Mean   :33517  \n#&gt;                        3rd Qu.:1166.3   3rd Qu.:43808  \n#&gt;                        Max.   :2654.3   Max.   :73554\n\nstr(Default)\n\n#&gt; 'data.frame':    10000 obs. of  4 variables:\n#&gt;  $ default: Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n#&gt;  $ student: Factor w/ 2 levels \"No\",\"Yes\": 1 2 1 1 1 2 1 2 1 1 ...\n#&gt;  $ balance: num  730 817 1074 529 786 ...\n#&gt;  $ income : num  44362 12106 31767 35704 38463 ...\n\nhead(Default)\n\n#&gt;   default student   balance    income\n#&gt; 1      No      No  729.5265 44361.625\n#&gt; 2      No     Yes  817.1804 12106.135\n#&gt; 3      No      No 1073.5492 31767.139\n#&gt; 4      No      No  529.2506 35704.494\n#&gt; 5      No      No  785.6559 38463.496\n#&gt; 6      No     Yes  919.5885  7491.559",
    "crumbs": [
      "Conteúdo Semanal",
      "KNN"
    ]
  },
  {
    "objectID": "semanas/Aula08.html#manipulando-os-dados",
    "href": "semanas/Aula08.html#manipulando-os-dados",
    "title": "KNN",
    "section": "Manipulando os dados",
    "text": "Manipulando os dados\n\ncredito &lt;- tibble(Default)\nsummary(credito)\n\n#&gt;  default    student       balance           income     \n#&gt;  No :9667   No :7056   Min.   :   0.0   Min.   :  772  \n#&gt;  Yes: 333   Yes:2944   1st Qu.: 481.7   1st Qu.:21340  \n#&gt;                        Median : 823.6   Median :34553  \n#&gt;                        Mean   : 835.4   Mean   :33517  \n#&gt;                        3rd Qu.:1166.3   3rd Qu.:43808  \n#&gt;                        Max.   :2654.3   Max.   :73554\n\n# renomeando colunas\ncredito &lt;- credito %&gt;% \n                rename( inadimplente = default, estudante = student, balanco = balance,\n                receita = income)\ncredito &lt;- credito %&gt;% mutate( inadimplente =  case_when(\n                           inadimplente == \"No\"  ~ \"Nao\",\n                           inadimplente == \"Yes\" ~ \"Sim\"\n                          )) %&gt;% mutate(inadimplente = factor(inadimplente))\ncredito &lt;- credito %&gt;% mutate( estudante =  case_when(\n                           estudante == \"No\"  ~ 0,\n                           estudante == \"Yes\" ~ 1\n                          )) \n\nstr(credito)\n\n#&gt; tibble [10,000 × 4] (S3: tbl_df/tbl/data.frame)\n#&gt;  $ inadimplente: Factor w/ 2 levels \"Nao\",\"Sim\": 1 1 1 1 1 1 1 1 1 1 ...\n#&gt;  $ estudante   : num [1:10000] 0 1 0 0 0 1 0 1 0 0 ...\n#&gt;  $ balanco     : num [1:10000] 730 817 1074 529 786 ...\n#&gt;  $ receita     : num [1:10000] 44362 12106 31767 35704 38463 ...\n\nsummary(credito)\n\n#&gt;  inadimplente   estudante         balanco          receita     \n#&gt;  Nao:9667     Min.   :0.0000   Min.   :   0.0   Min.   :  772  \n#&gt;  Sim: 333     1st Qu.:0.0000   1st Qu.: 481.7   1st Qu.:21340  \n#&gt;               Median :0.0000   Median : 823.6   Median :34553  \n#&gt;               Mean   :0.2944   Mean   : 835.4   Mean   :33517  \n#&gt;               3rd Qu.:1.0000   3rd Qu.:1166.3   3rd Qu.:43808  \n#&gt;               Max.   :1.0000   Max.   :2654.3   Max.   :73554",
    "crumbs": [
      "Conteúdo Semanal",
      "KNN"
    ]
  },
  {
    "objectID": "semanas/Aula08.html#matriz-de-dispersão",
    "href": "semanas/Aula08.html#matriz-de-dispersão",
    "title": "KNN",
    "section": "Matriz de dispersão",
    "text": "Matriz de dispersão\nVamos agora explorar os dados originais para termos algum visão do comportamento das variáveis explicativas e a variável dependente.\n\nlibrary(psych)\npairs.panels(credito, \n             method = \"pearson\", # metodo de correlação\n             hist.col = \"#00AFBB\",\n             density = TRUE,  # mostra graficos de densidade\n             ellipses = FALSE # mostra elipses de correlação\n             )",
    "crumbs": [
      "Conteúdo Semanal",
      "KNN"
    ]
  },
  {
    "objectID": "semanas/Aula08.html#avaliando-o-comportamento-das-variáveis-em-função-do-status-inadimplente-estudante",
    "href": "semanas/Aula08.html#avaliando-o-comportamento-das-variáveis-em-função-do-status-inadimplente-estudante",
    "title": "KNN",
    "section": "Avaliando o comportamento das variáveis em função do status (inadimplente / estudante)",
    "text": "Avaliando o comportamento das variáveis em função do status (inadimplente / estudante)\n\nlibrary(patchwork)\np1 &lt;- ggplot(credito, aes(x=inadimplente, y=balanco, color=inadimplente)) +\n  geom_boxplot()\np2 &lt;- ggplot(credito, aes(x=inadimplente, y=receita, color=inadimplente)) +\n  geom_boxplot()\np3 &lt;- ggplot(credito, aes(x=as.factor(estudante), y=balanco, color=as.factor(estudante))) +\n  geom_boxplot()\np4 &lt;- ggplot(credito, aes(x=as.factor(estudante), y=receita, color=as.factor(estudante))) +\n  geom_boxplot()\n(p1 + p2) / (p3 + p4)",
    "crumbs": [
      "Conteúdo Semanal",
      "KNN"
    ]
  },
  {
    "objectID": "semanas/Aula08.html#explorando-um-pouco-mais-balanço-e-receita",
    "href": "semanas/Aula08.html#explorando-um-pouco-mais-balanço-e-receita",
    "title": "KNN",
    "section": "Explorando um pouco mais Balanço e Receita",
    "text": "Explorando um pouco mais Balanço e Receita\n\np5 &lt;- ggplot(credito, aes(x=balanco)) +\n  geom_histogram(bins = round(1+3.322*log10(nrow(credito)),0))\np6 &lt;- ggplot(credito, aes(x=receita)) +\n    geom_histogram(bins = round(1+3.322*log10(nrow(credito)),0))\np5 + p6",
    "crumbs": [
      "Conteúdo Semanal",
      "KNN"
    ]
  },
  {
    "objectID": "semanas/Aula08.html#balanço-vs-receita",
    "href": "semanas/Aula08.html#balanço-vs-receita",
    "title": "KNN",
    "section": "Balanço vs Receita",
    "text": "Balanço vs Receita\n\nggplot(data = credito, aes(x=balanco,  y = receita, col = inadimplente)) + geom_point()",
    "crumbs": [
      "Conteúdo Semanal",
      "KNN"
    ]
  },
  {
    "objectID": "semanas/Aula08.html#knn-1",
    "href": "semanas/Aula08.html#knn-1",
    "title": "KNN",
    "section": "KNN",
    "text": "KNN\nVamos usar a função knn da biblioteca caret que tem ótimas funcionalidades. Observem que a saída pode ser as classes ou as probabilidades de pertencer a uma classe\nComo o KNN usa as distancias entre os pontos ele é afetado pela escala dos dados, portanto, é necessário que os dados sejam normalizados (padronizados) para eliminar este efeito.\nQuando temos diversas variáveis explicativas em diferentes escalas, em geral, elas devem ser transformadas para ter media zero e desvio padrão 1",
    "crumbs": [
      "Conteúdo Semanal",
      "KNN"
    ]
  },
  {
    "objectID": "semanas/Aula08.html#criando-conjuntos-de-treino-e-teste-e-normalizando-variáveis",
    "href": "semanas/Aula08.html#criando-conjuntos-de-treino-e-teste-e-normalizando-variáveis",
    "title": "KNN",
    "section": "Criando conjuntos de treino e teste e normalizando variáveis",
    "text": "Criando conjuntos de treino e teste e normalizando variáveis\n\nlibrary(caret)\nset.seed(2024)\ny &lt;- credito$inadimplente\ncredito_split &lt;- createDataPartition(y, times = 1, p = 0.80, list = FALSE)\n\nconj_treino &lt;- credito[credito_split,]\nconj_treino[,3:4] &lt;- scale(conj_treino[,3:4]) # scale normaliza\nconj_teste &lt;- credito[-credito_split,]\nconj_teste[,3:4] &lt;- scale(conj_teste[, 3:4])\n                           \nsummary(conj_treino)\n\n#&gt;  inadimplente   estudante         balanco            receita        \n#&gt;  Nao:7734     Min.   :0.0000   Min.   :-1.72131   Min.   :-2.45922  \n#&gt;  Sim: 267     1st Qu.:0.0000   1st Qu.:-0.73380   1st Qu.:-0.91305  \n#&gt;               Median :0.0000   Median :-0.02068   Median : 0.07759  \n#&gt;               Mean   :0.2945   Mean   : 0.00000   Mean   : 0.00000  \n#&gt;               3rd Qu.:1.0000   3rd Qu.: 0.68805   3rd Qu.: 0.77310  \n#&gt;               Max.   :1.0000   Max.   : 3.74460   Max.   : 2.93799\n\nsummary(conj_teste)\n\n#&gt;  inadimplente   estudante         balanco            receita        \n#&gt;  Nao:1933     Min.   :0.0000   Min.   :-1.75009   Min.   :-2.12222  \n#&gt;  Sim:  66     1st Qu.:0.0000   1st Qu.:-0.72205   1st Qu.:-0.91629  \n#&gt;               Median :0.0000   Median :-0.04085   Median : 0.09891  \n#&gt;               Mean   :0.2941   Mean   : 0.00000   Mean   : 0.00000  \n#&gt;               3rd Qu.:1.0000   3rd Qu.: 0.65118   3rd Qu.: 0.76539  \n#&gt;               Max.   :1.0000   Max.   : 3.50597   Max.   : 2.93133",
    "crumbs": [
      "Conteúdo Semanal",
      "KNN"
    ]
  },
  {
    "objectID": "semanas/Aula08.html#a-modelo",
    "href": "semanas/Aula08.html#a-modelo",
    "title": "KNN",
    "section": "1a Modelo",
    "text": "1a Modelo\nVamos usar a regra da raiz quadrada do tamanho da amostra para definir o número de vizinhos do KNN.\n\nlibrary(caret)\nsqrt(nrow(conj_treino)) ## ~90\n\n#&gt; [1] 89.44831\n\nset.seed(23)\n\nt_knn1 &lt;- knn3(inadimplente ~ balanco + receita + estudante, data = conj_treino, k = 90)\nt_knn1\n\n#&gt; 90-nearest neighbor model\n#&gt; Training set outcome distribution:\n#&gt; \n#&gt;  Nao  Sim \n#&gt; 7734  267",
    "crumbs": [
      "Conteúdo Semanal",
      "KNN"
    ]
  },
  {
    "objectID": "semanas/Aula08.html#avaliando-o-modelo",
    "href": "semanas/Aula08.html#avaliando-o-modelo",
    "title": "KNN",
    "section": "Avaliando o modelo",
    "text": "Avaliando o modelo\nAtravés da função matriz de confusão do pacote caret conseguimos obter as principais medidas de avaliação de um modelo de classificação.\nVeja que a acurácia deu um valor alto, mas isto não é suficiente para considerarmos que temos um bom modelo. Veja que a sensibilidade está muito baixa e que o ideal é que tenhamos valores altos de sensibilidade e especificidade.\nObservar que a prevalência é muito baixa o que está afetando os resultados do modelo.\n\ny_chapeu_knn1 &lt;- predict(t_knn1, conj_teste, type = \"class\")\n\n\nconfusionMatrix(y_chapeu_knn1, conj_teste$inadimplente, positive=\"Sim\") \n\n#&gt; Confusion Matrix and Statistics\n#&gt; \n#&gt;           Reference\n#&gt; Prediction  Nao  Sim\n#&gt;        Nao 1932   50\n#&gt;        Sim    1   16\n#&gt;                                           \n#&gt;                Accuracy : 0.9745          \n#&gt;                  95% CI : (0.9666, 0.9809)\n#&gt;     No Information Rate : 0.967           \n#&gt;     P-Value [Acc &gt; NIR] : 0.03105         \n#&gt;                                           \n#&gt;                   Kappa : 0.3771          \n#&gt;                                           \n#&gt;  Mcnemar's Test P-Value : 1.801e-11       \n#&gt;                                           \n#&gt;             Sensitivity : 0.242424        \n#&gt;             Specificity : 0.999483        \n#&gt;          Pos Pred Value : 0.941176        \n#&gt;          Neg Pred Value : 0.974773        \n#&gt;              Prevalence : 0.033017        \n#&gt;          Detection Rate : 0.008004        \n#&gt;    Detection Prevalence : 0.008504        \n#&gt;       Balanced Accuracy : 0.620953        \n#&gt;                                           \n#&gt;        'Positive' Class : Sim             \n#&gt;",
    "crumbs": [
      "Conteúdo Semanal",
      "KNN"
    ]
  },
  {
    "objectID": "semanas/Aula08.html#curva-roc",
    "href": "semanas/Aula08.html#curva-roc",
    "title": "KNN",
    "section": "Curva ROC",
    "text": "Curva ROC\nPara a curva ROC é necessário que obtenhamos as probabilidades e não das classes, vejam nos comandos abaixo como se obtem as probabilidades.\n\nlibrary(pROC)\n\n# \np_chapeu_knn1 &lt;- predict(t_knn1, conj_teste, type = \"prob\")\nhead(p_chapeu_knn1)\n\n#&gt;            Nao        Sim\n#&gt; [1,] 1.0000000 0.00000000\n#&gt; [2,] 0.9777778 0.02222222\n#&gt; [3,] 1.0000000 0.00000000\n#&gt; [4,] 1.0000000 0.00000000\n#&gt; [5,] 1.0000000 0.00000000\n#&gt; [6,] 0.9888889 0.01111111\n\n# Aqui gera o curva e salvo numa variável\nroc_knn1 &lt;- roc(conj_teste$inadimplente ~ p_chapeu_knn1[,2], plot = TRUE, print.auc=FALSE, col=\"black\", legacy.axes=TRUE)\n\nlegend(\"bottomright\",legend=c(\"KNN1\"), \n       col=c(\"black\"),lwd=4)",
    "crumbs": [
      "Conteúdo Semanal",
      "KNN"
    ]
  },
  {
    "objectID": "semanas/Aula08.html#area-embaixo-da-curva-roc",
    "href": "semanas/Aula08.html#area-embaixo-da-curva-roc",
    "title": "KNN",
    "section": "Area embaixo da curva ROC",
    "text": "Area embaixo da curva ROC\n\n# Area abaixo da Curva (AUC)\nas.numeric(roc_knn1$auc)\n\n#&gt; [1] 0.9468286",
    "crumbs": [
      "Conteúdo Semanal",
      "KNN"
    ]
  },
  {
    "objectID": "semanas/Aula08.html#variando-k",
    "href": "semanas/Aula08.html#variando-k",
    "title": "KNN",
    "section": "Variando K",
    "text": "Variando K\nAnteriormente usamos k=90. Este parametro deve ser ajustado para melhoramos os modelo KNN. Para isto vamos usar a função train da biblioteca caret\nObserve que a otimização de k é feita através de acurácia.\n\nset.seed(2024)\n\n# Usando validação cruzada para obter o valor de k através da função train da biblioteca caret e o controle do treino e fazendo um gride de valores para k.\nctrl &lt;- trainControl(method = \"repeatedcv\", \n                     number = 10,\n                     repeats = 5)\nt_knn2 &lt;- train(inadimplente ~ balanco + receita + estudante,\n                method = \"knn\", \n                trControl= ctrl,\n                tuneGrid = data.frame(k = seq(5,100, by=5)),\n                metric = \"Accuracy\",\n                data = conj_treino)\n## Resultados do treino\nt_knn2\n\n#&gt; k-Nearest Neighbors \n#&gt; \n#&gt; 8001 samples\n#&gt;    3 predictor\n#&gt;    2 classes: 'Nao', 'Sim' \n#&gt; \n#&gt; No pre-processing\n#&gt; Resampling: Cross-Validated (10 fold, repeated 5 times) \n#&gt; Summary of sample sizes: 7200, 7202, 7200, 7202, 7201, 7200, ... \n#&gt; Resampling results across tuning parameters:\n#&gt; \n#&gt;   k    Accuracy   Kappa    \n#&gt;     5  0.9687539  0.3713723\n#&gt;    10  0.9710787  0.4074849\n#&gt;    15  0.9719035  0.3949430\n#&gt;    20  0.9714038  0.3650255\n#&gt;    25  0.9716285  0.3568810\n#&gt;    30  0.9716288  0.3532068\n#&gt;    35  0.9715038  0.3445969\n#&gt;    40  0.9715037  0.3418931\n#&gt;    45  0.9714537  0.3369602\n#&gt;    50  0.9715037  0.3314707\n#&gt;    55  0.9711037  0.3129172\n#&gt;    60  0.9707038  0.2893980\n#&gt;    65  0.9705289  0.2761428\n#&gt;    70  0.9704536  0.2663185\n#&gt;    75  0.9699037  0.2364880\n#&gt;    80  0.9696287  0.2176720\n#&gt;    85  0.9693037  0.1946375\n#&gt;    90  0.9688291  0.1614589\n#&gt;    95  0.9685543  0.1362015\n#&gt;   100  0.9680543  0.1070964\n#&gt; \n#&gt; Accuracy was used to select the optimal model using the largest value.\n#&gt; The final value used for the model was k = 15.\n\nplot(t_knn2)\n\n\n\n\n\n\n## Previsões com o resultaddos do treino\nprev_knn2 &lt;- predict(t_knn2, conj_teste)\nconfusionMatrix(prev_knn2, conj_teste$inadimplente,  positive=\"Sim\")\n\n#&gt; Confusion Matrix and Statistics\n#&gt; \n#&gt;           Reference\n#&gt; Prediction  Nao  Sim\n#&gt;        Nao 1924   41\n#&gt;        Sim    9   25\n#&gt;                                           \n#&gt;                Accuracy : 0.975           \n#&gt;                  95% CI : (0.9672, 0.9814)\n#&gt;     No Information Rate : 0.967           \n#&gt;     P-Value [Acc &gt; NIR] : 0.02267         \n#&gt;                                           \n#&gt;                   Kappa : 0.4885          \n#&gt;                                           \n#&gt;  Mcnemar's Test P-Value : 1.165e-05       \n#&gt;                                           \n#&gt;             Sensitivity : 0.37879         \n#&gt;             Specificity : 0.99534         \n#&gt;          Pos Pred Value : 0.73529         \n#&gt;          Neg Pred Value : 0.97913         \n#&gt;              Prevalence : 0.03302         \n#&gt;          Detection Rate : 0.01251         \n#&gt;    Detection Prevalence : 0.01701         \n#&gt;       Balanced Accuracy : 0.68707         \n#&gt;                                           \n#&gt;        'Positive' Class : Sim             \n#&gt;",
    "crumbs": [
      "Conteúdo Semanal",
      "KNN"
    ]
  },
  {
    "objectID": "semanas/Aula08.html#curva-roc-dos-2-melhores-modelos-k90-e-k15",
    "href": "semanas/Aula08.html#curva-roc-dos-2-melhores-modelos-k90-e-k15",
    "title": "KNN",
    "section": "Curva ROC dos 2 melhores modelos k=90 e k=15",
    "text": "Curva ROC dos 2 melhores modelos k=90 e k=15\n\nprev_knn1 &lt;- predict(t_knn1, conj_teste, type = \"prob\")\nprev_knn2 &lt;- predict(t_knn2, conj_teste, type = \"prob\")\nroc_knn1 &lt;- roc(conj_teste$inadimplente ~ prev_knn1[,2], plot = TRUE, print.auc=FALSE, col=\"black\", legacy.axes=TRUE)\nroc_knn2 &lt;- roc(conj_teste$inadimplente ~ prev_knn2[,2], plot = TRUE, print.auc=FALSE, col=\"green\", legacy.axes=TRUE, add=TRUE)\nlegend(\"bottomright\",legend=c(\"KNN1\", \"KNN2\"), \n       col=c(\"black\",\"green\"),lwd=4)\n\n\n\n\n\n\n## Area embaixo das curvas\nas.numeric(roc_knn1$auc)\n\n#&gt; [1] 0.9468286\n\nas.numeric(roc_knn2$auc)\n\n#&gt; [1] 0.8927715\n\n\nObserve que os resultados de área abaixo da ROC não são suficientes para a escolha do k, pois precisamos estar atentos a sensibilidade e especificidade!",
    "crumbs": [
      "Conteúdo Semanal",
      "KNN"
    ]
  },
  {
    "objectID": "semanas/Aula09.html",
    "href": "semanas/Aula09.html",
    "title": "Regressão Logística",
    "section": "",
    "text": "library(tidyverse)\nlibrary(ISLR)\ndata(Default)\nsummary(Default)\n\n default    student       balance           income     \n No :9667   No :7056   Min.   :   0.0   Min.   :  772  \n Yes: 333   Yes:2944   1st Qu.: 481.7   1st Qu.:21340  \n                       Median : 823.6   Median :34553  \n                       Mean   : 835.4   Mean   :33517  \n                       3rd Qu.:1166.3   3rd Qu.:43808  \n                       Max.   :2654.3   Max.   :73554  \n\nstr(Default)\n\n'data.frame':   10000 obs. of  4 variables:\n $ default: Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n $ student: Factor w/ 2 levels \"No\",\"Yes\": 1 2 1 1 1 2 1 2 1 1 ...\n $ balance: num  730 817 1074 529 786 ...\n $ income : num  44362 12106 31767 35704 38463 ...\n\nhead(Default)\n\n  default student   balance    income\n1      No      No  729.5265 44361.625\n2      No     Yes  817.1804 12106.135\n3      No      No 1073.5492 31767.139\n4      No      No  529.2506 35704.494\n5      No      No  785.6559 38463.496\n6      No     Yes  919.5885  7491.559",
    "crumbs": [
      "Conteúdo Semanal",
      "Regressão Logística"
    ]
  },
  {
    "objectID": "semanas/Aula09.html#carregando-bibliotecas",
    "href": "semanas/Aula09.html#carregando-bibliotecas",
    "title": "Regressão Logística",
    "section": "",
    "text": "library(tidyverse)\nlibrary(ISLR)\ndata(Default)\nsummary(Default)\n\n default    student       balance           income     \n No :9667   No :7056   Min.   :   0.0   Min.   :  772  \n Yes: 333   Yes:2944   1st Qu.: 481.7   1st Qu.:21340  \n                       Median : 823.6   Median :34553  \n                       Mean   : 835.4   Mean   :33517  \n                       3rd Qu.:1166.3   3rd Qu.:43808  \n                       Max.   :2654.3   Max.   :73554  \n\nstr(Default)\n\n'data.frame':   10000 obs. of  4 variables:\n $ default: Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n $ student: Factor w/ 2 levels \"No\",\"Yes\": 1 2 1 1 1 2 1 2 1 1 ...\n $ balance: num  730 817 1074 529 786 ...\n $ income : num  44362 12106 31767 35704 38463 ...\n\nhead(Default)\n\n  default student   balance    income\n1      No      No  729.5265 44361.625\n2      No     Yes  817.1804 12106.135\n3      No      No 1073.5492 31767.139\n4      No      No  529.2506 35704.494\n5      No      No  785.6559 38463.496\n6      No     Yes  919.5885  7491.559",
    "crumbs": [
      "Conteúdo Semanal",
      "Regressão Logística"
    ]
  },
  {
    "objectID": "semanas/Aula09.html#manipulando-os-dados",
    "href": "semanas/Aula09.html#manipulando-os-dados",
    "title": "Regressão Logística",
    "section": "Manipulando os dados",
    "text": "Manipulando os dados\n\ncredito &lt;- tibble(Default)\nsummary(credito)\n\n default    student       balance           income     \n No :9667   No :7056   Min.   :   0.0   Min.   :  772  \n Yes: 333   Yes:2944   1st Qu.: 481.7   1st Qu.:21340  \n                       Median : 823.6   Median :34553  \n                       Mean   : 835.4   Mean   :33517  \n                       3rd Qu.:1166.3   3rd Qu.:43808  \n                       Max.   :2654.3   Max.   :73554  \n\n# renomeando colunas\ncredito &lt;- credito %&gt;% \n                rename( inadimplente = default, estudante = student, balanco = balance,\n                receita = income)\ncredito &lt;- credito %&gt;% mutate( inadimplente =  case_when(\n                           inadimplente == \"No\"  ~ \"Nao\",\n                           inadimplente == \"Yes\" ~ \"Sim\"\n                          )) %&gt;% mutate(inadimplente = factor(inadimplente))\ncredito &lt;- credito %&gt;% mutate( estudante =  case_when(\n                           estudante == \"No\"  ~ 0,\n                           estudante == \"Yes\" ~ 1\n                          )) \n\nstr(credito)\n\ntibble [10,000 × 4] (S3: tbl_df/tbl/data.frame)\n $ inadimplente: Factor w/ 2 levels \"Nao\",\"Sim\": 1 1 1 1 1 1 1 1 1 1 ...\n $ estudante   : num [1:10000] 0 1 0 0 0 1 0 1 0 0 ...\n $ balanco     : num [1:10000] 730 817 1074 529 786 ...\n $ receita     : num [1:10000] 44362 12106 31767 35704 38463 ...\n\nsummary(credito)\n\n inadimplente   estudante         balanco          receita     \n Nao:9667     Min.   :0.0000   Min.   :   0.0   Min.   :  772  \n Sim: 333     1st Qu.:0.0000   1st Qu.: 481.7   1st Qu.:21340  \n              Median :0.0000   Median : 823.6   Median :34553  \n              Mean   :0.2944   Mean   : 835.4   Mean   :33517  \n              3rd Qu.:1.0000   3rd Qu.:1166.3   3rd Qu.:43808  \n              Max.   :1.0000   Max.   :2654.3   Max.   :73554",
    "crumbs": [
      "Conteúdo Semanal",
      "Regressão Logística"
    ]
  },
  {
    "objectID": "semanas/Aula09.html#treino-e-teste",
    "href": "semanas/Aula09.html#treino-e-teste",
    "title": "Regressão Logística",
    "section": "Treino e Teste",
    "text": "Treino e Teste\n\nlibrary(caret)\nset.seed(2024)\ny &lt;- credito$inadimplente\ncredito_split &lt;- createDataPartition(y, times = 1, p = 0.80, list = FALSE)\n\nconj_treino &lt;- credito[credito_split,]\nconj_teste &lt;- credito[-credito_split,]\n                           \nsummary(conj_treino)\n\n inadimplente   estudante         balanco          receita     \n Nao:7734     Min.   :0.0000   Min.   :   0.0   Min.   :  772  \n Sim: 267     1st Qu.:0.0000   1st Qu.: 479.5   1st Qu.:21309  \n              Median :0.0000   Median : 825.9   Median :34468  \n              Mean   :0.2945   Mean   : 835.9   Mean   :33437  \n              3rd Qu.:1.0000   3rd Qu.:1170.0   3rd Qu.:43706  \n              Max.   :1.0000   Max.   :2654.3   Max.   :72461  \n\nsummary(conj_teste)\n\n inadimplente   estudante         balanco          receita     \n Nao:1933     Min.   :0.0000   Min.   :   0.0   Min.   : 5083  \n Sim:  66     1st Qu.:0.0000   1st Qu.: 489.5   1st Qu.:21422  \n              Median :0.0000   Median : 813.9   Median :35177  \n              Mean   :0.2941   Mean   : 833.3   Mean   :33837  \n              3rd Qu.:1.0000   3rd Qu.:1143.4   3rd Qu.:44208  \n              Max.   :1.0000   Max.   :2502.7   Max.   :73554",
    "crumbs": [
      "Conteúdo Semanal",
      "Regressão Logística"
    ]
  },
  {
    "objectID": "semanas/Aula09.html#matriz-de-dispersão",
    "href": "semanas/Aula09.html#matriz-de-dispersão",
    "title": "Regressão Logística",
    "section": "Matriz de dispersão",
    "text": "Matriz de dispersão\n\nlibrary(psych)\npairs.panels(conj_treino, \n             method = \"pearson\", # metodo de correlação\n             hist.col = \"#00AFBB\",\n             density = TRUE,  # mostra graficos de densidade\n             ellipses = FALSE # mostra elipses de correlação\n             )",
    "crumbs": [
      "Conteúdo Semanal",
      "Regressão Logística"
    ]
  },
  {
    "objectID": "semanas/Aula09.html#avaliando-o-comportamento-das-variáveis-em-função-do-status-inadimplente-estudante",
    "href": "semanas/Aula09.html#avaliando-o-comportamento-das-variáveis-em-função-do-status-inadimplente-estudante",
    "title": "Regressão Logística",
    "section": "Avaliando o comportamento das variáveis em função do status (inadimplente / estudante)",
    "text": "Avaliando o comportamento das variáveis em função do status (inadimplente / estudante)\n\nlibrary(patchwork)\np1 &lt;- ggplot(credito, aes(x=inadimplente, y=balanco, color=inadimplente)) +\n  geom_boxplot()\np2 &lt;- ggplot(credito, aes(x=inadimplente, y=receita, color=inadimplente)) +\n  geom_boxplot()\np3 &lt;- ggplot(credito, aes(x=as.factor(estudante), y=balanco, color=as.factor(estudante))) +\n  geom_boxplot()\np4 &lt;- ggplot(credito, aes(x=as.factor(estudante), y=receita, color=as.factor(estudante))) +\n  geom_boxplot()\n(p1 + p2) / (p3 + p4)",
    "crumbs": [
      "Conteúdo Semanal",
      "Regressão Logística"
    ]
  },
  {
    "objectID": "semanas/Aula09.html#balanço-vs-receita",
    "href": "semanas/Aula09.html#balanço-vs-receita",
    "title": "Regressão Logística",
    "section": "Balanço vs Receita",
    "text": "Balanço vs Receita\n\nggplot(data = credito, aes(x=balanco,  y = receita, col = inadimplente)) +\n  geom_point()",
    "crumbs": [
      "Conteúdo Semanal",
      "Regressão Logística"
    ]
  },
  {
    "objectID": "semanas/Aula09.html#avaliando-comportamento",
    "href": "semanas/Aula09.html#avaliando-comportamento",
    "title": "Regressão Logística",
    "section": "Avaliando comportamento",
    "text": "Avaliando comportamento\n\n# proporção de inadimplentes\ncredito %&gt;% select(inadimplente, balanco) %&gt;% summarize(prop = mean(inadimplente == \"Sim\")) \n\n# A tibble: 1 × 1\n    prop\n   &lt;dbl&gt;\n1 0.0333\n\n# media do balanço dos inadimplentes \ncredito %&gt;% filter(inadimplente == \"Sim\") %&gt;% summarize(valor= mean(balanco))   \n\n# A tibble: 1 × 1\n  valor\n  &lt;dbl&gt;\n1 1748.\n\nquantis &lt;- quantile(credito$balanco, probs = c(.1,.25, .50, .75, .9, .95, 0.97, 0.99))\nquantis\n\n      10%       25%       50%       75%       90%       95%       97%       99% \n 180.5753  481.7311  823.6370 1166.3084 1471.6253 1665.9626 1793.2910 2008.4709 \n\ncredito %&gt;% \n            mutate(grupo_balanco = case_when(\n               balanco&lt;=quantis[1] ~ quantis[1],\n               balanco&gt;quantis[1] & balanco&lt;=quantis[2] ~ quantis[2],\n               balanco&gt;quantis[2] & balanco&lt;=quantis[3]  ~ quantis[3],\n               balanco&gt;quantis[3] & balanco&lt;=quantis[4]  ~ quantis[4],\n               balanco&gt;quantis[4] & balanco&lt;=quantis[5]  ~ quantis[5],\n               balanco&gt;quantis[5] & balanco&lt;=quantis[6]  ~ quantis[6],\n               balanco&gt;quantis[6] & balanco&lt;=quantis[7]  ~ quantis[7],\n               balanco&gt;quantis[7] ~ quantis[8])) %&gt;%\n           group_by(grupo_balanco) %&gt;%\n           summarize(prop = mean(inadimplente == \"Sim\")) %&gt;%\n           ggplot(aes(grupo_balanco, prop)) +\n           geom_point() +\n           geom_line()",
    "crumbs": [
      "Conteúdo Semanal",
      "Regressão Logística"
    ]
  },
  {
    "objectID": "semanas/Aula09.html#a-regressão-logística-só-balanço",
    "href": "semanas/Aula09.html#a-regressão-logística-só-balanço",
    "title": "Regressão Logística",
    "section": "1a Regressão logística: só balanço",
    "text": "1a Regressão logística: só balanço\n\nmod1 &lt;- glm(inadimplente ~ balanco,data=conj_treino,family=binomial)\nsummary(mod1)\n\n\nCall:\nglm(formula = inadimplente ~ balanco, family = binomial, data = conj_treino)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -1.047e+01  3.931e-01  -26.64   &lt;2e-16 ***\nbalanco      5.386e-03  2.405e-04   22.40   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2340.6  on 8000  degrees of freedom\nResidual deviance: 1302.1  on 7999  degrees of freedom\nAIC: 1306.1\n\nNumber of Fisher Scoring iterations: 8\n\ncoef(mod1)\n\n  (Intercept)       balanco \n-10.471543192   0.005385567 \n\nsummary(mod1)$coef\n\n                 Estimate   Std. Error   z value      Pr(&gt;|z|)\n(Intercept) -10.471543192 0.3931122212 -26.63754 2.495385e-156\nbalanco       0.005385567 0.0002404507  22.39780 4.134795e-111",
    "crumbs": [
      "Conteúdo Semanal",
      "Regressão Logística"
    ]
  },
  {
    "objectID": "semanas/Aula09.html#avaliando-o-modelo",
    "href": "semanas/Aula09.html#avaliando-o-modelo",
    "title": "Regressão Logística",
    "section": "Avaliando o modelo",
    "text": "Avaliando o modelo\n\np_chapeu &lt;- predict(mod1, newdata = conj_teste, type = \"response\")\ny_chapeu &lt;- ifelse(p_chapeu &gt; 0.5, \"Sim\", \"Nao\") %&gt;% factor(levels = levels(conj_teste$inadimplente))\nconfusionMatrix(y_chapeu, conj_teste$inadimplente, positive=\"Sim\") \n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  Nao  Sim\n       Nao 1929   45\n       Sim    4   21\n                                          \n               Accuracy : 0.9755          \n                 95% CI : (0.9677, 0.9818)\n    No Information Rate : 0.967           \n    P-Value [Acc &gt; NIR] : 0.01625         \n                                          \n                  Kappa : 0.4516          \n                                          \n Mcnemar's Test P-Value : 1.102e-08       \n                                          \n            Sensitivity : 0.31818         \n            Specificity : 0.99793         \n         Pos Pred Value : 0.84000         \n         Neg Pred Value : 0.97720         \n             Prevalence : 0.03302         \n         Detection Rate : 0.01051         \n   Detection Prevalence : 0.01251         \n      Balanced Accuracy : 0.65806         \n                                          \n       'Positive' Class : Sim",
    "crumbs": [
      "Conteúdo Semanal",
      "Regressão Logística"
    ]
  },
  {
    "objectID": "semanas/Aula09.html#veja-as-probabilidade-de-inadimplencia-para-balanços-de-1000-2000-e-3000",
    "href": "semanas/Aula09.html#veja-as-probabilidade-de-inadimplencia-para-balanços-de-1000-2000-e-3000",
    "title": "Regressão Logística",
    "section": "Veja as probabilidade de inadimplencia para balanços de 1000, 2000 e 3000",
    "text": "Veja as probabilidade de inadimplencia para balanços de 1000, 2000 e 3000\n\npredict(mod1, newdata = data.frame(balanco = c(1000,2000,3000)), type=\"response\")\n\n          1           2           3 \n0.006144857 0.574342575 0.996615498",
    "crumbs": [
      "Conteúdo Semanal",
      "Regressão Logística"
    ]
  },
  {
    "objectID": "semanas/Aula09.html#curva-s",
    "href": "semanas/Aula09.html#curva-s",
    "title": "Regressão Logística",
    "section": "Curva S",
    "text": "Curva S\n\ninadimpl &lt;- as.numeric(conj_treino$inadimplente) - 1\nplot(inadimpl ~ balanco, data = conj_treino, \n     col = \"darkorange\", pch = \"|\", ylim = c(0, 1),\n     main = \"Regressão Logistica - Classificacão\")\nabline(h = 0, lty = 3)\nabline(h = 1, lty = 3)\nabline(h = 0.5, lty = 2)\ncurve(predict(mod1, data.frame(balanco = x),\n        type = \"response\"), add = TRUE, lwd = 3, col = \"dodgerblue\")\nabline(v = -coef(mod1)[1] / coef(mod1)[2], lwd = 2)",
    "crumbs": [
      "Conteúdo Semanal",
      "Regressão Logística"
    ]
  },
  {
    "objectID": "semanas/Aula09.html#valor-de-balanço-com-probabilidade-de-50",
    "href": "semanas/Aula09.html#valor-de-balanço-com-probabilidade-de-50",
    "title": "Regressão Logística",
    "section": "Valor de balanço com probabilidade de 50%",
    "text": "Valor de balanço com probabilidade de 50%\n-\\(\\beta_0\\)/\\(\\beta_1\\)\n\n-coef(mod1)[1] / coef(mod1)[2]\n\n(Intercept) \n   1944.371",
    "crumbs": [
      "Conteúdo Semanal",
      "Regressão Logística"
    ]
  },
  {
    "objectID": "semanas/Aula09.html#a-regressão-logística-todas-as-variáveis",
    "href": "semanas/Aula09.html#a-regressão-logística-todas-as-variáveis",
    "title": "Regressão Logística",
    "section": "2a Regressão logística: todas as variáveis",
    "text": "2a Regressão logística: todas as variáveis\n\nmod2 &lt;- glm(inadimplente ~ balanco + receita + estudante,data=conj_treino,family=binomial)\nsummary(mod2)\n\n\nCall:\nglm(formula = inadimplente ~ balanco + receita + estudante, family = binomial, \n    data = conj_treino)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -1.075e+01  5.417e-01 -19.847   &lt;2e-16 ***\nbalanco      5.628e-03  2.538e-04  22.180   &lt;2e-16 ***\nreceita      4.416e-06  9.088e-06   0.486   0.6270    \nestudante   -6.264e-01  2.612e-01  -2.398   0.0165 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2340.6  on 8000  degrees of freedom\nResidual deviance: 1281.3  on 7997  degrees of freedom\nAIC: 1289.3\n\nNumber of Fisher Scoring iterations: 8\n\ncoef(mod2)\n\n  (Intercept)       balanco       receita     estudante \n-1.075144e+01  5.628328e-03  4.415834e-06 -6.263630e-01 \n\nsummary(mod2)$coef\n\n                 Estimate   Std. Error     z value      Pr(&gt;|z|)\n(Intercept) -1.075144e+01 5.417102e-01 -19.8472260  1.164551e-87\nbalanco      5.628328e-03 2.537535e-04  22.1802958 5.322786e-109\nreceita      4.415834e-06 9.088132e-06   0.4858902  6.270450e-01\nestudante   -6.263630e-01 2.611624e-01  -2.3983656  1.646842e-02\n\n\nÉ possível se ver que receita não é significativa",
    "crumbs": [
      "Conteúdo Semanal",
      "Regressão Logística"
    ]
  },
  {
    "objectID": "semanas/Aula09.html#a-regressão-logística-sem-receita",
    "href": "semanas/Aula09.html#a-regressão-logística-sem-receita",
    "title": "Regressão Logística",
    "section": "3a Regressão Logística (sem receita)",
    "text": "3a Regressão Logística (sem receita)\n\nmod3 &lt;- glm(inadimplente ~ balanco + estudante,data=conj_treino,family=binomial)\nsummary(mod3)\n\n\nCall:\nglm(formula = inadimplente ~ balanco + estudante, family = binomial, \n    data = conj_treino)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -1.058e+01  4.027e-01 -26.264  &lt; 2e-16 ***\nbalanco      5.629e-03  2.537e-04  22.190  &lt; 2e-16 ***\nestudante   -7.246e-01  1.646e-01  -4.401 1.08e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2340.6  on 8000  degrees of freedom\nResidual deviance: 1281.6  on 7998  degrees of freedom\nAIC: 1287.6\n\nNumber of Fisher Scoring iterations: 8\n\ncoef(mod3)\n\n  (Intercept)       balanco     estudante \n-10.577195871   0.005629461  -0.724574801 \n\nsummary(mod3)$coef\n\n                 Estimate   Std. Error   z value      Pr(&gt;|z|)\n(Intercept) -10.577195871 0.4027279496 -26.26387 4.962858e-152\nbalanco       0.005629461 0.0002536895  22.19035 4.256243e-109\nestudante    -0.724574801 0.1646214717  -4.40146  1.075250e-05",
    "crumbs": [
      "Conteúdo Semanal",
      "Regressão Logística"
    ]
  },
  {
    "objectID": "semanas/Aula09.html#comparando-os-modelos",
    "href": "semanas/Aula09.html#comparando-os-modelos",
    "title": "Regressão Logística",
    "section": "Comparando os modelos",
    "text": "Comparando os modelos\n\nanova(mod2,mod3,test='LR')\n\nAnalysis of Deviance Table\n\nModel 1: inadimplente ~ balanco + receita + estudante\nModel 2: inadimplente ~ balanco + estudante\n  Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi)\n1      7997     1281.3                     \n2      7998     1281.6 -1 -0.23621    0.627",
    "crumbs": [
      "Conteúdo Semanal",
      "Regressão Logística"
    ]
  },
  {
    "objectID": "semanas/Aula09.html#stepaic",
    "href": "semanas/Aula09.html#stepaic",
    "title": "Regressão Logística",
    "section": "StepAIC",
    "text": "StepAIC\nAo invé de usarmos a estatística de Wald para selecionar as variáveis significativas, podemos usar o AIC (equivalente ao Cp) como usamos na regressão múltipla para selecionar as variáveis explicativas.\nA função stepAIC tem um parametro k que define se vamos usar o AIC ou o BIC para fazer a seleção. Quando k=2 temos o AIC e quando k=log(n) temos o BIC.\n\nlibrary(MASS)\nmod3a &lt;- stepAIC(mod2, k=2, trace=FALSE)\nsummary(mod3a)\n\n\nCall:\nglm(formula = inadimplente ~ balanco + estudante, family = binomial, \n    data = conj_treino)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -1.058e+01  4.027e-01 -26.264  &lt; 2e-16 ***\nbalanco      5.629e-03  2.537e-04  22.190  &lt; 2e-16 ***\nestudante   -7.246e-01  1.646e-01  -4.401 1.08e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2340.6  on 8000  degrees of freedom\nResidual deviance: 1281.6  on 7998  degrees of freedom\nAIC: 1287.6\n\nNumber of Fisher Scoring iterations: 8\n\n(k &lt;- log(nrow(conj_treino)))\n\n[1] 8.987322\n\nmod3b &lt;- stepAIC(mod2, k=k, trace=FALSE)\nsummary(mod3b)\n\n\nCall:\nglm(formula = inadimplente ~ balanco + estudante, family = binomial, \n    data = conj_treino)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -1.058e+01  4.027e-01 -26.264  &lt; 2e-16 ***\nbalanco      5.629e-03  2.537e-04  22.190  &lt; 2e-16 ***\nestudante   -7.246e-01  1.646e-01  -4.401 1.08e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2340.6  on 8000  degrees of freedom\nResidual deviance: 1281.6  on 7998  degrees of freedom\nAIC: 1287.6\n\nNumber of Fisher Scoring iterations: 8",
    "crumbs": [
      "Conteúdo Semanal",
      "Regressão Logística"
    ]
  },
  {
    "objectID": "semanas/Aula09.html#avaliando-o-modelo-novamente",
    "href": "semanas/Aula09.html#avaliando-o-modelo-novamente",
    "title": "Regressão Logística",
    "section": "Avaliando o modelo novamente",
    "text": "Avaliando o modelo novamente\n\np_chapeu &lt;- predict(mod3, newdata = conj_teste, type = \"response\")\ny_chapeu &lt;- ifelse(p_chapeu &gt; 0.5, \"Sim\", \"Nao\") %&gt;% factor(levels = levels(conj_teste$inadimplente))\nconfusionMatrix(y_chapeu, conj_teste$inadimplente, positive=\"Sim\") \n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  Nao  Sim\n       Nao 1927   43\n       Sim    6   23\n                                          \n               Accuracy : 0.9755          \n                 95% CI : (0.9677, 0.9818)\n    No Information Rate : 0.967           \n    P-Value [Acc &gt; NIR] : 0.01625         \n                                          \n                  Kappa : 0.4736          \n                                          \n Mcnemar's Test P-Value : 2.706e-07       \n                                          \n            Sensitivity : 0.34848         \n            Specificity : 0.99690         \n         Pos Pred Value : 0.79310         \n         Neg Pred Value : 0.97817         \n             Prevalence : 0.03302         \n         Detection Rate : 0.01151         \n   Detection Prevalence : 0.01451         \n      Balanced Accuracy : 0.67269         \n                                          \n       'Positive' Class : Sim",
    "crumbs": [
      "Conteúdo Semanal",
      "Regressão Logística"
    ]
  },
  {
    "objectID": "semanas/Aula09.html#mudando-a-probabilidade-limite-para-aumentar-a-sensibilidade",
    "href": "semanas/Aula09.html#mudando-a-probabilidade-limite-para-aumentar-a-sensibilidade",
    "title": "Regressão Logística",
    "section": "Mudando a probabilidade (limite) para aumentar a sensibilidade",
    "text": "Mudando a probabilidade (limite) para aumentar a sensibilidade\n\np_chapeu &lt;- predict(mod3, newdata = conj_teste, type = \"response\")\ny_chapeu &lt;- ifelse(p_chapeu &gt; 0.1, \"Sim\", \"Nao\") %&gt;% \n             factor(levels = levels(conj_teste$inadimplente))\nconfusionMatrix(y_chapeu, conj_teste$inadimplente, positive=\"Sim\")\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  Nao  Sim\n       Nao 1823   15\n       Sim  110   51\n                                          \n               Accuracy : 0.9375          \n                 95% CI : (0.9259, 0.9477)\n    No Information Rate : 0.967           \n    P-Value [Acc &gt; NIR] : 1               \n                                          \n                  Kappa : 0.4223          \n                                          \n Mcnemar's Test P-Value : &lt;2e-16          \n                                          \n            Sensitivity : 0.77273         \n            Specificity : 0.94309         \n         Pos Pred Value : 0.31677         \n         Neg Pred Value : 0.99184         \n             Prevalence : 0.03302         \n         Detection Rate : 0.02551         \n   Detection Prevalence : 0.08054         \n      Balanced Accuracy : 0.85791         \n                                          \n       'Positive' Class : Sim",
    "crumbs": [
      "Conteúdo Semanal",
      "Regressão Logística"
    ]
  },
  {
    "objectID": "semanas/Aula09.html#curva-roc-modelo-só-com-balanço",
    "href": "semanas/Aula09.html#curva-roc-modelo-só-com-balanço",
    "title": "Regressão Logística",
    "section": "Curva ROC modelo só com balanço",
    "text": "Curva ROC modelo só com balanço\n\nlibrary(pROC)\np_chapeu_log &lt;- predict(mod1, newdata = conj_teste, type = \"response\")\nhead(p_chapeu_log)\n\n           1            2            3            4            5            6 \n0.0023045487 0.0145058938 0.0000283305 0.0048096860 0.0028238187 0.0116350810 \n\nroc_log &lt;- roc(conj_teste$inadimplente ~ p_chapeu_log, plot = TRUE, print.auc=FALSE, legacy.axes=TRUE)\n\n\n\n\n\n\n# Area debaixo da curva\nas.numeric(roc_log$auc)\n\n[1] 0.9579708",
    "crumbs": [
      "Conteúdo Semanal",
      "Regressão Logística"
    ]
  },
  {
    "objectID": "semanas/Aula09.html#curva-roc-2-modelo-com-balanço-estudante",
    "href": "semanas/Aula09.html#curva-roc-2-modelo-com-balanço-estudante",
    "title": "Regressão Logística",
    "section": "Curva ROC 2: Modelo com balanço + estudante",
    "text": "Curva ROC 2: Modelo com balanço + estudante\n\np_chapeu_log &lt;- predict(mod3, newdata = conj_teste, type = \"response\")\nhead(p_chapeu_log)\n\n           1            2            3            4            5            6 \n1.227576e-03 1.727504e-02 2.549008e-05 5.457908e-03 3.128840e-03 6.698177e-03 \n\nroc_log2 &lt;- roc(conj_teste$inadimplente ~ p_chapeu_log, plot = TRUE, print.auc=FALSE, legacy.axes=TRUE)",
    "crumbs": [
      "Conteúdo Semanal",
      "Regressão Logística"
    ]
  },
  {
    "objectID": "semanas/Aula09.html#auc",
    "href": "semanas/Aula09.html#auc",
    "title": "Regressão Logística",
    "section": "AUC",
    "text": "AUC\n\n# Area debaixo da curva\nas.numeric(roc_log2$auc)\n\n[1] 0.9586919",
    "crumbs": [
      "Conteúdo Semanal",
      "Regressão Logística"
    ]
  },
  {
    "objectID": "semanas/Aula09.html#melhor-limite",
    "href": "semanas/Aula09.html#melhor-limite",
    "title": "Regressão Logística",
    "section": "Melhor limite",
    "text": "Melhor limite\n\nm_limite &lt;- coords(roc_log2, \"best\", ret = \"threshold\")$threshold\nm_limite\n\n[1] 0.03752365\n\np_chapeu &lt;- predict(mod3, newdata = conj_teste, type = \"response\")\ny_chapeu &lt;- ifelse(p_chapeu &gt; m_limite, \"Sim\", \"Nao\") %&gt;% \n             factor(levels = levels(conj_teste$inadimplente))\nconfusionMatrix(y_chapeu, conj_teste$inadimplente, positive=\"Sim\")\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  Nao  Sim\n       Nao 1701    6\n       Sim  232   60\n                                          \n               Accuracy : 0.8809          \n                 95% CI : (0.8659, 0.8948)\n    No Information Rate : 0.967           \n    P-Value [Acc &gt; NIR] : 1               \n                                          \n                  Kappa : 0.2974          \n                                          \n Mcnemar's Test P-Value : &lt;2e-16          \n                                          \n            Sensitivity : 0.90909         \n            Specificity : 0.87998         \n         Pos Pred Value : 0.20548         \n         Neg Pred Value : 0.99649         \n             Prevalence : 0.03302         \n         Detection Rate : 0.03002         \n   Detection Prevalence : 0.14607         \n      Balanced Accuracy : 0.89454         \n                                          \n       'Positive' Class : Sim",
    "crumbs": [
      "Conteúdo Semanal",
      "Regressão Logística"
    ]
  },
  {
    "objectID": "semanas/Aula09.html#duas-rocs-juntas",
    "href": "semanas/Aula09.html#duas-rocs-juntas",
    "title": "Regressão Logística",
    "section": "Duas ROCs juntas",
    "text": "Duas ROCs juntas\n\nplot(roc_log)\nplot(roc_log2, add=TRUE, col=\"blue\")\nlegend(\"bottomright\", legend=c(\"Mod 1\", \"Mod2\"),\n       col=c(par(\"fg\"), \"blue\"), lwd=2)",
    "crumbs": [
      "Conteúdo Semanal",
      "Regressão Logística"
    ]
  },
  {
    "objectID": "semanas/Aula09.html#curva-roc-3-com-o-knn",
    "href": "semanas/Aula09.html#curva-roc-3-com-o-knn",
    "title": "Regressão Logística",
    "section": "Curva ROC 3 com o KNN",
    "text": "Curva ROC 3 com o KNN\n\n# Ajustando KNN \nset.seed(23)\nconj_treino[,3:4] &lt;- scale(conj_treino[,3:4]) # scale normaliza\nconj_teste[,3:4] &lt;- scale(conj_teste[, 3:4])\nctrl &lt;- trainControl(method = \"cv\")\ntreina_knn &lt;- train(inadimplente ~ balanco + estudante, method = \"knn\", trControl= ctrl, tuneGrid = data.frame(k = seq(5,140, by=5)), data = conj_treino)\n# treina_knn\nplot(treina_knn)\n\n\n\n\n\n\nprev_knn &lt;- predict(treina_knn, conj_teste,type = \"prob\")\n\n## ROC\nroc_log2 &lt;- roc(conj_teste$inadimplente ~ p_chapeu_log, plot = TRUE, print.auc=FALSE, col= \"black\", legacy.axes=TRUE) \nroc_knn1 &lt;- roc(conj_teste$inadimplente ~ prev_knn[,2], plot = TRUE, print.auc=FALSE, col=\"green\", legacy.axes=TRUE, add=TRUE)\n\nlegend(\"bottomright\",legend=c(\"Reg. Log\", \"KNN\"), \n       col=c(\"black\",\"green\"),lwd=4)\n\n\n\n\n\n\n# Area abaixo da curva\n# Regressão Logística\nas.numeric(roc_log2$auc)\n\n[1] 0.9586919\n\n## KNN\nas.numeric(roc_knn1$auc)\n\n[1] 0.943462",
    "crumbs": [
      "Conteúdo Semanal",
      "Regressão Logística"
    ]
  },
  {
    "objectID": "semanas/Aula10.html#carregando-bibliotecas",
    "href": "semanas/Aula10.html#carregando-bibliotecas",
    "title": "LDA e QDA",
    "section": "Carregando Bibliotecas",
    "text": "Carregando Bibliotecas\n\nlibrary(tidyverse)\n\n#&gt; ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n#&gt; ✔ dplyr     1.1.4     ✔ readr     2.1.5\n#&gt; ✔ forcats   1.0.0     ✔ stringr   1.5.1\n#&gt; ✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n#&gt; ✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n#&gt; ✔ purrr     1.0.4     \n#&gt; ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n#&gt; ✖ dplyr::filter() masks stats::filter()\n#&gt; ✖ dplyr::lag()    masks stats::lag()\n#&gt; ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(ISLR)\ndata(Default)\nsummary(Default)\n\n#&gt;  default    student       balance           income     \n#&gt;  No :9667   No :7056   Min.   :   0.0   Min.   :  772  \n#&gt;  Yes: 333   Yes:2944   1st Qu.: 481.7   1st Qu.:21340  \n#&gt;                        Median : 823.6   Median :34553  \n#&gt;                        Mean   : 835.4   Mean   :33517  \n#&gt;                        3rd Qu.:1166.3   3rd Qu.:43808  \n#&gt;                        Max.   :2654.3   Max.   :73554\n\nstr(Default)\n\n#&gt; 'data.frame':    10000 obs. of  4 variables:\n#&gt;  $ default: Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n#&gt;  $ student: Factor w/ 2 levels \"No\",\"Yes\": 1 2 1 1 1 2 1 2 1 1 ...\n#&gt;  $ balance: num  730 817 1074 529 786 ...\n#&gt;  $ income : num  44362 12106 31767 35704 38463 ...\n\nhead(Default)\n\n#&gt;   default student   balance    income\n#&gt; 1      No      No  729.5265 44361.625\n#&gt; 2      No     Yes  817.1804 12106.135\n#&gt; 3      No      No 1073.5492 31767.139\n#&gt; 4      No      No  529.2506 35704.494\n#&gt; 5      No      No  785.6559 38463.496\n#&gt; 6      No     Yes  919.5885  7491.559",
    "crumbs": [
      "Conteúdo Semanal",
      "LDA-QDA"
    ]
  },
  {
    "objectID": "semanas/Aula10.html#manipulando-os-dados",
    "href": "semanas/Aula10.html#manipulando-os-dados",
    "title": "LDA e QDA",
    "section": "Manipulando os dados",
    "text": "Manipulando os dados\n\ncredito &lt;- tibble(Default)\nsummary(credito)\n\n#&gt;  default    student       balance           income     \n#&gt;  No :9667   No :7056   Min.   :   0.0   Min.   :  772  \n#&gt;  Yes: 333   Yes:2944   1st Qu.: 481.7   1st Qu.:21340  \n#&gt;                        Median : 823.6   Median :34553  \n#&gt;                        Mean   : 835.4   Mean   :33517  \n#&gt;                        3rd Qu.:1166.3   3rd Qu.:43808  \n#&gt;                        Max.   :2654.3   Max.   :73554\n\n# renomeando colunas\ncredito &lt;- credito %&gt;% \n                rename( inadimplente = default, estudante = student, balanco = balance,\n                receita = income)\ncredito &lt;- credito %&gt;% mutate( inadimplente =  case_when(\n                           inadimplente == \"No\"  ~ \"Nao\",\n                           inadimplente == \"Yes\" ~ \"Sim\"\n                          )) %&gt;% mutate(inadimplente = factor(inadimplente))\ncredito &lt;- credito %&gt;% mutate( estudante =  case_when(\n                           estudante == \"No\"  ~ 0,\n                           estudante == \"Yes\" ~ 1\n                          )) \n\nstr(credito)\n\n#&gt; tibble [10,000 × 4] (S3: tbl_df/tbl/data.frame)\n#&gt;  $ inadimplente: Factor w/ 2 levels \"Nao\",\"Sim\": 1 1 1 1 1 1 1 1 1 1 ...\n#&gt;  $ estudante   : num [1:10000] 0 1 0 0 0 1 0 1 0 0 ...\n#&gt;  $ balanco     : num [1:10000] 730 817 1074 529 786 ...\n#&gt;  $ receita     : num [1:10000] 44362 12106 31767 35704 38463 ...\n\nsummary(credito)\n\n#&gt;  inadimplente   estudante         balanco          receita     \n#&gt;  Nao:9667     Min.   :0.0000   Min.   :   0.0   Min.   :  772  \n#&gt;  Sim: 333     1st Qu.:0.0000   1st Qu.: 481.7   1st Qu.:21340  \n#&gt;               Median :0.0000   Median : 823.6   Median :34553  \n#&gt;               Mean   :0.2944   Mean   : 835.4   Mean   :33517  \n#&gt;               3rd Qu.:1.0000   3rd Qu.:1166.3   3rd Qu.:43808  \n#&gt;               Max.   :1.0000   Max.   :2654.3   Max.   :73554",
    "crumbs": [
      "Conteúdo Semanal",
      "LDA-QDA"
    ]
  },
  {
    "objectID": "semanas/Aula10.html#treino-e-teste",
    "href": "semanas/Aula10.html#treino-e-teste",
    "title": "LDA e QDA",
    "section": "Treino e Teste",
    "text": "Treino e Teste\n\nlibrary(caret)\n\n#&gt; Carregando pacotes exigidos: lattice\n\n\n#&gt; \n#&gt; Anexando pacote: 'caret'\n\n\n#&gt; O seguinte objeto é mascarado por 'package:purrr':\n#&gt; \n#&gt;     lift\n\nset.seed(2024)\ny &lt;- credito$inadimplente\nindice_teste &lt;- createDataPartition(y, times = 1, p = 0.2, list = FALSE)\n\nconj_treino &lt;- credito[-indice_teste,]\nconj_teste &lt;- credito[indice_teste,]\n\nsummary(conj_treino)\n\n#&gt;  inadimplente   estudante         balanco          receita     \n#&gt;  Nao:7733     Min.   :0.0000   Min.   :   0.0   Min.   :  772  \n#&gt;  Sim: 266     1st Qu.:0.0000   1st Qu.: 481.8   1st Qu.:21536  \n#&gt;               Median :0.0000   Median : 818.0   Median :34690  \n#&gt;               Mean   :0.2929   Mean   : 833.4   Mean   :33610  \n#&gt;               3rd Qu.:1.0000   3rd Qu.:1164.6   3rd Qu.:43850  \n#&gt;               Max.   :1.0000   Max.   :2654.3   Max.   :73554\n\nsummary(conj_teste)\n\n#&gt;  inadimplente   estudante         balanco          receita     \n#&gt;  Nao:1934     Min.   :0.0000   Min.   :   0.0   Min.   : 1498  \n#&gt;  Sim:  67     1st Qu.:0.0000   1st Qu.: 481.6   1st Qu.:20655  \n#&gt;               Median :0.0000   Median : 840.1   Median :33812  \n#&gt;               Mean   :0.3003   Mean   : 843.1   Mean   :33144  \n#&gt;               3rd Qu.:1.0000   3rd Qu.:1171.4   3rd Qu.:43593  \n#&gt;               Max.   :1.0000   Max.   :2578.5   Max.   :68722",
    "crumbs": [
      "Conteúdo Semanal",
      "LDA-QDA"
    ]
  },
  {
    "objectID": "semanas/Aula10.html#balanço-e-receita",
    "href": "semanas/Aula10.html#balanço-e-receita",
    "title": "LDA e QDA",
    "section": "Balanço e receita",
    "text": "Balanço e receita\n\nfeaturePlot(x = conj_treino[, c(\"balanco\", \"receita\", \"estudante\")], \n            y = conj_treino$inadimplente,\n            plot = \"density\", \n            scales = list(x = list(relation = \"free\"), \n                          y = list(relation = \"free\")), \n            adjust = 1.5, \n            pch = \"|\", \n            layout = c(2, 1), \n            auto.key = list(columns = 2))",
    "crumbs": [
      "Conteúdo Semanal",
      "LDA-QDA"
    ]
  },
  {
    "objectID": "semanas/Aula10.html#avaliando-o-comportamento-das-variáveis-em-função-do-status-inadimplente-estudante",
    "href": "semanas/Aula10.html#avaliando-o-comportamento-das-variáveis-em-função-do-status-inadimplente-estudante",
    "title": "LDA e QDA",
    "section": "Avaliando o comportamento das variáveis em função do status (inadimplente / estudante)",
    "text": "Avaliando o comportamento das variáveis em função do status (inadimplente / estudante)\n\nlibrary(patchwork)\np1 &lt;- ggplot(credito, aes(x=inadimplente, y=balanco, color=inadimplente)) +\n  geom_boxplot()\np2 &lt;- ggplot(credito, aes(x=inadimplente, y=receita, color=inadimplente)) +\n  geom_boxplot()\np3 &lt;- ggplot(credito, aes(x=as.factor(estudante), y=balanco, color=as.factor(estudante))) +\n  geom_boxplot()\np4 &lt;- ggplot(credito, aes(x=as.factor(estudante), y=receita, color=as.factor(estudante))) +\n  geom_boxplot()\n(p1 + p2) / (p3 + p4)",
    "crumbs": [
      "Conteúdo Semanal",
      "LDA-QDA"
    ]
  },
  {
    "objectID": "semanas/Aula10.html#calcula-erro",
    "href": "semanas/Aula10.html#calcula-erro",
    "title": "LDA e QDA",
    "section": "Calcula Erro",
    "text": "Calcula Erro\n\n# Este valor é igual a 1 - Accuracy da matriz de confusão\ncalc_erro_class &lt;- function(real, previsto) {\n  mean(real != previsto)\n}",
    "crumbs": [
      "Conteúdo Semanal",
      "LDA-QDA"
    ]
  },
  {
    "objectID": "semanas/Aula10.html#treino-e-teste-normalizado",
    "href": "semanas/Aula10.html#treino-e-teste-normalizado",
    "title": "LDA e QDA",
    "section": "Treino e Teste Normalizado",
    "text": "Treino e Teste Normalizado\n\nlibrary(caret)\nset.seed(2024)\n# Normalizando os dados\ncredito &lt;- credito %&gt;% mutate(balanco = scale(balanco), receita = scale(receita))\n\ny &lt;- credito$inadimplente\nindice_teste &lt;- createDataPartition(y, times = 1, p = 0.2, list = FALSE)\n\nconj_treino &lt;- credito[-indice_teste,]\nconj_teste &lt;- credito[indice_teste,]\n\nsummary(conj_treino)\n\n#&gt;  inadimplente   estudante              balanco.V1         \n#&gt;  Nao:7733     Min.   :0.0000   Min.   :-1.72699815214000  \n#&gt;  Sim: 266     1st Qu.:0.0000   1st Qu.:-0.73093769169700  \n#&gt;               Median :0.0000   Median :-0.03595597796600  \n#&gt;               Mean   :0.2929   Mean   :-0.00401639039487  \n#&gt;               3rd Qu.:1.0000   3rd Qu.: 0.68056023172100  \n#&gt;               Max.   :1.0000   Max.   : 3.76037076853000  \n#&gt;          receita.V1         \n#&gt;  Min.   :-2.45526723525000  \n#&gt;  1st Qu.:-0.89836473917200  \n#&gt;  Median : 0.08796491781240  \n#&gt;  Mean   : 0.00699812328583  \n#&gt;  3rd Qu.: 0.77481662870600  \n#&gt;  Max.   : 3.00204946164000\n\nsummary(conj_teste)\n\n#&gt;  inadimplente   estudante              balanco.V1         \n#&gt;  Nao:1934     Min.   :0.0000   Min.   :-1.72699815214000  \n#&gt;  Sim:  67     1st Qu.:0.0000   1st Qu.:-0.73139456297700  \n#&gt;               Median :0.0000   Median : 0.00984385369764  \n#&gt;               Mean   :0.3003   Mean   : 0.01605552562150  \n#&gt;               3rd Qu.:1.0000   3rd Qu.: 0.69476143173600  \n#&gt;               Max.   :1.0000   Max.   : 3.60355620415000  \n#&gt;          receita.V1        \n#&gt;  Min.   :-2.4008112726600  \n#&gt;  1st Qu.:-0.9643945022460  \n#&gt;  Median : 0.0220960712686  \n#&gt;  Mean   :-0.0279750065784  \n#&gt;  3rd Qu.: 0.7555020572170  \n#&gt;  Max.   : 2.6397198805100",
    "crumbs": [
      "Conteúdo Semanal",
      "LDA-QDA"
    ]
  },
  {
    "objectID": "semanas/Aula10.html#lda",
    "href": "semanas/Aula10.html#lda",
    "title": "LDA e QDA",
    "section": "LDA",
    "text": "LDA\n\nlibrary(MASS)\n\n#&gt; \n#&gt; Anexando pacote: 'MASS'\n\n\n#&gt; O seguinte objeto é mascarado por 'package:patchwork':\n#&gt; \n#&gt;     area\n\n\n#&gt; O seguinte objeto é mascarado por 'package:dplyr':\n#&gt; \n#&gt;     select\n\ntreina_lda &lt;- lda(inadimplente ~ balanco + estudante + receita, data = conj_treino)\ntreina_lda\n\n#&gt; Call:\n#&gt; lda(inadimplente ~ balanco + estudante + receita, data = conj_treino)\n#&gt; \n#&gt; Prior probabilities of groups:\n#&gt;        Nao        Sim \n#&gt; 0.96674584 0.03325416 \n#&gt; \n#&gt; Group means:\n#&gt;         balanco estudante      receita\n#&gt; Nao -0.06884968 0.2903142  0.008870876\n#&gt; Sim  1.88077996 0.3684211 -0.047445476\n#&gt; \n#&gt; Coefficients of linear discriminants:\n#&gt;                   LD1\n#&gt; balanco    1.09199528\n#&gt; estudante -0.14668654\n#&gt; receita    0.07531296\n\nplot(treina_lda)\n\n\n\n\n\n\nnames(predict(treina_lda, conj_treino))\n\n#&gt; [1] \"class\"     \"posterior\" \"x\"\n\ny_chapeu &lt;- predict(treina_lda, conj_teste)$class %&gt;% \n             factor(levels = levels(conj_teste$inadimplente))\nconfusionMatrix(data = y_chapeu, reference = conj_teste$inadimplente,  positive=\"Sim\")\n\n#&gt; Confusion Matrix and Statistics\n#&gt; \n#&gt;           Reference\n#&gt; Prediction  Nao  Sim\n#&gt;        Nao 1926   48\n#&gt;        Sim    8   19\n#&gt;                                           \n#&gt;                Accuracy : 0.972           \n#&gt;                  95% CI : (0.9638, 0.9788)\n#&gt;     No Information Rate : 0.9665          \n#&gt;     P-Value [Acc &gt; NIR] : 0.09339         \n#&gt;                                           \n#&gt;                   Kappa : 0.3926          \n#&gt;                                           \n#&gt;  Mcnemar's Test P-Value : 1.872e-07       \n#&gt;                                           \n#&gt;             Sensitivity : 0.283582        \n#&gt;             Specificity : 0.995863        \n#&gt;          Pos Pred Value : 0.703704        \n#&gt;          Neg Pred Value : 0.975684        \n#&gt;              Prevalence : 0.033483        \n#&gt;          Detection Rate : 0.009495        \n#&gt;    Detection Prevalence : 0.013493        \n#&gt;       Balanced Accuracy : 0.639723        \n#&gt;                                           \n#&gt;        'Positive' Class : Sim             \n#&gt; \n\n# Este valor é igual a 1 - Accuracy da matriz de confusão\ncalc_erro_class(conj_teste$inadimplente, y_chapeu)\n\n#&gt; [1] 0.02798601",
    "crumbs": [
      "Conteúdo Semanal",
      "LDA-QDA"
    ]
  },
  {
    "objectID": "semanas/Aula10.html#lda---ajustando-probabilidade-limite",
    "href": "semanas/Aula10.html#lda---ajustando-probabilidade-limite",
    "title": "LDA e QDA",
    "section": "LDA - Ajustando probabilidade limite",
    "text": "LDA - Ajustando probabilidade limite\n\np_chapeu &lt;- predict(treina_lda, conj_teste)$posterior\nhead(p_chapeu)\n\n#&gt;         Nao          Sim\n#&gt; 1 0.9857954 0.0142045675\n#&gt; 2 0.9981836 0.0018164269\n#&gt; 3 0.9180153 0.0819846555\n#&gt; 4 0.9988081 0.0011919412\n#&gt; 5 0.9519939 0.0480061272\n#&gt; 6 0.9998932 0.0001067956\n\ny_chapeu &lt;- ifelse(p_chapeu[, 2] &gt; 0.11, \"Sim\", \"Nao\") %&gt;% \n             factor(levels = levels(conj_teste$inadimplente))\nconfusionMatrix(data = y_chapeu, reference = conj_teste$inadimplente,  positive=\"Sim\") \n\n#&gt; Confusion Matrix and Statistics\n#&gt; \n#&gt;           Reference\n#&gt; Prediction  Nao  Sim\n#&gt;        Nao 1815   19\n#&gt;        Sim  119   48\n#&gt;                                          \n#&gt;                Accuracy : 0.931          \n#&gt;                  95% CI : (0.919, 0.9417)\n#&gt;     No Information Rate : 0.9665         \n#&gt;     P-Value [Acc &gt; NIR] : 1              \n#&gt;                                          \n#&gt;                   Kappa : 0.3807         \n#&gt;                                          \n#&gt;  Mcnemar's Test P-Value : &lt;2e-16         \n#&gt;                                          \n#&gt;             Sensitivity : 0.71642        \n#&gt;             Specificity : 0.93847        \n#&gt;          Pos Pred Value : 0.28743        \n#&gt;          Neg Pred Value : 0.98964        \n#&gt;              Prevalence : 0.03348        \n#&gt;          Detection Rate : 0.02399        \n#&gt;    Detection Prevalence : 0.08346        \n#&gt;       Balanced Accuracy : 0.82744        \n#&gt;                                          \n#&gt;        'Positive' Class : Sim            \n#&gt; \n\n# Este valor é igual a 1 - Accuracy da matriz de confusão\ncalc_erro_class(conj_teste$inadimplente, y_chapeu)\n\n#&gt; [1] 0.06896552",
    "crumbs": [
      "Conteúdo Semanal",
      "LDA-QDA"
    ]
  },
  {
    "objectID": "semanas/Aula10.html#seleção-de-variáveis",
    "href": "semanas/Aula10.html#seleção-de-variáveis",
    "title": "LDA e QDA",
    "section": "Seleção de variáveis",
    "text": "Seleção de variáveis\nNo LDA, a seleção de variáveis pode ser feita com o RFE (Recursive Feature Elimination). O RFE é um método de seleção de variáveis que utiliza a validação cruzada para avaliar o desempenho do modelo com diferentes subconjuntos de variáveis. O RFE é implementado na função rfe() do pacote caret.\n\n# Usar o RFE para selecionar as variáveis\n# Definir controle para RFE\ncontrol &lt;- rfeControl(functions = ldaFuncs, method = \"cv\", number = 10)\n\n# Aplicar o RFE\nset.seed(2024)\nresult &lt;- rfe(conj_treino[, 2:4], conj_treino$inadimplente, sizes = c(1:3), rfeControl = control)\n\n# Resultados\nprint(result)\n\n#&gt; \n#&gt; Recursive feature selection\n#&gt; \n#&gt; Outer resampling method: Cross-Validated (10 fold) \n#&gt; \n#&gt; Resampling performance over subset size:\n#&gt; \n#&gt;  Variables Accuracy  Kappa AccuracySD KappaSD Selected\n#&gt;          1   0.9724 0.3451   0.003761 0.12676         \n#&gt;          2   0.9729 0.3562   0.003398 0.10548         \n#&gt;          3   0.9735 0.3804   0.002942 0.09046        *\n#&gt; \n#&gt; The top 3 variables (out of 3):\n#&gt;    balanco, estudante, receita",
    "crumbs": [
      "Conteúdo Semanal",
      "LDA-QDA"
    ]
  },
  {
    "objectID": "semanas/Aula10.html#outra-opção",
    "href": "semanas/Aula10.html#outra-opção",
    "title": "LDA e QDA",
    "section": "Outra Opção",
    "text": "Outra Opção\nPodemos usar os gráfico exploratórios iniciais e também o resultado da regressão logística como ponto de partida para a seleção de variáveis.\n\ntreina_lda2 &lt;- lda(inadimplente ~ balanco + estudante, data = conj_treino)\ntreina_lda2\n\n#&gt; Call:\n#&gt; lda(inadimplente ~ balanco + estudante, data = conj_treino)\n#&gt; \n#&gt; Prior probabilities of groups:\n#&gt;        Nao        Sim \n#&gt; 0.96674584 0.03325416 \n#&gt; \n#&gt; Group means:\n#&gt;         balanco estudante\n#&gt; Nao -0.06884968 0.2903142\n#&gt; Sim  1.88077996 0.3684211\n#&gt; \n#&gt; Coefficients of linear discriminants:\n#&gt;                  LD1\n#&gt; balanco    1.0935003\n#&gt; estudante -0.2716957\n\nplot(treina_lda2)\n\n\n\n\n\n\ny_chapeu &lt;- predict(treina_lda2, conj_teste)$class %&gt;% \n             factor(levels = levels(conj_teste$inadimplente))\nconfusionMatrix(data = y_chapeu, reference = conj_teste$inadimplente,  positive=\"Sim\")\n\n#&gt; Confusion Matrix and Statistics\n#&gt; \n#&gt;           Reference\n#&gt; Prediction  Nao  Sim\n#&gt;        Nao 1926   48\n#&gt;        Sim    8   19\n#&gt;                                           \n#&gt;                Accuracy : 0.972           \n#&gt;                  95% CI : (0.9638, 0.9788)\n#&gt;     No Information Rate : 0.9665          \n#&gt;     P-Value [Acc &gt; NIR] : 0.09339         \n#&gt;                                           \n#&gt;                   Kappa : 0.3926          \n#&gt;                                           \n#&gt;  Mcnemar's Test P-Value : 1.872e-07       \n#&gt;                                           \n#&gt;             Sensitivity : 0.283582        \n#&gt;             Specificity : 0.995863        \n#&gt;          Pos Pred Value : 0.703704        \n#&gt;          Neg Pred Value : 0.975684        \n#&gt;              Prevalence : 0.033483        \n#&gt;          Detection Rate : 0.009495        \n#&gt;    Detection Prevalence : 0.013493        \n#&gt;       Balanced Accuracy : 0.639723        \n#&gt;                                           \n#&gt;        'Positive' Class : Sim             \n#&gt; \n\n# Este valor é igual a 1 - Accuracy da matriz de confusão\ncalc_erro_class(conj_teste$inadimplente, y_chapeu)\n\n#&gt; [1] 0.02798601\n\n\nPodemos observar que não houve mudança nos resultados ao retirar a variável receita.",
    "crumbs": [
      "Conteúdo Semanal",
      "LDA-QDA"
    ]
  },
  {
    "objectID": "semanas/Aula10.html#qda",
    "href": "semanas/Aula10.html#qda",
    "title": "LDA e QDA",
    "section": "QDA",
    "text": "QDA\n\ntreina_qda &lt;- qda(inadimplente ~ balanco + estudante + receita, data = conj_treino)\ntreina_qda\n\n#&gt; Call:\n#&gt; qda(inadimplente ~ balanco + estudante + receita, data = conj_treino)\n#&gt; \n#&gt; Prior probabilities of groups:\n#&gt;        Nao        Sim \n#&gt; 0.96674584 0.03325416 \n#&gt; \n#&gt; Group means:\n#&gt;         balanco estudante      receita\n#&gt; Nao -0.06884968 0.2903142  0.008870876\n#&gt; Sim  1.88077996 0.3684211 -0.047445476\n\ny_chapeu &lt;- predict(treina_qda, conj_teste)$class %&gt;% \n             factor(levels = levels(conj_teste$inadimplente))\nconfusionMatrix(data = y_chapeu, reference = conj_teste$inadimplente,  positive=\"Sim\") \n\n#&gt; Confusion Matrix and Statistics\n#&gt; \n#&gt;           Reference\n#&gt; Prediction  Nao  Sim\n#&gt;        Nao 1922   47\n#&gt;        Sim   12   20\n#&gt;                                           \n#&gt;                Accuracy : 0.9705          \n#&gt;                  95% CI : (0.9621, 0.9775)\n#&gt;     No Information Rate : 0.9665          \n#&gt;     P-Value [Acc &gt; NIR] : 0.1762          \n#&gt;                                           \n#&gt;                   Kappa : 0.3909          \n#&gt;                                           \n#&gt;  Mcnemar's Test P-Value : 9.581e-06       \n#&gt;                                           \n#&gt;             Sensitivity : 0.298507        \n#&gt;             Specificity : 0.993795        \n#&gt;          Pos Pred Value : 0.625000        \n#&gt;          Neg Pred Value : 0.976130        \n#&gt;              Prevalence : 0.033483        \n#&gt;          Detection Rate : 0.009995        \n#&gt;    Detection Prevalence : 0.015992        \n#&gt;       Balanced Accuracy : 0.646151        \n#&gt;                                           \n#&gt;        'Positive' Class : Sim             \n#&gt;",
    "crumbs": [
      "Conteúdo Semanal",
      "LDA-QDA"
    ]
  },
  {
    "objectID": "semanas/Aula10.html#qda---ajustando-probabilidade-limite",
    "href": "semanas/Aula10.html#qda---ajustando-probabilidade-limite",
    "title": "LDA e QDA",
    "section": "QDA - Ajustando probabilidade limite",
    "text": "QDA - Ajustando probabilidade limite\n\np_chapeu &lt;- predict(treina_qda, conj_teste)$posterior\nhead(p_chapeu)\n\n#&gt;         Nao          Sim\n#&gt; 1 0.9904625 9.537473e-03\n#&gt; 2 0.9997798 2.202256e-04\n#&gt; 3 0.9062854 9.371459e-02\n#&gt; 4 0.9999365 6.346083e-05\n#&gt; 5 0.9526017 4.739833e-02\n#&gt; 6 0.9999996 3.588934e-07\n\ny_chapeu &lt;- ifelse(p_chapeu[, 2] &gt; 0.11, \"Sim\", \"Nao\") %&gt;% \n             factor(levels = levels(conj_teste$inadimplente))\nconfusionMatrix(data = y_chapeu, reference = conj_teste$inadimplente,  positive=\"Sim\") \n\n#&gt; Confusion Matrix and Statistics\n#&gt; \n#&gt;           Reference\n#&gt; Prediction  Nao  Sim\n#&gt;        Nao 1803   17\n#&gt;        Sim  131   50\n#&gt;                                           \n#&gt;                Accuracy : 0.926           \n#&gt;                  95% CI : (0.9137, 0.9371)\n#&gt;     No Information Rate : 0.9665          \n#&gt;     P-Value [Acc &gt; NIR] : 1               \n#&gt;                                           \n#&gt;                   Kappa : 0.3726          \n#&gt;                                           \n#&gt;  Mcnemar's Test P-Value : &lt;2e-16          \n#&gt;                                           \n#&gt;             Sensitivity : 0.74627         \n#&gt;             Specificity : 0.93226         \n#&gt;          Pos Pred Value : 0.27624         \n#&gt;          Neg Pred Value : 0.99066         \n#&gt;              Prevalence : 0.03348         \n#&gt;          Detection Rate : 0.02499         \n#&gt;    Detection Prevalence : 0.09045         \n#&gt;       Balanced Accuracy : 0.83927         \n#&gt;                                           \n#&gt;        'Positive' Class : Sim             \n#&gt;",
    "crumbs": [
      "Conteúdo Semanal",
      "LDA-QDA"
    ]
  },
  {
    "objectID": "semanas/Aula10.html#curva-roc",
    "href": "semanas/Aula10.html#curva-roc",
    "title": "LDA e QDA",
    "section": "Curva ROC",
    "text": "Curva ROC\n\nlibrary(pROC)\n\n# KNN\nset.seed(21)\nctrl &lt;- trainControl(method = \"cv\")\ntreina_knn &lt;- train(inadimplente ~ balanco + estudante, method = \"knn\", trControl= ctrl, preProcess=c(\"center\", \"scale\"), tuneGrid = data.frame(k = seq(21,140, by=4)), data = conj_treino)\nprev_knn &lt;- predict(treina_knn, conj_teste,type = \"prob\")\n\n# Reg Log\nmod2 &lt;- glm(inadimplente ~ balanco + estudante,data=conj_treino,family=binomial)\np_chapeu_log &lt;- predict(mod2, newdata = conj_teste, type = \"response\")\n\n# LDA e QDA\np_chapeu_lda &lt;- predict(treina_lda, conj_teste)$posterior\np_chapeu_qda &lt;- predict(treina_qda, conj_teste)$posterior\n\nroc_log &lt;- roc(conj_teste$inadimplente ~ p_chapeu_log, plot = TRUE, print.auc=FALSE,\n                 col=\"black\", legacy.axes=TRUE)\nroc_lda &lt;- roc(conj_teste$inadimplente ~ p_chapeu_lda[,2], plot = TRUE, print.auc=FALSE, col=\"green\", legacy.axes=TRUE, add=TRUE)\nroc_qda &lt;- roc(conj_teste$inadimplente ~ p_chapeu_qda[,2], plot = TRUE, print.auc=FALSE, col=\"blue\", legacy.axes=TRUE, add=TRUE)\nroc_knn1 &lt;- roc(conj_teste$inadimplente ~ prev_knn[,2], plot = TRUE, print.auc=FALSE, col=\"red\", legacy.axes=TRUE, add=TRUE)\n\nlegend(\"bottomright\",legend=c(\"Reg Log\",\"LDA\",\"QDA\", \"KNN\"), \n       col=c(\"black\", \"green\",\"blue\", \"red\"),lwd=4)\n\n\n\n\n\n\nas.numeric(roc_log$auc)\n\n#&gt; [1] 0.9505009\n\nas.numeric(roc_lda$auc)\n\n#&gt; [1] 0.9498063\n\nas.numeric(roc_qda$auc)\n\n#&gt; [1] 0.9497909\n\nas.numeric(roc_knn1$auc)\n\n#&gt; [1] 0.9423397",
    "crumbs": [
      "Conteúdo Semanal",
      "LDA-QDA"
    ]
  },
  {
    "objectID": "semanas/Aula11.2A.html",
    "href": "semanas/Aula11.2A.html",
    "title": "Arvores de Regressão - Random Forest",
    "section": "",
    "text": "library(MASS)\nlibrary(tidyverse)\nlibrary(ranger) # Random Forest",
    "crumbs": [
      "Conteúdo Semanal",
      "Arvore de Regressão (Randon Forest)"
    ]
  },
  {
    "objectID": "semanas/Aula11.2A.html#bibliotecas",
    "href": "semanas/Aula11.2A.html#bibliotecas",
    "title": "Arvores de Regressão - Random Forest",
    "section": "",
    "text": "library(MASS)\nlibrary(tidyverse)\nlibrary(ranger) # Random Forest",
    "crumbs": [
      "Conteúdo Semanal",
      "Arvore de Regressão (Randon Forest)"
    ]
  },
  {
    "objectID": "semanas/Aula11.2A.html#avaliando-selecionando-dados",
    "href": "semanas/Aula11.2A.html#avaliando-selecionando-dados",
    "title": "Arvores de Regressão - Random Forest",
    "section": "Avaliando, selecionando dados",
    "text": "Avaliando, selecionando dados\n\ndata(\"Boston\")\nnames(Boston)\n\n [1] \"crim\"    \"zn\"      \"indus\"   \"chas\"    \"nox\"     \"rm\"      \"age\"    \n [8] \"dis\"     \"rad\"     \"tax\"     \"ptratio\" \"black\"   \"lstat\"   \"medv\"   \n\ndados &lt;- Boston",
    "crumbs": [
      "Conteúdo Semanal",
      "Arvore de Regressão (Randon Forest)"
    ]
  },
  {
    "objectID": "semanas/Aula11.2A.html#treino-e-teste-com-todas-as-variáveis",
    "href": "semanas/Aula11.2A.html#treino-e-teste-com-todas-as-variáveis",
    "title": "Arvores de Regressão - Random Forest",
    "section": "Treino e Teste com todas as variáveis",
    "text": "Treino e Teste com todas as variáveis\n\n## Vamos criar os conjuntos de treino teste e desenvolver a arvore \n## com todas as variáveis.\nlibrary(caret)\nset.seed(21)\nindice &lt;- createDataPartition(dados$medv, times=1, p=0.75, list=FALSE)\nconj_treino &lt;- dados[indice,]\nconj_teste &lt;- dados[-indice,]\nhead(conj_treino)\n\n     crim   zn indus chas   nox    rm  age    dis rad tax ptratio  black lstat\n1 0.00632 18.0  2.31    0 0.538 6.575 65.2 4.0900   1 296    15.3 396.90  4.98\n3 0.02729  0.0  7.07    0 0.469 7.185 61.1 4.9671   2 242    17.8 392.83  4.03\n4 0.03237  0.0  2.18    0 0.458 6.998 45.8 6.0622   3 222    18.7 394.63  2.94\n5 0.06905  0.0  2.18    0 0.458 7.147 54.2 6.0622   3 222    18.7 396.90  5.33\n6 0.02985  0.0  2.18    0 0.458 6.430 58.7 6.0622   3 222    18.7 394.12  5.21\n7 0.08829 12.5  7.87    0 0.524 6.012 66.6 5.5605   5 311    15.2 395.60 12.43\n  medv\n1 24.0\n3 34.7\n4 33.4\n5 36.2\n6 28.7\n7 22.9\n\nhead(conj_teste)\n\n      crim   zn indus chas   nox    rm  age    dis rad tax ptratio  black lstat\n2  0.02731  0.0  7.07    0 0.469 6.421 78.9 4.9671   2 242    17.8 396.90  9.14\n10 0.17004 12.5  7.87    0 0.524 6.004 85.9 6.5921   5 311    15.2 386.71 17.10\n12 0.11747 12.5  7.87    0 0.524 6.009 82.9 6.2267   5 311    15.2 396.90 13.27\n16 0.62739  0.0  8.14    0 0.538 5.834 56.5 4.4986   4 307    21.0 395.62  8.47\n19 0.80271  0.0  8.14    0 0.538 5.456 36.6 3.7965   4 307    21.0 288.99 11.69\n23 1.23247  0.0  8.14    0 0.538 6.142 91.7 3.9769   4 307    21.0 396.90 18.72\n   medv\n2  21.6\n10 18.9\n12 18.9\n16 19.9\n19 20.2\n23 15.2",
    "crumbs": [
      "Conteúdo Semanal",
      "Arvore de Regressão (Randon Forest)"
    ]
  },
  {
    "objectID": "semanas/Aula11.2A.html#ajuste-do-modelo-random-forest-com-caret",
    "href": "semanas/Aula11.2A.html#ajuste-do-modelo-random-forest-com-caret",
    "title": "Arvores de Regressão - Random Forest",
    "section": "Ajuste do modelo Random Forest com caret\n",
    "text": "Ajuste do modelo Random Forest com caret\n\n\nctrl &lt;- trainControl(method = \"cv\", number = 5)\n\nmodel_rf &lt;- train(\n  medv ~ ., data = conj_treino,\n  method = \"ranger\", # Usando o pacote ranger para Random Forest\ntrControl = ctrl,\n  tuneLength = 5,\n  importance = 'impurity'\n)",
    "crumbs": [
      "Conteúdo Semanal",
      "Arvore de Regressão (Randon Forest)"
    ]
  },
  {
    "objectID": "semanas/Aula11.2A.html#resultados-do-modelo",
    "href": "semanas/Aula11.2A.html#resultados-do-modelo",
    "title": "Arvores de Regressão - Random Forest",
    "section": "Resultados do modelo",
    "text": "Resultados do modelo\n\nprint(model_rf)\n\nRandom Forest \n\n381 samples\n 13 predictor\n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 304, 303, 306, 305, 306 \nResampling results across tuning parameters:\n\n  mtry  splitrule   RMSE      Rsquared   MAE     \n   2    variance    3.738441  0.8478051  2.478749\n   2    extratrees  4.086867  0.8238616  2.699963\n   4    variance    3.407047  0.8674394  2.316360\n   4    extratrees  3.557420  0.8607147  2.350807\n   7    variance    3.426677  0.8607768  2.332572\n   7    extratrees  3.405111  0.8687576  2.282396\n  10    variance    3.490597  0.8541012  2.369265\n  10    extratrees  3.394757  0.8661821  2.263713\n  13    variance    3.537076  0.8489625  2.381604\n  13    extratrees  3.439379  0.8611419  2.305458\n\nTuning parameter 'min.node.size' was held constant at a value of 5\nRMSE was used to select the optimal model using the smallest value.\nThe final values used for the model were mtry = 10, splitrule = extratrees\n and min.node.size = 5.\n\nplot(model_rf)",
    "crumbs": [
      "Conteúdo Semanal",
      "Arvore de Regressão (Randon Forest)"
    ]
  },
  {
    "objectID": "semanas/Aula11.2A.html#avaliação-no-conjunto-de-teste",
    "href": "semanas/Aula11.2A.html#avaliação-no-conjunto-de-teste",
    "title": "Arvores de Regressão - Random Forest",
    "section": "Avaliação no conjunto de teste",
    "text": "Avaliação no conjunto de teste\n\npred &lt;- predict(model_rf, newdata = conj_teste)\n\n# Métricas de desempenho\npostResample(pred, conj_teste$medv)\n\n     RMSE  Rsquared       MAE \n4.0789597 0.8276481 2.3000856",
    "crumbs": [
      "Conteúdo Semanal",
      "Arvore de Regressão (Randon Forest)"
    ]
  },
  {
    "objectID": "semanas/Aula11.2A.html#importância-das-variáveis",
    "href": "semanas/Aula11.2A.html#importância-das-variáveis",
    "title": "Arvores de Regressão - Random Forest",
    "section": "Importância das variáveis",
    "text": "Importância das variáveis\n\nvarimp &lt;- varImp(model_rf)$importance\nvarimp$Variable &lt;- rownames(varimp)\n\n# Gráfico de importância\nvarimp_plot &lt;- ggplot(varimp, aes(x = reorder(Variable, Overall), y = Overall)) +\n  geom_col(fill = \"steelblue\") +\n  coord_flip() +\n  labs(title = \"Importância das Variáveis - Random Forest (ranger)\",\n       x = \"Variável\", y = \"Importância\") +\n  theme_minimal()\n\nvarimp_plot",
    "crumbs": [
      "Conteúdo Semanal",
      "Arvore de Regressão (Randon Forest)"
    ]
  },
  {
    "objectID": "semanas/Aula11.2A.html#comparação-com-outro-modelo-regressão-linear",
    "href": "semanas/Aula11.2A.html#comparação-com-outro-modelo-regressão-linear",
    "title": "Arvores de Regressão - Random Forest",
    "section": "Comparação com outro modelo (Regressão Linear)",
    "text": "Comparação com outro modelo (Regressão Linear)\n\nmodel_lm &lt;- train(\n  medv ~ ., data = conj_treino,\n  method = \"lm\",\n  trControl = ctrl\n)\n\npred_lm &lt;- predict(model_lm, newdata = conj_teste)\npostResample(pred_lm, conj_teste$medv)\n\n     RMSE  Rsquared       MAE \n5.9426142 0.6315418 3.9610503",
    "crumbs": [
      "Conteúdo Semanal",
      "Arvore de Regressão (Randon Forest)"
    ]
  },
  {
    "objectID": "semanas/Aula11.2A.html#grafico-de-comparação",
    "href": "semanas/Aula11.2A.html#grafico-de-comparação",
    "title": "Arvores de Regressão - Random Forest",
    "section": "Grafico de comparação",
    "text": "Grafico de comparação\n\n# Gráfico de comparação\ncomparison_plot &lt;- ggplot() +\n  geom_point(aes(x = conj_teste$medv, y = pred), color = \"blue\", alpha = 0.5) +\n  geom_point(aes(x = conj_teste$medv, y = pred_lm), color = \"red\", alpha = 0.5) +\n  labs(title = \"Comparação de Previsões: Random Forest vs Regressão Linear\",\n       x = \"Valores Reais (medv)\", y = \"Previsões\") +\n  theme_minimal()\ncomparison_plot",
    "crumbs": [
      "Conteúdo Semanal",
      "Arvore de Regressão (Randon Forest)"
    ]
  },
  {
    "objectID": "semanas/Aula11.3.html",
    "href": "semanas/Aula11.3.html",
    "title": "Arvores de Regressão - XGBoost",
    "section": "",
    "text": "library(MASS)\nlibrary(tidyverse)",
    "crumbs": [
      "Conteúdo Semanal",
      "Arvore de Regressão (XGBoost)"
    ]
  },
  {
    "objectID": "semanas/Aula11.3.html#bibliotecas",
    "href": "semanas/Aula11.3.html#bibliotecas",
    "title": "Arvores de Regressão - XGBoost",
    "section": "",
    "text": "library(MASS)\nlibrary(tidyverse)",
    "crumbs": [
      "Conteúdo Semanal",
      "Arvore de Regressão (XGBoost)"
    ]
  },
  {
    "objectID": "semanas/Aula11.3.html#avaliando-selecionando-dados",
    "href": "semanas/Aula11.3.html#avaliando-selecionando-dados",
    "title": "Arvores de Regressão - XGBoost",
    "section": "Avaliando, selecionando dados",
    "text": "Avaliando, selecionando dados\n\ndata(\"Boston\")\nnames(Boston)\n\n [1] \"crim\"    \"zn\"      \"indus\"   \"chas\"    \"nox\"     \"rm\"      \"age\"    \n [8] \"dis\"     \"rad\"     \"tax\"     \"ptratio\" \"black\"   \"lstat\"   \"medv\"   \n\ndados &lt;- Boston",
    "crumbs": [
      "Conteúdo Semanal",
      "Arvore de Regressão (XGBoost)"
    ]
  },
  {
    "objectID": "semanas/Aula11.3.html#treino-e-teste-com-todas-as-variáveis",
    "href": "semanas/Aula11.3.html#treino-e-teste-com-todas-as-variáveis",
    "title": "Arvores de Regressão - XGBoost",
    "section": "Treino e Teste com todas as variáveis",
    "text": "Treino e Teste com todas as variáveis\n\n## Vamos criar os conjuntos de treino teste e desenvolver a arvore \n## com todas as variáveis.\nlibrary(caret)\nset.seed(21)\nindice &lt;- createDataPartition(dados$medv, times=1, p=0.75, list=FALSE)\nconj_treino &lt;- dados[indice,]\nconj_teste &lt;- dados[-indice,] \nstr(conj_treino)\n\n'data.frame':   381 obs. of  14 variables:\n $ crim   : num  0.00632 0.02729 0.03237 0.06905 0.02985 ...\n $ zn     : num  18 0 0 0 0 12.5 12.5 12.5 12.5 12.5 ...\n $ indus  : num  2.31 7.07 2.18 2.18 2.18 7.87 7.87 7.87 7.87 7.87 ...\n $ chas   : int  0 0 0 0 0 0 0 0 0 0 ...\n $ nox    : num  0.538 0.469 0.458 0.458 0.458 0.524 0.524 0.524 0.524 0.524 ...\n $ rm     : num  6.58 7.18 7 7.15 6.43 ...\n $ age    : num  65.2 61.1 45.8 54.2 58.7 66.6 96.1 100 94.3 39 ...\n $ dis    : num  4.09 4.97 6.06 6.06 6.06 ...\n $ rad    : int  1 2 3 3 3 5 5 5 5 5 ...\n $ tax    : num  296 242 222 222 222 311 311 311 311 311 ...\n $ ptratio: num  15.3 17.8 18.7 18.7 18.7 15.2 15.2 15.2 15.2 15.2 ...\n $ black  : num  397 393 395 397 394 ...\n $ lstat  : num  4.98 4.03 2.94 5.33 5.21 ...\n $ medv   : num  24 34.7 33.4 36.2 28.7 22.9 27.1 16.5 15 21.7 ...\n\nstr(conj_teste)\n\n'data.frame':   125 obs. of  14 variables:\n $ crim   : num  0.0273 0.17 0.1175 0.6274 0.8027 ...\n $ zn     : num  0 12.5 12.5 0 0 0 0 0 0 75 ...\n $ indus  : num  7.07 7.87 7.87 8.14 8.14 8.14 8.14 5.96 5.96 2.95 ...\n $ chas   : int  0 0 0 0 0 0 0 0 0 0 ...\n $ nox    : num  0.469 0.524 0.524 0.538 0.538 0.538 0.538 0.499 0.499 0.428 ...\n $ rm     : num  6.42 6 6.01 5.83 5.46 ...\n $ age    : num  78.9 85.9 82.9 56.5 36.6 91.7 82 61.4 30.2 21.8 ...\n $ dis    : num  4.97 6.59 6.23 4.5 3.8 ...\n $ rad    : int  2 5 5 4 4 4 4 5 5 3 ...\n $ tax    : num  242 311 311 307 307 307 307 279 279 252 ...\n $ ptratio: num  17.8 15.2 15.2 21 21 21 21 19.2 19.2 18.3 ...\n $ black  : num  397 387 397 396 289 ...\n $ lstat  : num  9.14 17.1 13.27 8.47 11.69 ...\n $ medv   : num  21.6 18.9 18.9 19.9 20.2 15.2 13.2 20 24.7 30.8 ...",
    "crumbs": [
      "Conteúdo Semanal",
      "Arvore de Regressão (XGBoost)"
    ]
  },
  {
    "objectID": "semanas/Aula11.3.html#preparando-os-dados",
    "href": "semanas/Aula11.3.html#preparando-os-dados",
    "title": "Arvores de Regressão - XGBoost",
    "section": "Preparando os dados",
    "text": "Preparando os dados\n\nx_treino &lt;- model.matrix(medv ~ . , data = conj_treino)[, -1]\ny_treino &lt;- conj_treino$medv\n\nx_teste &lt;- model.matrix(medv ~ . , data = conj_teste)[, -1]\ny_teste = conj_teste$medv",
    "crumbs": [
      "Conteúdo Semanal",
      "Arvore de Regressão (XGBoost)"
    ]
  },
  {
    "objectID": "semanas/Aula11.3.html#a-tentativa-xgboost",
    "href": "semanas/Aula11.3.html#a-tentativa-xgboost",
    "title": "Arvores de Regressão - XGBoost",
    "section": "1a tentativa Xgboost",
    "text": "1a tentativa Xgboost\n\nlibrary(xgboost)\nset.seed(21)\ncv &lt;- xgb.cv(data = as.matrix(x_treino), label = as.matrix(y_treino),\n             objective = \"reg:squarederror\", nrounds = 100, nfold = 5, eta = 0.3, max_depth = 6,\n             verbose = FALSE)\n# cv\nelog &lt;- as.data.frame(cv$evaluation_log)\nelog %&gt;% \n   summarize(ntrees.train = which.min(train_rmse_mean),   # find the index of min(train_rmse_mean)\n             ntrees.test  = which.min(test_rmse_mean))   # find the index of min(test_rmse_mean)\n\n  ntrees.train ntrees.test\n1          100          39\n\n(nrounds &lt;- which.min(elog$test_rmse_mean))\n\n[1] 39",
    "crumbs": [
      "Conteúdo Semanal",
      "Arvore de Regressão (XGBoost)"
    ]
  },
  {
    "objectID": "semanas/Aula11.3.html#modelo-final",
    "href": "semanas/Aula11.3.html#modelo-final",
    "title": "Arvores de Regressão - XGBoost",
    "section": "Modelo Final",
    "text": "Modelo Final\n\n modelo_xgb &lt;- xgboost(data = as.matrix(x_treino), label = as.matrix(y_treino),\n             objective = \"reg:squarederror\", nrounds = nrounds, eta = 0.3, max_depth = 6,\n             verbose = FALSE)",
    "crumbs": [
      "Conteúdo Semanal",
      "Arvore de Regressão (XGBoost)"
    ]
  },
  {
    "objectID": "semanas/Aula11.3.html#previsões",
    "href": "semanas/Aula11.3.html#previsões",
    "title": "Arvores de Regressão - XGBoost",
    "section": "Previsões",
    "text": "Previsões\n\nconj_teste$prev &lt;- predict(modelo_xgb, as.matrix(x_teste))\n\n\nggplot(conj_teste, aes(x = prev, y = medv)) + \n  geom_point() + \n  geom_abline()",
    "crumbs": [
      "Conteúdo Semanal",
      "Arvore de Regressão (XGBoost)"
    ]
  },
  {
    "objectID": "semanas/Aula11.3.html#calculando-o-rmse",
    "href": "semanas/Aula11.3.html#calculando-o-rmse",
    "title": "Arvores de Regressão - XGBoost",
    "section": "Calculando o RMSE",
    "text": "Calculando o RMSE\n\nconj_teste %&gt;%\n  mutate(residuos = medv - prev) %&gt;%\n  summarize(rmse = sqrt(mean(residuos^2)))\n\n      rmse\n1 3.874079\n\ncaret::postResample(conj_teste$prev, conj_teste$medv)\n\n     RMSE  Rsquared       MAE \n3.8740789 0.8403368 2.3881401",
    "crumbs": [
      "Conteúdo Semanal",
      "Arvore de Regressão (XGBoost)"
    ]
  },
  {
    "objectID": "semanas/Aula11.3.html#comparação-com-outro-modelo-regressão-linear",
    "href": "semanas/Aula11.3.html#comparação-com-outro-modelo-regressão-linear",
    "title": "Arvores de Regressão - XGBoost",
    "section": "Comparação com outro modelo (Regressão Linear)",
    "text": "Comparação com outro modelo (Regressão Linear)\n\nctrl &lt;- trainControl(method = \"cv\", number = 5)\n\nmodel_lm &lt;- train(\n  medv ~ ., data = conj_treino,\n  method = \"lm\",\n  trControl = ctrl\n)\n\npred_lm &lt;- predict(model_lm, newdata = conj_teste)\npostResample(pred_lm, conj_teste$medv)\n\n     RMSE  Rsquared       MAE \n5.9426142 0.6315418 3.9610503",
    "crumbs": [
      "Conteúdo Semanal",
      "Arvore de Regressão (XGBoost)"
    ]
  },
  {
    "objectID": "semanas/Aula12.2.html",
    "href": "semanas/Aula12.2.html",
    "title": "Arvores de Classificação - GBM",
    "section": "",
    "text": "library(ISLR)\nlibrary(dplyr)\nlibrary(gbm)\nlibrary(caret)\nlibrary(pROC)",
    "crumbs": [
      "Conteúdo Semanal",
      "Arvore de Classificação - GBM"
    ]
  },
  {
    "objectID": "semanas/Aula12.2.html#bibliotecas",
    "href": "semanas/Aula12.2.html#bibliotecas",
    "title": "Arvores de Classificação - GBM",
    "section": "",
    "text": "library(ISLR)\nlibrary(dplyr)\nlibrary(gbm)\nlibrary(caret)\nlibrary(pROC)",
    "crumbs": [
      "Conteúdo Semanal",
      "Arvore de Classificação - GBM"
    ]
  },
  {
    "objectID": "semanas/Aula12.2.html#dados",
    "href": "semanas/Aula12.2.html#dados",
    "title": "Arvores de Classificação - GBM",
    "section": "Dados",
    "text": "Dados\nVamos começar a aplicar a metodologia de árvores usando árvores de classificação para analisar os dados existentes em Carseats. Este conjunto de dados (simulado) é sobre venda de assentos de criança para carros. Ele tem 400 observações das seguintes variáveis (11), cujos nomes serão convertidos para o português:\nSales: vendas em unidades (em mil) em cada local\nCompPrice: preço cobrado pelo competidor em cada local\nIncome: nível de renda da comunidade local (em mil US$)\nAdvertising: orçamento local de propaganda (em mil US$)\nPopulation: população na região (em mil)\nPrice: preço cobrado pela empresa em cada local\nShelveLoc: um fator com níveis Ruim, Bom e Medio indicando a qualidade da localização das prateleiras para os assentos em cada lugar\nAge: idade media da população local\nEducation: nível de educação em cada local\nUrban: um fator Sim e Não indicando se a loja esta em uma área urbana ou rural\nUS: um fator indicando se a loja é nos EUA ou não\nNeste dados, Sales é a variável resposta, só que ela é uma variável contínua, por este motivo vamos usá-la para criar uma variável binária. Vamos usar a função ifelse() para criar a variável binária, que chamaremos de alta, ela assume os valores Sim se Sales for maior que 8 e assume o valor Não caso contrário:\n\ndata(Carseats)\nsummary(Carseats)\n\n     Sales          CompPrice       Income        Advertising    \n Min.   : 0.000   Min.   : 77   Min.   : 21.00   Min.   : 0.000  \n 1st Qu.: 5.390   1st Qu.:115   1st Qu.: 42.75   1st Qu.: 0.000  \n Median : 7.490   Median :125   Median : 69.00   Median : 5.000  \n Mean   : 7.496   Mean   :125   Mean   : 68.66   Mean   : 6.635  \n 3rd Qu.: 9.320   3rd Qu.:135   3rd Qu.: 91.00   3rd Qu.:12.000  \n Max.   :16.270   Max.   :175   Max.   :120.00   Max.   :29.000  \n   Population        Price        ShelveLoc        Age          Education   \n Min.   : 10.0   Min.   : 24.0   Bad   : 96   Min.   :25.00   Min.   :10.0  \n 1st Qu.:139.0   1st Qu.:100.0   Good  : 85   1st Qu.:39.75   1st Qu.:12.0  \n Median :272.0   Median :117.0   Medium:219   Median :54.50   Median :14.0  \n Mean   :264.8   Mean   :115.8                Mean   :53.32   Mean   :13.9  \n 3rd Qu.:398.5   3rd Qu.:131.0                3rd Qu.:66.00   3rd Qu.:16.0  \n Max.   :509.0   Max.   :191.0                Max.   :80.00   Max.   :18.0  \n Urban       US     \n No :118   No :142  \n Yes:282   Yes:258  \n                    \n                    \n                    \n                    \n\nstr(Carseats)\n\n'data.frame':   400 obs. of  11 variables:\n $ Sales      : num  9.5 11.22 10.06 7.4 4.15 ...\n $ CompPrice  : num  138 111 113 117 141 124 115 136 132 132 ...\n $ Income     : num  73 48 35 100 64 113 105 81 110 113 ...\n $ Advertising: num  11 16 10 4 3 13 0 15 0 0 ...\n $ Population : num  276 260 269 466 340 501 45 425 108 131 ...\n $ Price      : num  120 83 80 97 128 72 108 120 124 124 ...\n $ ShelveLoc  : Factor w/ 3 levels \"Bad\",\"Good\",\"Medium\": 1 2 3 3 1 1 3 2 3 3 ...\n $ Age        : num  42 65 59 55 38 78 71 67 76 76 ...\n $ Education  : num  17 10 12 14 13 16 15 10 10 17 ...\n $ Urban      : Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 2 2 1 2 2 1 1 ...\n $ US         : Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 2 1 2 1 2 1 2 ...",
    "crumbs": [
      "Conteúdo Semanal",
      "Arvore de Classificação - GBM"
    ]
  },
  {
    "objectID": "semanas/Aula12.2.html#manipulando-os-dados",
    "href": "semanas/Aula12.2.html#manipulando-os-dados",
    "title": "Arvores de Classificação - GBM",
    "section": "Manipulando os dados",
    "text": "Manipulando os dados\n\ncad_crianca &lt;- Carseats %&gt;% rename(vendas = Sales, \n                                   preco_comp = CompPrice,\n                                   renda = Income,\n                                   propaganda = Advertising,\n                                   populacao = Population,\n                                   preco = Price,\n                                   local_prat = ShelveLoc,\n                                   idade = Age,\n                                   educacao = Education,\n                                   urbano = Urban,\n                                   eua = US)\n\ncad_crianca &lt;- cad_crianca %&gt;%\n  mutate(vendaAlta = ifelse(vendas &gt; 8, \"Alta\", \"Baixa\")) %&gt;%\n  mutate(vendaAlta = as.factor(vendaAlta)) %&gt;%\n  select(-vendas)  # Remover Sales original\n\n# Verificar distribuição\ntable(cad_crianca$vendaAlta)\n\n\n Alta Baixa \n  164   236 \n\nstr(cad_crianca)\n\n'data.frame':   400 obs. of  11 variables:\n $ preco_comp: num  138 111 113 117 141 124 115 136 132 132 ...\n $ renda     : num  73 48 35 100 64 113 105 81 110 113 ...\n $ propaganda: num  11 16 10 4 3 13 0 15 0 0 ...\n $ populacao : num  276 260 269 466 340 501 45 425 108 131 ...\n $ preco     : num  120 83 80 97 128 72 108 120 124 124 ...\n $ local_prat: Factor w/ 3 levels \"Bad\",\"Good\",\"Medium\": 1 2 3 3 1 1 3 2 3 3 ...\n $ idade     : num  42 65 59 55 38 78 71 67 76 76 ...\n $ educacao  : num  17 10 12 14 13 16 15 10 10 17 ...\n $ urbano    : Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 2 2 1 2 2 1 1 ...\n $ eua       : Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 2 1 2 1 2 1 2 ...\n $ vendaAlta : Factor w/ 2 levels \"Alta\",\"Baixa\": 1 1 1 2 2 1 2 1 2 2 ...\n\nsummary(cad_crianca)\n\n   preco_comp      renda          propaganda       populacao    \n Min.   : 77   Min.   : 21.00   Min.   : 0.000   Min.   : 10.0  \n 1st Qu.:115   1st Qu.: 42.75   1st Qu.: 0.000   1st Qu.:139.0  \n Median :125   Median : 69.00   Median : 5.000   Median :272.0  \n Mean   :125   Mean   : 68.66   Mean   : 6.635   Mean   :264.8  \n 3rd Qu.:135   3rd Qu.: 91.00   3rd Qu.:12.000   3rd Qu.:398.5  \n Max.   :175   Max.   :120.00   Max.   :29.000   Max.   :509.0  \n     preco        local_prat      idade          educacao    urbano   \n Min.   : 24.0   Bad   : 96   Min.   :25.00   Min.   :10.0   No :118  \n 1st Qu.:100.0   Good  : 85   1st Qu.:39.75   1st Qu.:12.0   Yes:282  \n Median :117.0   Medium:219   Median :54.50   Median :14.0            \n Mean   :115.8                Mean   :53.32   Mean   :13.9            \n 3rd Qu.:131.0                3rd Qu.:66.00   3rd Qu.:16.0            \n Max.   :191.0                Max.   :80.00   Max.   :18.0            \n  eua      vendaAlta  \n No :142   Alta :164  \n Yes:258   Baixa:236",
    "crumbs": [
      "Conteúdo Semanal",
      "Arvore de Classificação - GBM"
    ]
  },
  {
    "objectID": "semanas/Aula12.2.html#treino-e-teste",
    "href": "semanas/Aula12.2.html#treino-e-teste",
    "title": "Arvores de Classificação - GBM",
    "section": "Treino e Teste",
    "text": "Treino e Teste\n\nset.seed(21)\ny &lt;- cad_crianca$vendaAlta\nindice_teste &lt;- createDataPartition(y, times = 1, p = 0.2, list = FALSE)\n\nconj_treino &lt;- cad_crianca[-indice_teste,]\nconj_teste &lt;- cad_crianca[indice_teste,]\n\nstr(conj_treino)\n\n'data.frame':   319 obs. of  11 variables:\n $ preco_comp: num  111 117 141 124 115 132 132 121 117 122 ...\n $ renda     : num  48 100 64 113 105 110 113 78 94 35 ...\n $ propaganda: num  16 4 3 13 0 0 0 9 4 2 ...\n $ populacao : num  260 466 340 501 45 108 131 150 503 393 ...\n $ preco     : num  83 97 128 72 108 124 124 100 94 136 ...\n $ local_prat: Factor w/ 3 levels \"Bad\",\"Good\",\"Medium\": 2 3 1 1 3 3 3 1 2 3 ...\n $ idade     : num  65 55 38 78 71 76 76 26 50 62 ...\n $ educacao  : num  10 14 13 16 15 10 17 10 13 18 ...\n $ urbano    : Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 1 2 1 1 1 2 2 ...\n $ eua       : Factor w/ 2 levels \"No\",\"Yes\": 2 2 1 2 1 1 2 2 2 1 ...\n $ vendaAlta : Factor w/ 2 levels \"Alta\",\"Baixa\": 1 2 2 1 2 2 2 1 1 2 ...\n\nprop.table(table(conj_treino$alta))\n\nnumeric(0)\n\nstr(conj_teste)\n\n'data.frame':   81 obs. of  11 variables:\n $ preco_comp: num  138 113 136 125 139 98 115 131 157 118 ...\n $ renda     : num  73 35 81 90 32 118 54 84 53 71 ...\n $ propaganda: num  11 10 15 2 0 0 0 11 0 4 ...\n $ populacao : num  276 269 425 367 176 19 406 29 403 148 ...\n $ preco     : num  120 80 120 131 82 107 128 96 124 114 ...\n $ local_prat: Factor w/ 3 levels \"Bad\",\"Good\",\"Medium\": 1 3 2 3 2 3 3 3 1 3 ...\n $ idade     : num  42 59 67 35 54 64 42 44 58 80 ...\n $ educacao  : num  17 12 10 18 11 17 17 17 16 13 ...\n $ urbano    : Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 2 1 2 2 1 2 2 ...\n $ eua       : Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 2 1 1 2 2 1 1 ...\n $ vendaAlta : Factor w/ 2 levels \"Alta\",\"Baixa\": 1 1 1 2 1 2 2 1 2 2 ...\n\nprop.table(table(conj_teste$alta))\n\nnumeric(0)",
    "crumbs": [
      "Conteúdo Semanal",
      "Arvore de Classificação - GBM"
    ]
  },
  {
    "objectID": "semanas/Aula12.2.html#gbm",
    "href": "semanas/Aula12.2.html#gbm",
    "title": "Arvores de Classificação - GBM",
    "section": "GBM",
    "text": "GBM\nAjustando um modelo GBM\n\nset.seed(21)\n\n# Definir grade de parâmetros\ntune_grid &lt;- expand.grid(\n  n.trees = c(100, 300, 500),\n  interaction.depth = c(1, 3, 5),\n  shrinkage = c(0.01, 0.1),\n  n.minobsinnode = c(5, 10)\n)\n\n# Treinamento com validação cruzada\nctrl &lt;- trainControl(method = \"cv\", number = 5)\n\ngbm_caret &lt;- train(\n  vendaAlta ~ .,\n  data = conj_treino,\n  method = \"gbm\",\n  distribution = \"bernoulli\",\n  trControl = ctrl,\n  tuneGrid = tune_grid,\n  verbose = FALSE\n)",
    "crumbs": [
      "Conteúdo Semanal",
      "Arvore de Classificação - GBM"
    ]
  },
  {
    "objectID": "semanas/Aula12.2.html#melhor-modelo",
    "href": "semanas/Aula12.2.html#melhor-modelo",
    "title": "Arvores de Classificação - GBM",
    "section": "Melhor modelo",
    "text": "Melhor modelo\n\n# Melhor modelo\ngbm_caret$bestTune\n\n   n.trees interaction.depth shrinkage n.minobsinnode\n23     300                 1       0.1             10",
    "crumbs": [
      "Conteúdo Semanal",
      "Arvore de Classificação - GBM"
    ]
  },
  {
    "objectID": "semanas/Aula12.2.html#verificando-os-resultados",
    "href": "semanas/Aula12.2.html#verificando-os-resultados",
    "title": "Arvores de Classificação - GBM",
    "section": "Verificando os Resultados",
    "text": "Verificando os Resultados\n\n# Previsões\npred_class &lt;- predict(gbm_caret, newdata = conj_teste)\n\n# Matriz de confusão\nconfusionMatrix(pred_class, conj_teste$vendaAlta)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction Alta Baixa\n     Alta    25     6\n     Baixa    8    42\n                                         \n               Accuracy : 0.8272         \n                 95% CI : (0.727, 0.9022)\n    No Information Rate : 0.5926         \n    P-Value [Acc &gt; NIR] : 5.302e-06      \n                                         \n                  Kappa : 0.6386         \n                                         \n Mcnemar's Test P-Value : 0.7893         \n                                         \n            Sensitivity : 0.7576         \n            Specificity : 0.8750         \n         Pos Pred Value : 0.8065         \n         Neg Pred Value : 0.8400         \n             Prevalence : 0.4074         \n         Detection Rate : 0.3086         \n   Detection Prevalence : 0.3827         \n      Balanced Accuracy : 0.8163         \n                                         \n       'Positive' Class : Alta",
    "crumbs": [
      "Conteúdo Semanal",
      "Arvore de Classificação - GBM"
    ]
  },
  {
    "objectID": "semanas/Aula12.2.html#curva-roc",
    "href": "semanas/Aula12.2.html#curva-roc",
    "title": "Arvores de Classificação - GBM",
    "section": "Curva ROC",
    "text": "Curva ROC\n\n# Probabilidades previstas\npred_prob &lt;- predict(gbm_caret, newdata = conj_teste, type = \"prob\")\n\n# Curva ROC\nroc_obj &lt;- roc(response = conj_teste$vendaAlta, predictor = pred_prob$Alta)\nplot(roc_obj, col = \"blue\", lwd = 2, main = \"Curva ROC - GBM\")\n\n\n\n\n\n\nauc(roc_obj)\n\nArea under the curve: 0.9394",
    "crumbs": [
      "Conteúdo Semanal",
      "Arvore de Classificação - GBM"
    ]
  },
  {
    "objectID": "semanas/Aula12.2.html#importância-das-variáveis",
    "href": "semanas/Aula12.2.html#importância-das-variáveis",
    "title": "Arvores de Classificação - GBM",
    "section": "Importância das variáveis",
    "text": "Importância das variáveis\n\n# Obter importância\nimportancia &lt;- varImp(gbm_caret)\n\n# Visualização gráfica\nplot(importancia, top = 10, main = \"Importância das Variáveis - GBM\")",
    "crumbs": [
      "Conteúdo Semanal",
      "Arvore de Classificação - GBM"
    ]
  },
  {
    "objectID": "semanas/Aula12.3A.html",
    "href": "semanas/Aula12.3A.html",
    "title": "Arvores de Classificação - XGboost no Tidymodels",
    "section": "",
    "text": "library(tidyverse)\nlibrary(ISLR)"
  },
  {
    "objectID": "semanas/Aula12.3A.html#bibliotecas",
    "href": "semanas/Aula12.3A.html#bibliotecas",
    "title": "Arvores de Classificação - XGboost no Tidymodels",
    "section": "",
    "text": "library(tidyverse)\nlibrary(ISLR)"
  },
  {
    "objectID": "semanas/Aula12.3A.html#dados",
    "href": "semanas/Aula12.3A.html#dados",
    "title": "Arvores de Classificação - XGboost no Tidymodels",
    "section": "Dados",
    "text": "Dados\n\ndata(Default)\nsummary(Default)\n\n default    student       balance           income     \n No :9667   No :7056   Min.   :   0.0   Min.   :  772  \n Yes: 333   Yes:2944   1st Qu.: 481.7   1st Qu.:21340  \n                       Median : 823.6   Median :34553  \n                       Mean   : 835.4   Mean   :33517  \n                       3rd Qu.:1166.3   3rd Qu.:43808  \n                       Max.   :2654.3   Max.   :73554  \n\nstr(Default)\n\n'data.frame':   10000 obs. of  4 variables:\n $ default: Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n $ student: Factor w/ 2 levels \"No\",\"Yes\": 1 2 1 1 1 2 1 2 1 1 ...\n $ balance: num  730 817 1074 529 786 ...\n $ income : num  44362 12106 31767 35704 38463 ...\n\nhead(Default)\n\n  default student   balance    income\n1      No      No  729.5265 44361.625\n2      No     Yes  817.1804 12106.135\n3      No      No 1073.5492 31767.139\n4      No      No  529.2506 35704.494\n5      No      No  785.6559 38463.496\n6      No     Yes  919.5885  7491.559"
  },
  {
    "objectID": "semanas/Aula12.3A.html#manipulando-os-dados",
    "href": "semanas/Aula12.3A.html#manipulando-os-dados",
    "title": "Arvores de Classificação - XGboost no Tidymodels",
    "section": "Manipulando os dados",
    "text": "Manipulando os dados\n\ncredito &lt;- tibble(Default)\nsummary(credito)\n\n default    student       balance           income     \n No :9667   No :7056   Min.   :   0.0   Min.   :  772  \n Yes: 333   Yes:2944   1st Qu.: 481.7   1st Qu.:21340  \n                       Median : 823.6   Median :34553  \n                       Mean   : 835.4   Mean   :33517  \n                       3rd Qu.:1166.3   3rd Qu.:43808  \n                       Max.   :2654.3   Max.   :73554  \n\n# renomeando colunas\ncredito &lt;- credito %&gt;% \n                rename( inadimplente = default, estudante = student, balanco = balance,\n                receita = income)\ncredito &lt;- credito %&gt;% mutate( inadimplente =  case_when(\n                           inadimplente == \"No\"  ~ \"Nao\",\n                           inadimplente == \"Yes\" ~ \"Sim\"\n                          )) %&gt;% mutate(inadimplente = factor(inadimplente))\ncredito &lt;- credito %&gt;% mutate( estudante =  case_when(\n                           estudante == \"No\"  ~ 0,\n                           estudante == \"Yes\" ~ 1\n                          )) \n\nstr(credito)\n\ntibble [10,000 × 4] (S3: tbl_df/tbl/data.frame)\n $ inadimplente: Factor w/ 2 levels \"Nao\",\"Sim\": 1 1 1 1 1 1 1 1 1 1 ...\n $ estudante   : num [1:10000] 0 1 0 0 0 1 0 1 0 0 ...\n $ balanco     : num [1:10000] 730 817 1074 529 786 ...\n $ receita     : num [1:10000] 44362 12106 31767 35704 38463 ...\n\nsummary(credito)\n\n inadimplente   estudante         balanco          receita     \n Nao:9667     Min.   :0.0000   Min.   :   0.0   Min.   :  772  \n Sim: 333     1st Qu.:0.0000   1st Qu.: 481.7   1st Qu.:21340  \n              Median :0.0000   Median : 823.6   Median :34553  \n              Mean   :0.2944   Mean   : 835.4   Mean   :33517  \n              3rd Qu.:1.0000   3rd Qu.:1166.3   3rd Qu.:43808  \n              Max.   :1.0000   Max.   :2654.3   Max.   :73554"
  },
  {
    "objectID": "semanas/Aula12.3A.html#treino-e-teste",
    "href": "semanas/Aula12.3A.html#treino-e-teste",
    "title": "Arvores de Classificação - XGboost no Tidymodels",
    "section": "Treino e Teste",
    "text": "Treino e Teste\n\nlibrary(tidymodels)\nset.seed(2024)\n\ndados_split &lt;- initial_split(credito, strata = inadimplente)\nconj_treino &lt;- training(dados_split)\nconj_teste &lt;- testing(dados_split)"
  },
  {
    "objectID": "semanas/Aula12.3A.html#treinando",
    "href": "semanas/Aula12.3A.html#treinando",
    "title": "Arvores de Classificação - XGboost no Tidymodels",
    "section": "Treinando",
    "text": "Treinando\n\n## 1a tentativa Xgboost\nxgb_spec &lt;- boost_tree(\n  trees = 1000,\n  tree_depth = tune(), min_n = tune(),\n  loss_reduction = tune(),         ## first three: model complexity\n  sample_size = tune(), mtry = tune(),         ## randomness\n  learn_rate = tune()                          ## step size\n) %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"classification\")\n\nxgb_spec\n\nBoosted Tree Model Specification (classification)\n\nMain Arguments:\n  mtry = tune()\n  trees = 1000\n  min_n = tune()\n  tree_depth = tune()\n  learn_rate = tune()\n  loss_reduction = tune()\n  sample_size = tune()\n\nComputational engine: xgboost \n\n\n\nxgb_grid &lt;- grid_latin_hypercube(\n  tree_depth(),\n  min_n(),\n  loss_reduction(),\n  sample_size = sample_prop(),\n  finalize(mtry(), conj_treino),\n  learn_rate(),\n  size = 30\n)\n\nxgb_grid\n\n# A tibble: 30 × 6\n   tree_depth min_n loss_reduction sample_size  mtry learn_rate\n        &lt;int&gt; &lt;int&gt;          &lt;dbl&gt;       &lt;dbl&gt; &lt;int&gt;      &lt;dbl&gt;\n 1          8    32       3.30e+ 0       0.324     2   1.61e- 5\n 2         14    11       1.14e- 7       0.132     1   8.04e-10\n 3         13     5       1.36e- 2       0.924     3   3.83e- 2\n 4          6    37       3.22e- 6       0.666     4   2.75e- 5\n 5          1    11       4.68e- 9       0.189     2   6.81e- 9\n 6         13    17       5.55e- 2       0.563     2   1.11e-10\n 7         15    16       1.74e-10       0.106     1   2.35e- 8\n 8          3     4       3.68e- 3       0.385     3   5.14e- 6\n 9          8    35       4.34e- 5       0.843     3   6.70e- 5\n10         13    20       3.07e- 4       0.969     1   4.09e-10\n# ℹ 20 more rows\n\n\n\nxgb_wf &lt;- workflow() %&gt;%\n  add_formula(inadimplente ~ .) %&gt;%\n  add_model(xgb_spec)\n\nxgb_wf\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Formula\nModel: boost_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\ninadimplente ~ .\n\n── Model ───────────────────────────────────────────────────────────────────────\nBoosted Tree Model Specification (classification)\n\nMain Arguments:\n  mtry = tune()\n  trees = 1000\n  min_n = tune()\n  tree_depth = tune()\n  learn_rate = tune()\n  loss_reduction = tune()\n  sample_size = tune()\n\nComputational engine: xgboost"
  },
  {
    "objectID": "semanas/Aula12.3A.html#section",
    "href": "semanas/Aula12.3A.html#section",
    "title": "Arvores de Classificação - XGboost no Tidymodels",
    "section": "",
    "text": "set.seed(2024)\nvb_folds &lt;- vfold_cv(conj_treino, v = 5, strata = inadimplente)\nvb_folds\n\n#  5-fold cross-validation using stratification \n# A tibble: 5 × 2\n  splits              id   \n  &lt;list&gt;              &lt;chr&gt;\n1 &lt;split [6000/1500]&gt; Fold1\n2 &lt;split [6000/1500]&gt; Fold2\n3 &lt;split [6000/1500]&gt; Fold3\n4 &lt;split [6000/1500]&gt; Fold4\n5 &lt;split [6000/1500]&gt; Fold5\n\n\n\ndoParallel::registerDoParallel()\n\nset.seed(2024)\nxgb_res &lt;- tune_grid(\n  xgb_wf,\n  resamples = vb_folds,\n  grid = xgb_grid,\n  control = control_grid(save_pred = TRUE)\n)\n\nxgb_res\n\n# Tuning results\n# 5-fold cross-validation using stratification \n# A tibble: 5 × 5\n  splits              id    .metrics           .notes           .predictions\n  &lt;list&gt;              &lt;chr&gt; &lt;list&gt;             &lt;list&gt;           &lt;list&gt;      \n1 &lt;split [6000/1500]&gt; Fold1 &lt;tibble [90 × 10]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n2 &lt;split [6000/1500]&gt; Fold2 &lt;tibble [90 × 10]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n3 &lt;split [6000/1500]&gt; Fold3 &lt;tibble [90 × 10]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n4 &lt;split [6000/1500]&gt; Fold4 &lt;tibble [90 × 10]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n5 &lt;split [6000/1500]&gt; Fold5 &lt;tibble [90 × 10]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n\n\n\nxgb_res %&gt;% collect_metrics()\n\n# A tibble: 90 × 12\n    mtry min_n tree_depth learn_rate loss_reduction sample_size .metric    \n   &lt;int&gt; &lt;int&gt;      &lt;int&gt;      &lt;dbl&gt;          &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;      \n 1     2    32          8   1.61e- 5    3.30              0.324 accuracy   \n 2     2    32          8   1.61e- 5    3.30              0.324 brier_class\n 3     2    32          8   1.61e- 5    3.30              0.324 roc_auc    \n 4     1    11         14   8.04e-10    0.000000114       0.132 accuracy   \n 5     1    11         14   8.04e-10    0.000000114       0.132 brier_class\n 6     1    11         14   8.04e-10    0.000000114       0.132 roc_auc    \n 7     3     5         13   3.83e- 2    0.0136            0.924 accuracy   \n 8     3     5         13   3.83e- 2    0.0136            0.924 brier_class\n 9     3     5         13   3.83e- 2    0.0136            0.924 roc_auc    \n10     4    37          6   2.75e- 5    0.00000322        0.666 accuracy   \n# ℹ 80 more rows\n# ℹ 5 more variables: .estimator &lt;chr&gt;, mean &lt;dbl&gt;, n &lt;int&gt;, std_err &lt;dbl&gt;,\n#   .config &lt;chr&gt;\n\n\n\nxgb_res %&gt;%\n  collect_metrics() %&gt;%\n  filter(.metric == \"roc_auc\") %&gt;%\n  select(mean, mtry:sample_size) %&gt;%\n  pivot_longer(mtry:sample_size,\n               values_to = \"value\",\n               names_to = \"parameter\"\n  ) %&gt;%\n  ggplot(aes(value, mean, color = parameter)) +\n  geom_point(alpha = 0.8, show.legend = FALSE) +\n  facet_wrap(~parameter, scales = \"free_x\") +\n  labs(x = NULL, y = \"AUC\")\n\n\n\n\n\n\n\n\nmelhor_auc &lt;- select_best(xgb_res)\n\n\nfinal_xgb &lt;- finalize_workflow(\n  xgb_wf,\n  melhor_auc\n)\n\nfinal_xgb\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Formula\nModel: boost_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\ninadimplente ~ .\n\n── Model ───────────────────────────────────────────────────────────────────────\nBoosted Tree Model Specification (classification)\n\nMain Arguments:\n  mtry = 3\n  trees = 1000\n  min_n = 24\n  tree_depth = 4\n  learn_rate = 0.0139744688194663\n  loss_reduction = 1.15404948868465\n  sample_size = 0.407858160007745\n\nComputational engine: xgboost \n\n\n\nlibrary(vip)\n\nfinal_xgb %&gt;%\n  fit(data = conj_treino) %&gt;%\n  pull_workflow_fit() %&gt;%\n  vip(geom = \"point\")\n\n\n\n\n\n\n\n\nfinal_res &lt;- last_fit(final_xgb, dados_split)\n\ncollect_metrics(final_res)\n\n# A tibble: 3 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary        0.966  Preprocessor1_Model1\n2 roc_auc     binary        0.948  Preprocessor1_Model1\n3 brier_class binary        0.0257 Preprocessor1_Model1\n\n\n\nfinal_res %&gt;%\n  collect_predictions() %&gt;%\n  roc_curve(truth=inadimplente, .pred_Sim, event_level = \"second\") %&gt;%\n  autoplot()\n\n\n\n\n\n\n\n\nmat_conf &lt;- final_res %&gt;%\n  collect_predictions() %&gt;%\n  conf_mat(truth = inadimplente, estimate = .pred_class, event_level = \"second\")\nsummary(mat_conf)\n\n# A tibble: 13 × 3\n   .metric              .estimator .estimate\n   &lt;chr&gt;                &lt;chr&gt;          &lt;dbl&gt;\n 1 accuracy             binary         0.966\n 2 kap                  binary         0.377\n 3 sens                 binary         0.994\n 4 spec                 binary         0.281\n 5 ppv                  binary         0.972\n 6 npv                  binary         0.643\n 7 mcc                  binary         0.411\n 8 j_index              binary         0.275\n 9 bal_accuracy         binary         0.638\n10 detection_prevalence binary         0.983\n11 precision            binary         0.972\n12 recall               binary         0.994\n13 f_meas               binary         0.983"
  },
  {
    "objectID": "semanas/Aula14.html",
    "href": "semanas/Aula14.html",
    "title": "Análise de Associação",
    "section": "",
    "text": "Este exemplo de Análise de Associação foi baseado no curso do DataCamp “Market Basket Analysis”\nAs bibilotecas usadas são as arules e arulesViz.",
    "crumbs": [
      "Conteúdo Semanal",
      "Análise de Associação"
    ]
  },
  {
    "objectID": "semanas/Aula14.html#bibliotecas",
    "href": "semanas/Aula14.html#bibliotecas",
    "title": "Análise de Associação",
    "section": "Bibliotecas",
    "text": "Bibliotecas\n\nlibrary(tidyverse)\nlibrary(arules)\nlibrary(arulesViz)",
    "crumbs": [
      "Conteúdo Semanal",
      "Análise de Associação"
    ]
  },
  {
    "objectID": "semanas/Aula14.html#dados",
    "href": "semanas/Aula14.html#dados",
    "title": "Análise de Associação",
    "section": "Dados",
    "text": "Dados\nO conjunto de dados Movies consiste de\n\nlibrary(readxl)\nfilmes &lt;- read_excel(\"Movie_subset.xlsx\")\nstr(filmes)\n\ntibble [19,455 × 5] (S3: tbl_df/tbl/data.frame)\n $ userId : num [1:19455] 1323 1323 1323 1323 1323 ...\n $ movieId: num [1:19455] 1 3 5 10 11 12 15 16 17 19 ...\n $ title  : chr [1:19455] \"Toy Story\" \"Grumpier Old Men\" \"Father of the Bride Part II\" \"GoldenEye\" ...\n $ year   : num [1:19455] 1995 1995 1995 1995 1995 ...\n $ genres : chr [1:19455] \"Adventure|Animation|Children|Comedy|Fantasy\" \"Comedy|Romance\" \"Comedy\" \"Action|Adventure|Thriller\" ...\n\nhead(filmes)\n\n# A tibble: 6 × 5\n  userId movieId title                        year genres                       \n   &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                       &lt;dbl&gt; &lt;chr&gt;                        \n1   1323       1 Toy Story                    1995 Adventure|Animation|Children…\n2   1323       3 Grumpier Old Men             1995 Comedy|Romance               \n3   1323       5 Father of the Bride Part II  1995 Comedy                       \n4   1323      10 GoldenEye                    1995 Action|Adventure|Thriller    \n5   1323      11 American President, The      1995 Comedy|Drama|Romance         \n6   1323      12 Dracula: Dead and Loving It  1995 Comedy|Horror",
    "crumbs": [
      "Conteúdo Semanal",
      "Análise de Associação"
    ]
  },
  {
    "objectID": "semanas/Aula14.html#explorando-os-dados",
    "href": "semanas/Aula14.html#explorando-os-dados",
    "title": "Análise de Associação",
    "section": "Explorando os dados",
    "text": "Explorando os dados\n\nn_distinct(filmes$title)\n\n[1] 4508\n\nn_distinct(filmes$userId)\n\n[1] 100\n\nfilmes %&gt;%\n  group_by(userId) %&gt;% \n  summarise(n_filmes = n_distinct(movieId)) %&gt;%\n  ggplot(aes(x = n_filmes)) +\n  geom_bar() + \n  ggtitle(\"Distribuição do número de filmes vistos\")",
    "crumbs": [
      "Conteúdo Semanal",
      "Análise de Associação"
    ]
  },
  {
    "objectID": "semanas/Aula14.html#tranformando-em-transações",
    "href": "semanas/Aula14.html#tranformando-em-transações",
    "title": "Análise de Associação",
    "section": "Tranformando em Transações",
    "text": "Tranformando em Transações\n\nlista_de_filmes &lt;- split(filmes$title, filmes$userId)\nfilmes_trx = as(lista_de_filmes, \"transactions\")\n#Imagem de parte das transações\nimage(filmes_trx[1:100,1:100])",
    "crumbs": [
      "Conteúdo Semanal",
      "Análise de Associação"
    ]
  },
  {
    "objectID": "semanas/Aula14.html#visualizando-as-transações",
    "href": "semanas/Aula14.html#visualizando-as-transações",
    "title": "Análise de Associação",
    "section": "Visualizando as transações",
    "text": "Visualizando as transações\nA biblioteca arules tem funções para visualização das transações masi frequentes.\nElas podem ser vistas na forma de frequencias absolutas ou relativas.\nFrequencia Relativa e Absoluta\n\npar(mfrow=c(2,1))\nitemFrequencyPlot(filmes_trx,\n                  type = \"relative\",\n                  topN = 10,\n                  horiz = TRUE,\n                  main = 'Frequencia Relativa')\n\nitemFrequencyPlot(filmes_trx,\n                  type = \"absolute\",\n                  topN = 10,\n                  horiz = TRUE,\n                  main = 'Frequencia Absoluta')\n\n\n\n\n\n\n\nMostrando os filmes menos populares\nPara mostrar o final da lista de prefrência é necessário utilizarmos funções adicionais.\n\npar(mar=c(2,30,2,2), mfrow=c(1,1))\nbarplot(sort(table(unlist(LIST(filmes_trx))))[1:10],\n        horiz = TRUE,\n        las = 1,\n        main = 'Menos populares')\n\n\n\n\n\n\n\nExtraindo os filmes mais frequentes\nA função apriori que aplica o algoritmo de mesmo nome permite que obtenhamos a lista de filmes mais frequentes e também as regras criadas a partir da utilização do algoritmo.\nNeste exemplo somente o suporte teve um limite mínimo defindo\n\nconj_filmes &lt;- apriori(filmes_trx,\n                    parameter = list(support = 0.4,\n                               target = 'frequent'\n                    ))\n\nApriori\n\nParameter specification:\n confidence minval smax arem  aval originalSupport maxtime support minlen\n         NA    0.1    1 none FALSE            TRUE       5     0.4      1\n maxlen            target  ext\n     10 frequent itemsets TRUE\n\nAlgorithmic control:\n filter tree heap memopt load sort verbose\n    0.1 TRUE TRUE  FALSE TRUE    2    TRUE\n\nAbsolute minimum support count: 40 \n\nset item appearances ...[0 item(s)] done [0.00s].\nset transactions ...[4508 item(s), 100 transaction(s)] done [0.00s].\nsorting and recoding items ... [15 item(s)] done [0.00s].\ncreating transaction tree ... done [0.00s].\nchecking subsets of size 1 2 done [0.00s].\nsorting transactions ... done [0.00s].\nwriting ... [16 set(s)] done [0.00s].\ncreating S4 object  ... done [0.00s].\n\ninspect(sort(conj_filmes, by='support', decreasing = T)[1:5])\n\n    items                       support count\n[1] {Matrix, The}               0.60    60   \n[2] {American Beauty}           0.57    57   \n[3] {Fight Club}                0.54    54   \n[4] {Silence of the Lambs, The} 0.50    50   \n[5] {Shawshank Redemption, The} 0.48    48   \n\n\nNeste exemplo eliminamos os conjuntos com um só elemento e reduzimos o suporte para 30%\n\nitemset2 = apriori(filmes_trx, parameter = \n                           list(support = 0.3,\n                                minlen = 2,\n                                target = 'frequent'\n                            ))\n\nApriori\n\nParameter specification:\n confidence minval smax arem  aval originalSupport maxtime support minlen\n         NA    0.1    1 none FALSE            TRUE       5     0.3      2\n maxlen            target  ext\n     10 frequent itemsets TRUE\n\nAlgorithmic control:\n filter tree heap memopt load sort verbose\n    0.1 TRUE TRUE  FALSE TRUE    2    TRUE\n\nAbsolute minimum support count: 30 \n\nset item appearances ...[0 item(s)] done [0.00s].\nset transactions ...[4508 item(s), 100 transaction(s)] done [0.00s].\nsorting and recoding items ... [56 item(s)] done [0.00s].\ncreating transaction tree ... done [0.00s].\nchecking subsets of size 1 2 3 4 done [0.00s].\nsorting transactions ... done [0.00s].\nwriting ... [115 set(s)] done [0.00s].\ncreating S4 object  ... done [0.00s].\n\ninspect(sort(itemset2, \n             by='support', decreasing = T)[1:5])\n\n    items                                                 support count\n[1] {Matrix, The,                                                      \n     Silence of the Lambs, The}                              0.40    40\n[2] {Lord of the Rings: The Fellowship of the Ring, The,               \n     Lord of the Rings: The Two Towers, The}                 0.38    38\n[3] {American Beauty,                                                  \n     Pulp Fiction}                                           0.38    38\n[4] {Pulp Fiction,                                                     \n     Silence of the Lambs, The}                              0.38    38\n[5] {Matrix, The,                                                      \n     Star Wars: Episode IV - A New Hope}                     0.38    38\n\n\nAqui invertemos a ordem apresentando os 5 menos populares\n\ninspect(sort(itemset2, \n             by='support', decreasing = F)[1:5])\n\n    items                                            support count\n[1] {Matrix, The, Minority Report}                   0.3     30   \n[2] {Matrix, The, Shrek}                             0.3     30   \n[3] {Pulp Fiction, Schindler's List}                 0.3     30   \n[4] {Braveheart, Jurassic Park}                      0.3     30   \n[5] {Braveheart, Star Wars: Episode IV - A New Hope} 0.3     30   \n\n\nAvaliando diferentes níveis de confiança\nUsando o suporte de 40% e 30% vamos ver as diferenças nos números de regras, variando o nível de confiança.\n\nconfidenceLevels = seq(from=0.95, to=0.5, by=-0.05)\n\nregras_sup04 &lt;- NULL\n\n\nfor (i in 1:length(confidenceLevels)) \n  {\n  regras_sup04[i] = \n  length(apriori(filmes_trx,\n                 parameter=list(sup=0.40, \n                                conf=confidenceLevels[i],\n                                target=\"rules\")))\n}\n\n\nregras_sup03 &lt;- NULL\n\n\nfor (i in 1:length(confidenceLevels)) \n  {\n  regras_sup03[i] = \n  length(apriori(filmes_trx,\n                 parameter=list(sup=0.30, \n                                conf=confidenceLevels[i],\n                                target=\"rules\")))\n}\n\nNúmero de regras com suporte de 40%\n\nqplot(confidenceLevels, regras_sup04, \n      geom=c(\"point\", \"line\"),xlab=\"Nível de Confiança\",\n      ylab=\"Numero de regras achadas\", \n      main=\"Apriori com nível de suporte de 40%\") +\n  theme_bw()\n\n\n\n\n\n\n\nVisualizando os resultados\n\nn_regras &lt;- data.frame(regras_sup04, regras_sup03,\n                      confidenceLevels)\n\n# 40% e 30%\nggplot(data=n_regras, aes(x=confidenceLevels)) +\n  # regras_sup04\n  geom_line(aes(y= regras_sup04, colour=\"Nível de Suporte de 40%\")) + \n  geom_point(aes(y=regras_sup04,colour=\"Nível de Suporte de 40%\")) +\n  # regras_sup03\n  geom_line(aes(y=regras_sup03, colour=\"Nível de Suporte de 30%\")) +\n  geom_point(aes(y=regras_sup03,colour=\"Nível de Suporte de 30%\")) + \n  # \n  theme_bw() + ylab(\"Numero de regras\") +\n  ggtitle(\"Numero de regras extraídas com o apriori\")",
    "crumbs": [
      "Conteúdo Semanal",
      "Análise de Associação"
    ]
  },
  {
    "objectID": "semanas/Aula14.html#extraindo-regras",
    "href": "semanas/Aula14.html#extraindo-regras",
    "title": "Análise de Associação",
    "section": "Extraindo regras",
    "text": "Extraindo regras\n\nregras_filmes2 = apriori(filmes_trx,\n                          parameter = list(supp = 0.3,\n                                           conf = 0.9,\n                                           minlen = 2),\n                         appearance = list(rhs=\"Matrix, The\"),\n                         control = list(verbose=F))\ninspect(regras_filmes2)\n\n    lhs                                                                          rhs           support confidence coverage     lift count\n[1] {Minority Report}                                                         =&gt; {Matrix, The}    0.30  0.9090909     0.33 1.515152    30\n[2] {Braveheart}                                                              =&gt; {Matrix, The}    0.35  0.9210526     0.38 1.535088    35\n[3] {Star Wars: Episode V - The Empire Strikes Back}                          =&gt; {Matrix, The}    0.36  0.9230769     0.39 1.538462    36\n[4] {Jurassic Park,                                                                                                                      \n     Silence of the Lambs, The}                                               =&gt; {Matrix, The}    0.30  0.9090909     0.33 1.515152    30\n[5] {Star Wars: Episode IV - A New Hope,                                                                                                 \n     Star Wars: Episode V - The Empire Strikes Back}                          =&gt; {Matrix, The}    0.34  0.9444444     0.36 1.574074    34\n[6] {Back to the Future,                                                                                                                 \n     Raiders of the Lost Ark (Indiana Jones and the Raiders of the Lost Ark)} =&gt; {Matrix, The}    0.30  0.9375000     0.32 1.562500    30\n[7] {Forrest Gump,                                                                                                                       \n     Silence of the Lambs, The}                                               =&gt; {Matrix, The}    0.31  0.9393939     0.33 1.565657    31\n[8] {Back to the Future,                                                                                                                 \n     Star Wars: Episode IV - A New Hope}                                      =&gt; {Matrix, The}    0.31  0.9393939     0.33 1.565657    31\n\n\nVisualizado como um gráfico de dispersão\nAs medidas selecionadas são confiança e interesse (lift)\n\nregras_filmes = apriori(filmes_trx,\n                          parameter = list(supp = 0.3,\n                                           conf = 0.9,\n                                           minlen = 2, \n                                           target = \"rules\"))\n\nApriori\n\nParameter specification:\n confidence minval smax arem  aval originalSupport maxtime support minlen\n        0.9    0.1    1 none FALSE            TRUE       5     0.3      2\n maxlen target  ext\n     10  rules TRUE\n\nAlgorithmic control:\n filter tree heap memopt load sort verbose\n    0.1 TRUE TRUE  FALSE TRUE    2    TRUE\n\nAbsolute minimum support count: 30 \n\nset item appearances ...[0 item(s)] done [0.00s].\nset transactions ...[4508 item(s), 100 transaction(s)] done [0.00s].\nsorting and recoding items ... [56 item(s)] done [0.00s].\ncreating transaction tree ... done [0.00s].\nchecking subsets of size 1 2 3 4 done [0.00s].\nwriting ... [26 rule(s)] done [0.00s].\ncreating S4 object  ... done [0.00s].\n\nplot(regras_filmes,\n     measure = c(\"confidence\", \"lift\"),\n     shading = \"support\",\n     jitter = 1,\n     engine =\"html\")\n\n\n\n\n\n\n# Interactive matrix-based plot\nplot(regras_filmes, method = \"matrix\",\n     shading =\"confidence\",\n     engine = \"html\"\n     )\n\n\n\n\n\n\n# Parallel coordinate plots with confidence as color coding\nplot(regras_filmes, \n     method = \"paracoord\", \n     shading = \"confidence\")\n\n\n\n\n\n\n\n\n# Plot movie rules as a graph\nplot(regras_filmes,\n     method = \"graph\",\n     engine = \"htmlwidget\")\n\n\n\n\n\n\n# Retrieve the top 10 rules with highest confidence\ntop10_rules_movies = head(sort(regras_filmes,by=\"confidence\"),10)\n# Plot as an interactive graph the top 10 rules\nplot(top10_rules_movies,\n     method = \"graph\",engine = \"htmlwidget\")\n\n\n\n\n\nExtraindo regras com um determinando filme\n\n# Extract rules with Pulp Fiction on the right side\npulpfiction_rules_rhs = apriori(filmes_trx, \n                           parameter = list(supp = 0.3,\n                                            conf = 0.5), \n                       appearance = list(default = \"lhs\",\n                                         rhs = \"Pulp Fiction\")) \n\nApriori\n\nParameter specification:\n confidence minval smax arem  aval originalSupport maxtime support minlen\n        0.5    0.1    1 none FALSE            TRUE       5     0.3      1\n maxlen target  ext\n     10  rules TRUE\n\nAlgorithmic control:\n filter tree heap memopt load sort verbose\n    0.1 TRUE TRUE  FALSE TRUE    2    TRUE\n\nAbsolute minimum support count: 30 \n\nset item appearances ...[1 item(s)] done [0.00s].\nset transactions ...[4508 item(s), 100 transaction(s)] done [0.00s].\nsorting and recoding items ... [56 item(s)] done [0.00s].\ncreating transaction tree ... done [0.00s].\nchecking subsets of size 1 2 3 4 done [0.00s].\nwriting ... [19 rule(s)] done [0.00s].\ncreating S4 object  ... done [0.00s].\n\n# Inspect the first rules\ninspect(head(pulpfiction_rules_rhs,5))\n\n    lhs                                                     rhs            support confidence coverage     lift count\n[1] {Schindler's List}                                   =&gt; {Pulp Fiction}    0.30  0.6818182     0.44 1.450677    30\n[2] {Jurassic Park}                                      =&gt; {Pulp Fiction}    0.31  0.7209302     0.43 1.533894    31\n[3] {Seven (a.k.a. Se7en)}                               =&gt; {Pulp Fiction}    0.30  0.8108108     0.37 1.725129    30\n[4] {Lord of the Rings: The Fellowship of the Ring, The} =&gt; {Pulp Fiction}    0.31  0.6888889     0.45 1.465721    31\n[5] {Sixth Sense, The}                                   =&gt; {Pulp Fiction}    0.31  0.7045455     0.44 1.499033    31\n\n\n\n# Find rules with highest lift\ninspect(head(sort(pulpfiction_rules_rhs, by=\"lift\"), 10))\n\n     lhs                             rhs            support confidence coverage     lift count\n[1]  {Fight Club,                                                                             \n      Silence of the Lambs, The}  =&gt; {Pulp Fiction}    0.34  0.9189189     0.37 1.955147    34\n[2]  {American Beauty,                                                                        \n      Silence of the Lambs, The}  =&gt; {Pulp Fiction}    0.31  0.8857143     0.35 1.884498    31\n[3]  {Shawshank Redemption, The,                                                              \n      Silence of the Lambs, The}  =&gt; {Pulp Fiction}    0.31  0.8857143     0.35 1.884498    31\n[4]  {Fight Club,                                                                             \n      Matrix, The}                =&gt; {Pulp Fiction}    0.30  0.8333333     0.36 1.773050    30\n[5]  {Seven (a.k.a. Se7en)}       =&gt; {Pulp Fiction}    0.30  0.8108108     0.37 1.725129    30\n[6]  {American Beauty,                                                                        \n      Matrix, The}                =&gt; {Pulp Fiction}    0.30  0.8108108     0.37 1.725129    30\n[7]  {Matrix, The,                                                                            \n      Silence of the Lambs, The}  =&gt; {Pulp Fiction}    0.32  0.8000000     0.40 1.702128    32\n[8]  {American Beauty,                                                                        \n      Fight Club}                 =&gt; {Pulp Fiction}    0.30  0.7894737     0.38 1.679731    30\n[9]  {Forrest Gump}               =&gt; {Pulp Fiction}    0.33  0.7857143     0.42 1.671733    33\n[10] {Silence of the Lambs, The}  =&gt; {Pulp Fiction}    0.38  0.7600000     0.50 1.617021    38",
    "crumbs": [
      "Conteúdo Semanal",
      "Análise de Associação"
    ]
  },
  {
    "objectID": "semanas/Aula14.html#extraindo-regras-com-um-filme-na-esquerda",
    "href": "semanas/Aula14.html#extraindo-regras-com-um-filme-na-esquerda",
    "title": "Análise de Associação",
    "section": "Extraindo regras com um filme na esquerda",
    "text": "Extraindo regras com um filme na esquerda\n\n# Extract rules with Pulp Fiction on the left side\npulpfiction_rules_lhs = apriori(filmes_trx, \n                           parameter = list(supp = 0.3,\n                                            conf = 0.5, \n                                            minlen = 2), \n                           appearance = list(\n                             default = \"rhs\",\n                             lhs = \"Pulp Fiction\")) \n\nApriori\n\nParameter specification:\n confidence minval smax arem  aval originalSupport maxtime support minlen\n        0.5    0.1    1 none FALSE            TRUE       5     0.3      2\n maxlen target  ext\n     10  rules TRUE\n\nAlgorithmic control:\n filter tree heap memopt load sort verbose\n    0.1 TRUE TRUE  FALSE TRUE    2    TRUE\n\nAbsolute minimum support count: 30 \n\nset item appearances ...[1 item(s)] done [0.00s].\nset transactions ...[4508 item(s), 100 transaction(s)] done [0.00s].\nsorting and recoding items ... [56 item(s)] done [0.00s].\ncreating transaction tree ... done [0.00s].\nchecking subsets of size 1 2 done [0.00s].\nwriting ... [12 rule(s)] done [0.00s].\ncreating S4 object  ... done [0.00s].\n\n# Inspect the first rules\ninspect(head(pulpfiction_rules_lhs))\n\n    lhs               rhs                                                  support confidence coverage     lift count\n[1] {Pulp Fiction} =&gt; {Schindler's List}                                      0.30  0.6382979     0.47 1.450677    30\n[2] {Pulp Fiction} =&gt; {Jurassic Park}                                         0.31  0.6595745     0.47 1.533894    31\n[3] {Pulp Fiction} =&gt; {Seven (a.k.a. Se7en)}                                  0.30  0.6382979     0.47 1.725129    30\n[4] {Pulp Fiction} =&gt; {Lord of the Rings: The Fellowship of the Ring, The}    0.31  0.6595745     0.47 1.465721    31\n[5] {Pulp Fiction} =&gt; {Sixth Sense, The}                                      0.31  0.6595745     0.47 1.499033    31\n[6] {Pulp Fiction} =&gt; {Forrest Gump}                                          0.33  0.7021277     0.47 1.671733    33",
    "crumbs": [
      "Conteúdo Semanal",
      "Análise de Associação"
    ]
  }
]