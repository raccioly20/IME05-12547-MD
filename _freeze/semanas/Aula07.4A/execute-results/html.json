{
  "hash": "b705acdd996318e0221801b83110e296",
  "result": {
    "markdown": "---\ntitle: \"Regularização de Modelos\"\nauthor: \"Ricardo Accioly\"\ndate: \"2023-09-15\"\nformat:\n html:\n    code-link: true\n---\n\n\n## Regularização de modelos\n\n## Carregando Bibliotecas\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(MASS)\nlibrary(tidyverse)\nlibrary(glmnet)\ndata(Boston)\n```\n:::\n\n\n## Carregando os dados\n\nVamos utilizar neste exemplo os dados contidos na biblioteca MASS. A base de dados Boston tem 506 de valores preços medianos de casas na região de Boston com 13 outras variáveis explicativas (potencialmente). Vamos explorar os dados e ajustar modelos com penalização o Ridge e o LASSO e depois vamos comparar com os mínimos quadrados.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(Boston)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     crim zn indus chas   nox    rm  age    dis rad tax ptratio  black lstat\n1 0.00632 18  2.31    0 0.538 6.575 65.2 4.0900   1 296    15.3 396.90  4.98\n2 0.02731  0  7.07    0 0.469 6.421 78.9 4.9671   2 242    17.8 396.90  9.14\n3 0.02729  0  7.07    0 0.469 7.185 61.1 4.9671   2 242    17.8 392.83  4.03\n4 0.03237  0  2.18    0 0.458 6.998 45.8 6.0622   3 222    18.7 394.63  2.94\n5 0.06905  0  2.18    0 0.458 7.147 54.2 6.0622   3 222    18.7 396.90  5.33\n6 0.02985  0  2.18    0 0.458 6.430 58.7 6.0622   3 222    18.7 394.12  5.21\n  medv\n1 24.0\n2 21.6\n3 34.7\n4 33.4\n5 36.2\n6 28.7\n```\n:::\n\n```{.r .cell-code}\nsummary(Boston)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      crim                zn             indus            chas        \n Min.   : 0.00632   Min.   :  0.00   Min.   : 0.46   Min.   :0.00000  \n 1st Qu.: 0.08205   1st Qu.:  0.00   1st Qu.: 5.19   1st Qu.:0.00000  \n Median : 0.25651   Median :  0.00   Median : 9.69   Median :0.00000  \n Mean   : 3.61352   Mean   : 11.36   Mean   :11.14   Mean   :0.06917  \n 3rd Qu.: 3.67708   3rd Qu.: 12.50   3rd Qu.:18.10   3rd Qu.:0.00000  \n Max.   :88.97620   Max.   :100.00   Max.   :27.74   Max.   :1.00000  \n      nox               rm             age              dis        \n Min.   :0.3850   Min.   :3.561   Min.   :  2.90   Min.   : 1.130  \n 1st Qu.:0.4490   1st Qu.:5.886   1st Qu.: 45.02   1st Qu.: 2.100  \n Median :0.5380   Median :6.208   Median : 77.50   Median : 3.207  \n Mean   :0.5547   Mean   :6.285   Mean   : 68.57   Mean   : 3.795  \n 3rd Qu.:0.6240   3rd Qu.:6.623   3rd Qu.: 94.08   3rd Qu.: 5.188  \n Max.   :0.8710   Max.   :8.780   Max.   :100.00   Max.   :12.127  \n      rad              tax           ptratio          black       \n Min.   : 1.000   Min.   :187.0   Min.   :12.60   Min.   :  0.32  \n 1st Qu.: 4.000   1st Qu.:279.0   1st Qu.:17.40   1st Qu.:375.38  \n Median : 5.000   Median :330.0   Median :19.05   Median :391.44  \n Mean   : 9.549   Mean   :408.2   Mean   :18.46   Mean   :356.67  \n 3rd Qu.:24.000   3rd Qu.:666.0   3rd Qu.:20.20   3rd Qu.:396.23  \n Max.   :24.000   Max.   :711.0   Max.   :22.00   Max.   :396.90  \n     lstat            medv      \n Min.   : 1.73   Min.   : 5.00  \n 1st Qu.: 6.95   1st Qu.:17.02  \n Median :11.36   Median :21.20  \n Mean   :12.65   Mean   :22.53  \n 3rd Qu.:16.95   3rd Qu.:25.00  \n Max.   :37.97   Max.   :50.00  \n```\n:::\n:::\n\n\nObservamos acima que todas as variáveis são quantitativas e que não há necessidade de transformações.\n\n## Significado das variáveis\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Boston Database\n# \n#1) crim - taxa de criminalidade per capita por cidade.\n# \n#2) zn - proporção de terrenos residenciais para lotes acima de 25,000 sq.ft.\n# \n#3) indus - proporção de negócios não comerciais por acres e por cidade.\n# \n#4) chas - variável dummy do Rio Charles(= 1 se próximo do rio; 0 de outra forma).\n# \n#5) nox - concentração de óxido de nitrogênio (partes por 10 milhões).\n# \n#6) rm - número médio de quartos por habitação\n# \n#7) age - proporção da unidade ocupadas pelos proprietários construídas antes 1940.\n# \n#8) dis - média ponderada das distâncias dos 5 pontos de emprego em Boston.\n# \n#9) rad - indice de acessibilidade das avenidas radiais.\n# \n#10) tax - valor cheio da taxa de propriedade por $10,000.\n# \n#11) ptratio - razão aluno-professor por cidade.\n# \n#12) black - 1000(Bk−0.63)21000(Bk−0.63)2 proporção de negros por cidade.\n# \n#13) lstat - percentual de baixo status da população.\n# \n#14) medv - valor mediano das cas ocupadas pelos proprietário em $1000s. (Var. Resposta)\n```\n:::\n\n\n## Conjunto de treino e de teste\n\nObservar que retiramos a variável **rad**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(caret)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nCarregando pacotes exigidos: lattice\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'caret'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:purrr':\n\n    lift\n```\n:::\n\n```{.r .cell-code}\nset.seed(21)\ny <- Boston$medv\nindice_teste <- createDataPartition(y, times = 1, p = 0.2, list = FALSE)\n\n\nconj_treino <- Boston[-indice_teste, ]\nconj_teste <- Boston[indice_teste, ]\n\nstr(conj_treino)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'data.frame':\t403 obs. of  14 variables:\n $ crim   : num  0.00632 0.02731 0.02729 0.03237 0.06905 ...\n $ zn     : num  18 0 0 0 0 0 12.5 12.5 12.5 12.5 ...\n $ indus  : num  2.31 7.07 7.07 2.18 2.18 2.18 7.87 7.87 7.87 7.87 ...\n $ chas   : int  0 0 0 0 0 0 0 0 0 0 ...\n $ nox    : num  0.538 0.469 0.469 0.458 0.458 0.458 0.524 0.524 0.524 0.524 ...\n $ rm     : num  6.58 6.42 7.18 7 7.15 ...\n $ age    : num  65.2 78.9 61.1 45.8 54.2 58.7 66.6 96.1 85.9 94.3 ...\n $ dis    : num  4.09 4.97 4.97 6.06 6.06 ...\n $ rad    : int  1 2 2 3 3 3 5 5 5 5 ...\n $ tax    : num  296 242 242 222 222 222 311 311 311 311 ...\n $ ptratio: num  15.3 17.8 17.8 18.7 18.7 18.7 15.2 15.2 15.2 15.2 ...\n $ black  : num  397 397 393 395 397 ...\n $ lstat  : num  4.98 9.14 4.03 2.94 5.33 ...\n $ medv   : num  24 21.6 34.7 33.4 36.2 28.7 22.9 27.1 18.9 15 ...\n```\n:::\n\n```{.r .cell-code}\nstr(conj_teste)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'data.frame':\t103 obs. of  14 variables:\n $ crim   : num  0.211 0.63 0.627 1.252 0.852 ...\n $ zn     : num  12.5 0 0 0 0 0 0 75 0 0 ...\n $ indus  : num  7.87 8.14 8.14 8.14 8.14 8.14 8.14 2.95 6.91 6.91 ...\n $ chas   : int  0 0 0 0 0 0 0 0 0 0 ...\n $ nox    : num  0.524 0.538 0.538 0.538 0.538 0.538 0.538 0.428 0.448 0.448 ...\n $ rm     : num  5.63 5.95 5.83 5.57 5.96 ...\n $ age    : num  100 61.8 56.5 98.1 89.2 94.1 96.9 21.8 6.5 95.3 ...\n $ dis    : num  6.08 4.71 4.5 3.8 4.01 ...\n $ rad    : int  5 4 4 4 4 4 4 3 3 3 ...\n $ tax    : num  311 307 307 307 307 307 307 252 233 233 ...\n $ ptratio: num  15.2 21 21 21 21 21 21 18.3 17.9 17.9 ...\n $ black  : num  387 397 396 377 393 ...\n $ lstat  : num  29.93 8.26 8.47 21.02 13.83 ...\n $ medv   : num  16.5 20.4 19.9 13.6 19.6 12.7 13.5 30.8 24.7 14.4 ...\n```\n:::\n:::\n\n\n## Métodos de Regularização com o caret\n\nUsando o caret para selecionar o melhor modelo\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_control <- trainControl(\n  method = \"repeatedcv\",\n  number = 10,  # validação cruzada com 10 folds\n  repeats = 5,  # 5 repetições\n  savePredictions = \"final\"  # salva os resultados do modelo final\n)\n```\n:::\n\n\n## Cross-Validation no caret\n\nNós podemos usar o k-fold cross validation para identificar o melhor valor de $\\lambda$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(21)\nmdl_ridge <- train(\n  medv ~ .,\n  data = conj_treino,\n  method = \"glmnet\",\n  metric = \"RMSE\",  \n  preProcess = c(\"center\", \"scale\"),\n  tuneGrid = expand.grid(\n    .alpha = 0,  # regressão ridge\n    .lambda = seq(0, 5, length.out = 201)\n  ),\n  trControl = train_control\n  )\nmdl_ridge\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nglmnet \n\n403 samples\n 13 predictor\n\nPre-processing: centered (13), scaled (13) \nResampling: Cross-Validated (10 fold, repeated 5 times) \nSummary of sample sizes: 363, 362, 364, 362, 363, 363, ... \nResampling results across tuning parameters:\n\n  lambda  RMSE      Rsquared   MAE     \n  0.000   4.591424  0.7558789  3.240456\n  0.025   4.591424  0.7558789  3.240456\n  0.050   4.591424  0.7558789  3.240456\n  0.075   4.591424  0.7558789  3.240456\n  0.100   4.591424  0.7558789  3.240456\n  0.125   4.591424  0.7558789  3.240456\n  0.150   4.591424  0.7558789  3.240456\n  0.175   4.591424  0.7558789  3.240456\n  0.200   4.591424  0.7558789  3.240456\n  0.225   4.591424  0.7558789  3.240456\n  0.250   4.591424  0.7558789  3.240456\n  0.275   4.591424  0.7558789  3.240456\n  0.300   4.591424  0.7558789  3.240456\n  0.325   4.591424  0.7558789  3.240456\n  0.350   4.591424  0.7558789  3.240456\n  0.375   4.591424  0.7558789  3.240456\n  0.400   4.591424  0.7558789  3.240456\n  0.425   4.591424  0.7558789  3.240456\n  0.450   4.591424  0.7558789  3.240456\n  0.475   4.591424  0.7558789  3.240456\n  0.500   4.591424  0.7558789  3.240456\n  0.525   4.591424  0.7558789  3.240456\n  0.550   4.591424  0.7558789  3.240456\n  0.575   4.591424  0.7558789  3.240456\n  0.600   4.591424  0.7558789  3.240456\n  0.625   4.591424  0.7558789  3.240456\n  0.650   4.591424  0.7558789  3.240456\n  0.675   4.591754  0.7558700  3.240564\n  0.700   4.593034  0.7558270  3.240966\n  0.725   4.594219  0.7557743  3.241145\n  0.750   4.595335  0.7557289  3.241280\n  0.775   4.596477  0.7556822  3.241479\n  0.800   4.597661  0.7556336  3.241770\n  0.825   4.598871  0.7555829  3.242170\n  0.850   4.600107  0.7555309  3.242642\n  0.875   4.601382  0.7554771  3.243139\n  0.900   4.602683  0.7554213  3.243659\n  0.925   4.604004  0.7553644  3.244240\n  0.950   4.605355  0.7553067  3.244914\n  0.975   4.606742  0.7552464  3.245641\n  1.000   4.608144  0.7551849  3.246464\n  1.025   4.609569  0.7551225  3.247308\n  1.050   4.611029  0.7550584  3.248162\n  1.075   4.612514  0.7549924  3.249046\n  1.100   4.614010  0.7549258  3.249940\n  1.125   4.615521  0.7548592  3.250892\n  1.150   4.617064  0.7547908  3.251857\n  1.175   4.618632  0.7547205  3.252829\n  1.200   4.620208  0.7546495  3.253782\n  1.225   4.621797  0.7545780  3.254780\n  1.250   4.623409  0.7545059  3.255825\n  1.275   4.625053  0.7544316  3.256919\n  1.300   4.626705  0.7543565  3.257997\n  1.325   4.628367  0.7542808  3.259062\n  1.350   4.630037  0.7542054  3.260123\n  1.375   4.631731  0.7541289  3.261207\n  1.400   4.633455  0.7540504  3.262310\n  1.425   4.635185  0.7539712  3.263417\n  1.450   4.636923  0.7538913  3.264517\n  1.475   4.638669  0.7538114  3.265616\n  1.500   4.640433  0.7537312  3.266724\n  1.525   4.642226  0.7536489  3.267834\n  1.550   4.644035  0.7535654  3.268960\n  1.575   4.645843  0.7534819  3.270101\n  1.600   4.647661  0.7533974  3.271256\n  1.625   4.649482  0.7533139  3.272395\n  1.650   4.651322  0.7532296  3.273553\n  1.675   4.653189  0.7531433  3.274736\n  1.700   4.655069  0.7530559  3.275916\n  1.725   4.656946  0.7529687  3.277084\n  1.750   4.658833  0.7528806  3.278271\n  1.775   4.660722  0.7527930  3.279477\n  1.800   4.662626  0.7527051  3.280703\n  1.825   4.664554  0.7526155  3.281969\n  1.850   4.666503  0.7525243  3.283268\n  1.875   4.668453  0.7524328  3.284553\n  1.900   4.670402  0.7523415  3.285821\n  1.925   4.672359  0.7522494  3.287096\n  1.950   4.674318  0.7521579  3.288352\n  1.975   4.676289  0.7520663  3.289613\n  2.000   4.678283  0.7519728  3.290903\n  2.025   4.680298  0.7518778  3.292244\n  2.050   4.682317  0.7517822  3.293602\n  2.075   4.684331  0.7516868  3.294951\n  2.100   4.686353  0.7515906  3.296305\n  2.125   4.688372  0.7514949  3.297661\n  2.150   4.690395  0.7513998  3.299024\n  2.175   4.692434  0.7513038  3.300403\n  2.200   4.694495  0.7512061  3.301784\n  2.225   4.696571  0.7511071  3.303178\n  2.250   4.698650  0.7510077  3.304590\n  2.275   4.700721  0.7509088  3.305990\n  2.300   4.702799  0.7508094  3.307411\n  2.325   4.704874  0.7507101  3.308865\n  2.350   4.706950  0.7506115  3.310328\n  2.375   4.709037  0.7505127  3.311799\n  2.400   4.711142  0.7504125  3.313273\n  2.425   4.713265  0.7503108  3.314749\n  2.450   4.715398  0.7502082  3.316229\n  2.475   4.717528  0.7501055  3.317700\n  2.500   4.719652  0.7500032  3.319175\n  2.525   4.721783  0.7499002  3.320687\n  2.550   4.723910  0.7497975  3.322253\n  2.575   4.726037  0.7496953  3.323829\n  2.600   4.728172  0.7495932  3.325414\n  2.625   4.730321  0.7494900  3.326995\n  2.650   4.732489  0.7493852  3.328578\n  2.675   4.734669  0.7492794  3.330181\n  2.700   4.736852  0.7491732  3.331796\n  2.725   4.739028  0.7490670  3.333410\n  2.750   4.741201  0.7489614  3.335030\n  2.775   4.743378  0.7488550  3.336654\n  2.800   4.745549  0.7487493  3.338275\n  2.825   4.747721  0.7486440  3.339886\n  2.850   4.749898  0.7485388  3.341516\n  2.875   4.752088  0.7484327  3.343150\n  2.900   4.754294  0.7483251  3.344805\n  2.925   4.756514  0.7482164  3.346458\n  2.950   4.758740  0.7481070  3.348105\n  2.975   4.760961  0.7479977  3.349739\n  3.000   4.763176  0.7478887  3.351365\n  3.025   4.765391  0.7477798  3.353009\n  3.050   4.767607  0.7476702  3.354655\n  3.075   4.769815  0.7475616  3.356290\n  3.100   4.772023  0.7474532  3.357925\n  3.125   4.774236  0.7473451  3.359554\n  3.150   4.776460  0.7472361  3.361204\n  3.175   4.778699  0.7471257  3.362875\n  3.200   4.780950  0.7470143  3.364577\n  3.225   4.783210  0.7469021  3.366286\n  3.250   4.785469  0.7467896  3.368017\n  3.275   4.787721  0.7466772  3.369738\n  3.300   4.789966  0.7465654  3.371445\n  3.325   4.792212  0.7464534  3.373148\n  3.350   4.794457  0.7463410  3.374845\n  3.375   4.796691  0.7462298  3.376527\n  3.400   4.798925  0.7461189  3.378206\n  3.425   4.801163  0.7460082  3.379893\n  3.450   4.803410  0.7458967  3.381606\n  3.475   4.805668  0.7457842  3.383339\n  3.500   4.807939  0.7456705  3.385079\n  3.525   4.810220  0.7455559  3.386819\n  3.550   4.812506  0.7454408  3.388554\n  3.575   4.814785  0.7453258  3.390283\n  3.600   4.817058  0.7452110  3.392012\n  3.625   4.819324  0.7450968  3.393728\n  3.650   4.821593  0.7449822  3.395449\n  3.675   4.823858  0.7448675  3.397177\n  3.700   4.826114  0.7447538  3.398890\n  3.725   4.828367  0.7446404  3.400592\n  3.750   4.830623  0.7445272  3.402287\n  3.775   4.832886  0.7444135  3.403979\n  3.800   4.835157  0.7442992  3.405667\n  3.825   4.837440  0.7441836  3.407362\n  3.850   4.839733  0.7440671  3.409063\n  3.875   4.842034  0.7439498  3.410779\n  3.900   4.844337  0.7438321  3.412495\n  3.925   4.846631  0.7437148  3.414202\n  3.950   4.848918  0.7435976  3.415907\n  3.975   4.851199  0.7434811  3.417606\n  4.000   4.853482  0.7433643  3.419310\n  4.025   4.855761  0.7432473  3.421013\n  4.050   4.858030  0.7431310  3.422705\n  4.075   4.860294  0.7430154  3.424384\n  4.100   4.862558  0.7428999  3.426064\n  4.125   4.864825  0.7427847  3.427737\n  4.150   4.867099  0.7426688  3.429416\n  4.175   4.869381  0.7425522  3.431109\n  4.200   4.871673  0.7424343  3.432804\n  4.225   4.873974  0.7423157  3.434502\n  4.250   4.876281  0.7421964  3.436202\n  4.275   4.878591  0.7420766  3.437904\n  4.300   4.880894  0.7419572  3.439605\n  4.325   4.883189  0.7418378  3.441299\n  4.350   4.885478  0.7417192  3.442994\n  4.375   4.887762  0.7416007  3.444682\n  4.400   4.890047  0.7414819  3.446366\n  4.425   4.892327  0.7413631  3.448046\n  4.450   4.894595  0.7412455  3.449709\n  4.475   4.896859  0.7411282  3.451370\n  4.500   4.899123  0.7410111  3.453027\n  4.525   4.901389  0.7408942  3.454676\n  4.550   4.903662  0.7407766  3.456325\n  4.575   4.905940  0.7406586  3.457977\n  4.600   4.908228  0.7405392  3.459643\n  4.625   4.910523  0.7404193  3.461313\n  4.650   4.912826  0.7402985  3.462983\n  4.675   4.915131  0.7401773  3.464652\n  4.700   4.917437  0.7400559  3.466319\n  4.725   4.919732  0.7399349  3.467993\n  4.750   4.922022  0.7398140  3.469659\n  4.775   4.924305  0.7396939  3.471318\n  4.800   4.926583  0.7395740  3.472980\n  4.825   4.928862  0.7394537  3.474639\n  4.850   4.931136  0.7393335  3.476288\n  4.875   4.933398  0.7392141  3.477925\n  4.900   4.935654  0.7390955  3.479562\n  4.925   4.937908  0.7389770  3.481191\n  4.950   4.940162  0.7388587  3.482812\n  4.975   4.942420  0.7387403  3.484437\n  5.000   4.944683  0.7386213  3.486066\n\nTuning parameter 'alpha' was held constant at a value of 0\nRMSE was used to select the optimal model using the smallest value.\nThe final values used for the model were alpha = 0 and lambda = 0.65.\n```\n:::\n\n```{.r .cell-code}\nggplot(mdl_ridge) +\n  labs(title = \"Regressão Ridge Ajuste do Parametro\", x = \"lambda\")\n```\n\n::: {.cell-output-display}\n![](Aula07.4A_files/figure-html/r4-1.png){width=672}\n:::\n\n```{.r .cell-code}\nplot(varImp(mdl_ridge))\n```\n\n::: {.cell-output-display}\n![](Aula07.4A_files/figure-html/r4-2.png){width=672}\n:::\n:::\n\n\n## Avaliando o modelo com o conjunto de teste\n\nAqui usamos a reamostragem com os dados de teste para avaliar o modelo\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Metricas de desempenho\npr_ridge <- postResample(pred = predict(mdl_ridge, newdata = conj_teste), obs = conj_teste$medv)\npr_ridge\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     RMSE  Rsquared       MAE \n5.8656795 0.5934055 3.6947963 \n```\n:::\n:::\n\n\n## Modelo Final Ridge\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(21)\nmdl_final_ridge <- train(\n  medv ~ .,\n  data = conj_treino,\n  method = \"glmnet\",\n  metric = \"RMSE\",\n  preProcess = c(\"center\", \"scale\"),\n  tuneGrid = data.frame(\n    .alpha = mdl_ridge$bestTune$alpha,  # hiperparametro otimizado\n    .lambda = mdl_ridge$bestTune$lambda),  # hiperparametro otimizado\n  trControl = train_control\n  )\nmdl_final_ridge\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nglmnet \n\n403 samples\n 13 predictor\n\nPre-processing: centered (13), scaled (13) \nResampling: Cross-Validated (10 fold, repeated 5 times) \nSummary of sample sizes: 363, 362, 364, 362, 363, 363, ... \nResampling results:\n\n  RMSE      Rsquared   MAE     \n  4.591424  0.7558789  3.240456\n\nTuning parameter 'alpha' was held constant at a value of 0\nTuning\n parameter 'lambda' was held constant at a value of 0.65\n```\n:::\n\n```{.r .cell-code}\nsqrt(mean((conj_teste$medv - predict(mdl_final_ridge, newdata = conj_teste)) ^ 2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 5.865679\n```\n:::\n:::\n\n\n## LASSO\n\n## Validação Cruzada no LASSO\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(21)\nmdl_lasso <- train(\n  medv ~ .,\n  data = conj_treino,\n  method = \"glmnet\",\n  metric = \"RMSE\",\n  preProcess = c(\"center\", \"scale\"),\n  tuneGrid = expand.grid(\n    .alpha = 1,  # regressão lasso\n    .lambda = seq(0, 5, length.out = 201)\n  ),\n  trControl = train_control\n  )\nmdl_lasso$bestTune\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  alpha lambda\n1     1      0\n```\n:::\n\n```{.r .cell-code}\nggplot(mdl_lasso) +\n  labs(title = \"Regressão Lasso - Ajuste de parametro\", x = \"lambda\")\n```\n\n::: {.cell-output-display}\n![](Aula07.4A_files/figure-html/Lasso2-1.png){width=672}\n:::\n:::\n\n\n## Avaliando com conjunto de teste\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Metricas de desempenho\npr_lasso <- postResample(pred = predict(mdl_lasso, newdata = conj_teste), obs = conj_teste$medv)\npr_lasso\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     RMSE  Rsquared       MAE \n5.7958980 0.6057383 3.7273820 \n```\n:::\n:::\n\n\n## Modelo Final Lasso\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(21)\nmdl_final_lasso <- train(\n  medv ~ .,\n  data = conj_treino,\n  method = \"glmnet\",\n  metric = \"RMSE\",\n  preProcess = c(\"center\", \"scale\"),\n  tuneGrid = data.frame(\n    .alpha = mdl_lasso$bestTune$alpha,  # hiperparametro otimizado\n    .lambda = mdl_lasso$bestTune$lambda),  # hiperparametro otimizado\n  trControl = train_control\n  )\nmdl_final_lasso\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nglmnet \n\n403 samples\n 13 predictor\n\nPre-processing: centered (13), scaled (13) \nResampling: Cross-Validated (10 fold, repeated 5 times) \nSummary of sample sizes: 363, 362, 364, 362, 363, 363, ... \nResampling results:\n\n  RMSE      Rsquared  MAE     \n  4.590922  0.755213  3.279609\n\nTuning parameter 'alpha' was held constant at a value of 1\nTuning\n parameter 'lambda' was held constant at a value of 0\n```\n:::\n\n```{.r .cell-code}\nsqrt(mean((conj_teste$medv - predict(mdl_final_lasso, newdata = conj_teste)) ^ 2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 5.795898\n```\n:::\n:::\n",
    "supporting": [
      "Aula07.4A_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}