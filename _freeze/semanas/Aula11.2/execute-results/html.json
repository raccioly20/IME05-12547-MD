{
  "hash": "23c911fbad3331e35ec8a406448aa40e",
  "result": {
    "markdown": "---\ntitle: 'Arvores de Regressão - Gradient Boosting '\nauthor: \"Ricardo Accioly\"\ndate: \"2023-05-23\"\nformat:\n html:\n    code-link: true\n---\n\n\n\n\n## Bibliotecas\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(MASS)\nlibrary(tidyverse)\n```\n:::\n\n\n## Avaliando, selecionando dados\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(\"Boston\")\nnames(Boston)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] \"crim\"    \"zn\"      \"indus\"   \"chas\"    \"nox\"     \"rm\"      \"age\"    \n [8] \"dis\"     \"rad\"     \"tax\"     \"ptratio\" \"black\"   \"lstat\"   \"medv\"   \n```\n:::\n\n```{.r .cell-code}\ndados <- Boston \n```\n:::\n\n\n## Treino e Teste com todas as variáveis\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Vamos criar os conjuntos de treino teste e desenvolver a arvore \n## com todas as variáveis.\nlibrary(caret)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nCarregando pacotes exigidos: lattice\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'caret'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:purrr':\n\n    lift\n```\n:::\n\n```{.r .cell-code}\nset.seed(21)\nindice <- createDataPartition(dados$medv, times=1, p=0.75, list=FALSE)\nconj_treino <- dados[indice,]\nconj_teste <- dados[-indice,]\nhead(conj_treino)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     crim   zn indus chas   nox    rm  age    dis rad tax ptratio  black lstat\n1 0.00632 18.0  2.31    0 0.538 6.575 65.2 4.0900   1 296    15.3 396.90  4.98\n3 0.02729  0.0  7.07    0 0.469 7.185 61.1 4.9671   2 242    17.8 392.83  4.03\n4 0.03237  0.0  2.18    0 0.458 6.998 45.8 6.0622   3 222    18.7 394.63  2.94\n5 0.06905  0.0  2.18    0 0.458 7.147 54.2 6.0622   3 222    18.7 396.90  5.33\n6 0.02985  0.0  2.18    0 0.458 6.430 58.7 6.0622   3 222    18.7 394.12  5.21\n7 0.08829 12.5  7.87    0 0.524 6.012 66.6 5.5605   5 311    15.2 395.60 12.43\n  medv\n1 24.0\n3 34.7\n4 33.4\n5 36.2\n6 28.7\n7 22.9\n```\n:::\n\n```{.r .cell-code}\nhead(conj_teste)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      crim   zn indus chas   nox    rm  age    dis rad tax ptratio  black lstat\n2  0.02731  0.0  7.07    0 0.469 6.421 78.9 4.9671   2 242    17.8 396.90  9.14\n10 0.17004 12.5  7.87    0 0.524 6.004 85.9 6.5921   5 311    15.2 386.71 17.10\n12 0.11747 12.5  7.87    0 0.524 6.009 82.9 6.2267   5 311    15.2 396.90 13.27\n16 0.62739  0.0  8.14    0 0.538 5.834 56.5 4.4986   4 307    21.0 395.62  8.47\n19 0.80271  0.0  8.14    0 0.538 5.456 36.6 3.7965   4 307    21.0 288.99 11.69\n23 1.23247  0.0  8.14    0 0.538 6.142 91.7 3.9769   4 307    21.0 396.90 18.72\n   medv\n2  21.6\n10 18.9\n12 18.9\n16 19.9\n19 20.2\n23 15.2\n```\n:::\n:::\n\n\n## 1a tentativa GBM\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(gbm)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoaded gbm 2.1.8.1\n```\n:::\n\n```{.r .cell-code}\nset.seed(21)\n# treinar o modelo GBM\ngbm.fit <- gbm(formula = medv ~ .,\n                distribution = \"gaussian\", #  minimizar erro quadrático\n                data = conj_treino,\n                n.trees = 10000,  # número de árvores\n                interaction.depth = 3,  # profundidade da arvore\n                shrinkage = 0.1,   # aprendizado rápido\n                cv.folds = 5, # 5 envelopes de validaçõa cruzada\n                n.cores = NULL, # \n                verbose = FALSE)\n\n# Achar o índice do modelo com menor erro de valiação cruzada\nmin_MSE <- which.min(gbm.fit$cv.error)\n\n# Obter o MSE e calcular o RMSE\nsqrt(gbm.fit$cv.error[min_MSE])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 3.385086\n```\n:::\n\n```{.r .cell-code}\n# Mostrar a função de perda (loss function) como resultado do número de árvores combinadas\n# A linha preta são os valores para as perdas de treino e a verde as de teste\n# A linha azul pontilhada indica o número de árvores que minimiza os erros da validação cruzada\ngbm.perf(gbm.fit, method = \"cv\")\n```\n\n::: {.cell-output-display}\n![](Aula11.2_files/figure-html/gbm1-1.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 497\n```\n:::\n:::\n\n\n## Criando um grid para avaliar os parametros e os respectivos RMSEs\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhiper_grid <- expand.grid(\n  shrinkage = c(.01, .05, .1),\n  interaction.depth = c(1, 3, 5, 7),\n  n.minobsinnode = c(5, 10, 15),\n  bag.fraction = c(.65, .8, 1),\n  optimal_trees = 0, \n  min_RMSE = 0   \n  )\n\n# numero total de combinações\nnrow(hiper_grid)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 108\n```\n:::\n:::\n\n\n## Avaliando o grid de parametros\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Busca no grid \nfor(i in 1:nrow(hiper_grid)) {\n  \n  # \n  set.seed(21)\n  \n  # treina o modelo\n  gbm.tune <- gbm(\n    formula = medv ~ .,\n    distribution = \"gaussian\",\n    data = conj_treino,\n    n.trees = 6000,\n    interaction.depth = hiper_grid$interaction.depth[i],\n    shrinkage = hiper_grid$shrinkage[i],\n    n.minobsinnode = hiper_grid$n.minobsinnode[i],\n    bag.fraction = hiper_grid$bag.fraction[i],\n    train.fraction = .75,\n    n.cores = NULL, \n    verbose = FALSE\n  )\n  \n # adiciona os erros de treino e arvores ao grid\n  hiper_grid$optimal_trees[i] <- which.min(gbm.tune$valid.error)\n  hiper_grid$min_RMSE[i] <- sqrt(min(gbm.tune$valid.error))\n}\n\nhiper_grid %>% dplyr::arrange(min_RMSE) %>% head(10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   shrinkage interaction.depth n.minobsinnode bag.fraction optimal_trees\n1       0.10                 1             15         0.65          3074\n2       0.10                 1             10         0.65          5262\n3       0.05                 1             15         0.80          4392\n4       0.10                 1             15         0.80          3321\n5       0.05                 1             15         0.65          3074\n6       0.05                 1             10         1.00          4455\n7       0.10                 1             10         1.00          2073\n8       0.10                 1             15         1.00          4124\n9       0.05                 1             15         1.00          5996\n10      0.05                 5             15         0.65           178\n   min_RMSE\n1  4.420519\n2  4.655791\n3  4.667658\n4  4.686840\n5  4.699871\n6  4.775397\n7  4.780322\n8  4.788251\n9  4.818321\n10 4.818439\n```\n:::\n:::\n\n\n## Modelo final\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# \nset.seed(21)\n\n# treina o modelo GBM\ngbm.fit.final <- gbm(\n  formula = medv ~ .,\n  distribution = \"gaussian\",\n  data = conj_treino,\n  n.trees = 3074,\n  interaction.depth = 1,\n  shrinkage = 0.10,\n  n.minobsinnode = 15,\n  bag.fraction = 0.65, \n  train.fraction = 1,\n  n.cores = NULL, \n  verbose = FALSE\n  )  \n```\n:::\n\n\n## Variable importance\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(\n  gbm.fit.final, \n  cBars = 13,\n  method = relative.influence, # também pode ser usado permutation.test.gbm\n  las = 2\n  )\n```\n\n::: {.cell-output-display}\n![](Aula11.2_files/figure-html/vi-1.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n            var    rel.inf\nrm           rm 34.2998187\nlstat     lstat 29.4374141\ndis         dis  8.3574770\nnox         nox  5.9614131\ncrim       crim  5.5011677\nchas       chas  4.5023266\nptratio ptratio  3.5157834\nblack     black  3.4315023\nage         age  2.3514968\ntax         tax  1.4830798\nindus     indus  0.6084729\nrad         rad  0.4554056\nzn           zn  0.0946421\n```\n:::\n:::\n\n\n## Previsão\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fazendo Previsões\nprevisao1 <- predict(gbm.fit.final, \n                     newdata = conj_teste,\n                     n.trees=gbm.fit.final$n.trees)\nhead(previsao1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 19.73374 18.76262 19.89281 20.76348 18.34094 16.44815\n```\n:::\n\n```{.r .cell-code}\n# Calcula os erros de previsão\ncaret::RMSE(previsao1, conj_teste$medv)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 4.189849\n```\n:::\n:::\n\n\n## Entendendo melhor os resultados\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(lime)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'lime'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:dplyr':\n\n    explain\n```\n:::\n\n```{.r .cell-code}\nmodel_type.gbm <- function(x, ...) {\n  return(\"regression\")\n}\n\npredict_model.gbm <- function(x, newdata, ...) {\n  pred <- predict(x, newdata, n.trees = x$n.trees)\n  return(as.data.frame(pred))\n}\n# Algumas observações para avaliar\nobs_pontuais <- conj_teste[1:2, ]\n\n# aplica o LIME\nexplicador <- lime(conj_treino, gbm.fit.final)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: chas does not contain enough variance to use quantile binning. Using\nstandard binning instead.\n```\n:::\n\n```{.r .cell-code}\nexplicacao <- explain(obs_pontuais, explicador, n_features = 5)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in gower_work(x = x, y = y, pair_x = pair_x, pair_y = pair_y, n = NULL,\n: skipping variable with zero or non-finite range\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in gower_work(x = x, y = y, pair_x = pair_x, pair_y = pair_y, n = NULL,\n: skipping variable with zero or non-finite range\n```\n:::\n\n```{.r .cell-code}\nplot_features(explicacao)\n```\n\n::: {.cell-output-display}\n![](Aula11.2_files/figure-html/LIME-1.png){width=672}\n:::\n:::\n\n\n## Gráfico de Dependencia Parcial (Partial Dependence Plot)\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngraf_rm <- plot(gbm.fit.final, i = \"rm\")\ngraf_lstat <- plot(gbm.fit.final, i = \"lstat\")\ngridExtra::grid.arrange(graf_lstat, graf_rm, ncol = 2)\n```\n\n::: {.cell-output-display}\n![](Aula11.2_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n",
    "supporting": [
      "Aula11.2_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}