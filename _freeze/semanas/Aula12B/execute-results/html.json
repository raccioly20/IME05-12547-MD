{
  "hash": "4f72898bc68ce86f3d6f0ae08378926c",
  "result": {
    "markdown": "---\ntitle: \"Arvores de Regressão\"\nauthor: \"Ricardo Accioly\"\ndate: \"2022-08-25\"\noutput:\n  word_document:\n    toc: yes\n  html_document:\n    toc: yes\n---\n\n\n\n\n\n## Bibliotecas\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(MASS)\nlibrary(tidyverse)\nlibrary(psych)\n```\n:::\n\n\n\n## Avaliando, selecionando dados\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(\"Boston\")\nnames(Boston)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] \"crim\"    \"zn\"      \"indus\"   \"chas\"    \"nox\"     \"rm\"      \"age\"    \n [8] \"dis\"     \"rad\"     \"tax\"     \"ptratio\" \"black\"   \"lstat\"   \"medv\"   \n```\n:::\n\n```{.r .cell-code}\ndescribe(Boston)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        vars   n   mean     sd median trimmed    mad    min    max  range  skew\ncrim       1 506   3.61   8.60   0.26    1.68   0.33   0.01  88.98  88.97  5.19\nzn         2 506  11.36  23.32   0.00    5.08   0.00   0.00 100.00 100.00  2.21\nindus      3 506  11.14   6.86   9.69   10.93   9.37   0.46  27.74  27.28  0.29\nchas       4 506   0.07   0.25   0.00    0.00   0.00   0.00   1.00   1.00  3.39\nnox        5 506   0.55   0.12   0.54    0.55   0.13   0.38   0.87   0.49  0.72\nrm         6 506   6.28   0.70   6.21    6.25   0.51   3.56   8.78   5.22  0.40\nage        7 506  68.57  28.15  77.50   71.20  28.98   2.90 100.00  97.10 -0.60\ndis        8 506   3.80   2.11   3.21    3.54   1.91   1.13  12.13  11.00  1.01\nrad        9 506   9.55   8.71   5.00    8.73   2.97   1.00  24.00  23.00  1.00\ntax       10 506 408.24 168.54 330.00  400.04 108.23 187.00 711.00 524.00  0.67\nptratio   11 506  18.46   2.16  19.05   18.66   1.70  12.60  22.00   9.40 -0.80\nblack     12 506 356.67  91.29 391.44  383.17   8.09   0.32 396.90 396.58 -2.87\nlstat     13 506  12.65   7.14  11.36   11.90   7.11   1.73  37.97  36.24  0.90\nmedv      14 506  22.53   9.20  21.20   21.56   5.93   5.00  50.00  45.00  1.10\n        kurtosis   se\ncrim       36.60 0.38\nzn          3.95 1.04\nindus      -1.24 0.30\nchas        9.48 0.01\nnox        -0.09 0.01\nrm          1.84 0.03\nage        -0.98 1.25\ndis         0.46 0.09\nrad        -0.88 0.39\ntax        -1.15 7.49\nptratio    -0.30 0.10\nblack       7.10 4.06\nlstat       0.46 0.32\nmedv        1.45 0.41\n```\n:::\n\n```{.r .cell-code}\ndados <- Boston \nextrato <- dados %>% select(medv, nox, rm)  \nsummary(extrato)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      medv            nox               rm       \n Min.   : 5.00   Min.   :0.3850   Min.   :3.561  \n 1st Qu.:17.02   1st Qu.:0.4490   1st Qu.:5.886  \n Median :21.20   Median :0.5380   Median :6.208  \n Mean   :22.53   Mean   :0.5547   Mean   :6.285  \n 3rd Qu.:25.00   3rd Qu.:0.6240   3rd Qu.:6.623  \n Max.   :50.00   Max.   :0.8710   Max.   :8.780  \n```\n:::\n\n```{.r .cell-code}\nboxplot(extrato$medv)\n```\n\n::: {.cell-output-display}\n![](Aula12B_files/figure-html/dados-1.png){width=672}\n:::\n:::\n\n\n\n## Visualizando os dados\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Distribuição de dados na maior parte simétrica com valores na cauda direta \n## maior do que o esperado para uam distribuição simétrica\nggplot(extrato, aes(x=medv)) +\n  geom_histogram(bins = round(1+3.322*log10(nrow(extrato)),0))\n```\n\n::: {.cell-output-display}\n![](Aula12B_files/figure-html/ggplot-1.png){width=672}\n:::\n\n```{.r .cell-code}\n## Grafico de dispersão nox vs rm \nggplot(extrato, aes(x=rm, y=nox)) + \n  geom_point()\n```\n\n::: {.cell-output-display}\n![](Aula12B_files/figure-html/ggplot-2.png){width=672}\n:::\n:::\n\n\n\n## Arvore de Regressão\n\nNa biblioteca rpart as arvores de regressão são obtidas usando o método anova. Existem alguns controles que podem ser feitos nos parametros da arvore. \n\nNeste exemplo só definimos o menor conjunto de dados numa partição (minsplit) e o parametro de complexidade cp. Posteriormente vamos ampliar este controle.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n##Usando rpart para desenvolver a arvore  \nlibrary(rpart)\narvreg <- rpart(medv ~ ., \n                data=extrato,\n                method=\"anova\", #para arvore de regressão\n                control=rpart.control(minsplit=30,cp=0.06))\nplot(arvreg)\ntext(arvreg,pretty=0)\n```\n\n::: {.cell-output-display}\n![](Aula12B_files/figure-html/arvore-1.png){width=672}\n:::\n:::\n\n\n## Segmentos \n\nA partir da árvore obtida no item anterior podemos fazer uma representação gráfica das partições obtidas.   \n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(extrato, aes(x=rm, y=nox)) + \n  geom_point() +\n  geom_segment(aes(x = 0, y = 0.6695, xend = 6.941, yend = 0.6695), \n               linetype=\"dashed\", color=\"red\", size=1) +\n  geom_vline(xintercept = 6.941, linetype=\"dashed\", color=\"red\", size=1) +\n  geom_vline(xintercept = 7.437, linetype=\"dashed\", color=\"red\", size=1) \n```\n\n::: {.cell-output-display}\n![](Aula12B_files/figure-html/segmentos-1.png){width=672}\n:::\n\n```{.r .cell-code}\n  # scale_y_continuous(limits = c(0.3, 1)) +\n```\n:::\n\n\n## Treino e Teste com todas as variáveis\n\nAgora vamos trabalhar com o conjunto completo criando um conjunto de treino e teste.  \n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Vamos criar os conjuntos de treino teste e desenvolver a arvore \n## com todas as variáveis.\nlibrary(caret)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nCarregando pacotes exigidos: lattice\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'caret'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:purrr':\n\n    lift\n```\n:::\n\n```{.r .cell-code}\nset.seed(21)\nindice <- createDataPartition(dados$medv, times=1, p=0.75, list=FALSE)\nconj_treino <- dados[indice,]\nconj_teste <- dados[-indice,]\nhead(conj_treino)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     crim   zn indus chas   nox    rm  age    dis rad tax ptratio  black lstat\n1 0.00632 18.0  2.31    0 0.538 6.575 65.2 4.0900   1 296    15.3 396.90  4.98\n3 0.02729  0.0  7.07    0 0.469 7.185 61.1 4.9671   2 242    17.8 392.83  4.03\n4 0.03237  0.0  2.18    0 0.458 6.998 45.8 6.0622   3 222    18.7 394.63  2.94\n5 0.06905  0.0  2.18    0 0.458 7.147 54.2 6.0622   3 222    18.7 396.90  5.33\n6 0.02985  0.0  2.18    0 0.458 6.430 58.7 6.0622   3 222    18.7 394.12  5.21\n7 0.08829 12.5  7.87    0 0.524 6.012 66.6 5.5605   5 311    15.2 395.60 12.43\n  medv\n1 24.0\n3 34.7\n4 33.4\n5 36.2\n6 28.7\n7 22.9\n```\n:::\n\n```{.r .cell-code}\nhead(conj_teste)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      crim   zn indus chas   nox    rm  age    dis rad tax ptratio  black lstat\n2  0.02731  0.0  7.07    0 0.469 6.421 78.9 4.9671   2 242    17.8 396.90  9.14\n10 0.17004 12.5  7.87    0 0.524 6.004 85.9 6.5921   5 311    15.2 386.71 17.10\n12 0.11747 12.5  7.87    0 0.524 6.009 82.9 6.2267   5 311    15.2 396.90 13.27\n16 0.62739  0.0  8.14    0 0.538 5.834 56.5 4.4986   4 307    21.0 395.62  8.47\n19 0.80271  0.0  8.14    0 0.538 5.456 36.6 3.7965   4 307    21.0 288.99 11.69\n23 1.23247  0.0  8.14    0 0.538 6.142 91.7 3.9769   4 307    21.0 396.90 18.72\n   medv\n2  21.6\n10 18.9\n12 18.9\n16 19.9\n19 20.2\n23 15.2\n```\n:::\n:::\n\n\n\n\n## Arvore de Regressão Treino\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## A função rpart tem diversos parametros aqui foi configurado um deles\n# cp o parametro de complexidade\n# Um valor de cp muito pequeno ocasiona overfitting e um valor muito grande \n# resulta numa arvore muito pequena (underfitting).\n# Nos dois casos se diminui o desempenho do modelo.\narvreg1 <- rpart(medv ~ ., \n                data=conj_treino,\n                method=\"anova\", #para arvore de regerssão\n                control=rpart.control(minsplit=30,cp=0.01))\n\narvreg1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nn= 381 \n\nnode), split, n, deviance, yval\n      * denotes terminal node\n\n 1) root 381 31196.9300 22.34672  \n   2) rm< 6.8375 311 10862.7700 19.42958  \n     4) lstat>=14.405 131  2579.9460 14.70534  \n       8) crim>=7.006285 53   523.4645 11.23774 *\n       9) crim< 7.006285 78   986.1646 17.06154 *\n     5) lstat< 14.405 180  3231.2930 22.86778  \n      10) rm< 6.5445 145  1867.9540 21.78414 *\n      11) rm>=6.5445 35   487.6657 27.35714 *\n   3) rm>=6.8375 70  5929.5660 35.30714  \n     6) rm< 7.435 49  2010.0620 31.05102  \n      12) lstat>=9.65 10   467.4640 23.84000 *\n      13) lstat< 9.65 39   889.2800 32.90000 *\n     7) rm>=7.435 21   960.7895 45.23810 *\n```\n:::\n:::\n\n\n\n## Erros a partir do conjunto de treino\n\n- O erro relativo (Rel error) é obtido através de 1 - R2\n- O xerror é obtido através da validação cruzdada (10 fold)\n- O xtsd é o desvio padrão dos valores obtidos na validação cruzada.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Mostra 2 gráficos:\n# 1) Variação do R2 aparente e relativo vs número de partições\n# 2) Erro Relativo vs número de partições\nrsq.rpart(arvreg1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nRegression tree:\nrpart(formula = medv ~ ., data = conj_treino, method = \"anova\", \n    control = rpart.control(minsplit = 30, cp = 0.01))\n\nVariables actually used in tree construction:\n[1] crim  lstat rm   \n\nRoot node error: 31197/381 = 81.882\n\nn= 381 \n\n        CP nsplit rel error  xerror     xstd\n1 0.461731      0   1.00000 1.01028 0.096645\n2 0.161924      1   0.53827 0.65330 0.065046\n3 0.094840      2   0.37634 0.49315 0.059728\n4 0.034308      3   0.28151 0.37245 0.050769\n5 0.028069      4   0.24720 0.34074 0.051145\n6 0.020942      5   0.21913 0.29254 0.042692\n7 0.010000      6   0.19819 0.28562 0.042529\n```\n:::\n\n::: {.cell-output-display}\n![](Aula12B_files/figure-html/erros-1.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](Aula12B_files/figure-html/erros-2.png){width=672}\n:::\n\n```{.r .cell-code}\n## Mostra a variação do Erro relativo vs cp(parametro de complexidade)\nplotcp(arvreg1)\n```\n\n::: {.cell-output-display}\n![](Aula12B_files/figure-html/erros-3.png){width=672}\n:::\n:::\n\n\n\n## Gráfico de importancia das variáveis\n\nA importancia das variáveis é calculada com base nos resultados das melhores partições\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Gráfico de Importância de variável\nvar_imp <- arvreg1$variable.importance\nnomes_var <- names(var_imp)\nvar_impdf <- data.frame(Importancia=unname(var_imp), Variavel=nomes_var) %>%\n                        arrange(Importancia)\nvar_impdf$Variavel <- factor(var_impdf$Variavel, levels=var_impdf$Variavel)\nggplot(var_impdf, aes(x=Variavel, y=Importancia)) + \n         geom_col() + \n        coord_flip()\n```\n\n::: {.cell-output-display}\n![](Aula12B_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\n\n## Mostrando a árvore e gerando previsões\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Mostrando a arvore\npar(xpd = NA)\nplot(arvreg1)\ntext(arvreg1,pretty=0)\n```\n\n::: {.cell-output-display}\n![](Aula12B_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Fazendo Previsões\nprevisao1 <- arvreg1 %>% predict(conj_teste)\nhead(previsao1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       2       10       12       16       19       23 \n21.78414 17.06154 21.78414 21.78414 21.78414 17.06154 \n```\n:::\n\n```{.r .cell-code}\n# Calcula os erros de previsão\nRMSE(previsao1, conj_teste$medv)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 5.303593\n```\n:::\n:::\n\n\n\n\n## Arvore de Regressão com caret\n\nAqui vamos usar a biblioteca caret que tem umas facilidades para otimização do cp e apresentação dos resultados\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(21)\n## Otimizamos o valor de cp usando um 10-fold cv\n# O parametro tuneLength diz para o algoritmo escolher diferentes valores para cp\n# O parametro tuneGrid permite decidir que valores cp deve assumir enquanto que o\n# tuneLength somente limita o número default de parametros que se usa.\narvreg2 <- train(medv ~ . , data = conj_treino, method = \"rpart\",\n                 trControl = trainControl(\"cv\", number = 10),\n                 tuneGrid = data.frame(cp = seq(0.01,0.10, length.out=100)) \n                 )\n# Mostra a acurácia vs cp (parametro de complexidade)\nplot(arvreg2)\n```\n\n::: {.cell-output-display}\n![](Aula12B_files/figure-html/arvore3-1.png){width=672}\n:::\n\n```{.r .cell-code}\n## Indica o melhor valor de cp\narvreg2$bestTune\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          cp\n4 0.01272727\n```\n:::\n:::\n\n\n\n## Desenhando a Árvore\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Apresenta o modelo final de arvore ajustado\npar(xpd = NA)\nplot(arvreg2$finalModel)\ntext(arvreg2$finalModel,  digits = 3)\n```\n\n::: {.cell-output-display}\n![](Aula12B_files/figure-html/arvore4-1.png){width=672}\n:::\n\n```{.r .cell-code}\n## usando o rpart.plot\nlibrary(rpart.plot)\nrpart.plot(arvreg2$finalModel)\n```\n\n::: {.cell-output-display}\n![](Aula12B_files/figure-html/arvore4-2.png){width=672}\n:::\n:::\n\n\n\n## Previsões\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Regras de Decisão\narvreg2$finalModel\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nn= 381 \n\nnode), split, n, deviance, yval\n      * denotes terminal node\n\n 1) root 381 31196.9300 22.34672  \n   2) rm< 6.8375 311 10862.7700 19.42958  \n     4) lstat>=14.405 131  2579.9460 14.70534  \n       8) crim>=7.006285 53   523.4645 11.23774 *\n       9) crim< 7.006285 78   986.1646 17.06154 *\n     5) lstat< 14.405 180  3231.2930 22.86778  \n      10) rm< 6.5445 145  1867.9540 21.78414 *\n      11) rm>=6.5445 35   487.6657 27.35714 *\n   3) rm>=6.8375 70  5929.5660 35.30714  \n     6) rm< 7.435 49  2010.0620 31.05102  \n      12) lstat>=11.315 7   367.8886 21.88571 *\n      13) lstat< 11.315 42   956.1507 32.57857 *\n     7) rm>=7.435 21   960.7895 45.23810 *\n```\n:::\n\n```{.r .cell-code}\n# Fazendo Previsões\nprevisao2 <- arvreg2 %>% predict(conj_teste)\nhead(previsao2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       2       10       12       16       19       23 \n21.78414 17.06154 21.78414 21.78414 21.78414 17.06154 \n```\n:::\n\n```{.r .cell-code}\n# Calcula os erros de previsão\nRMSE(previsao2, conj_teste$medv)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 5.304604\n```\n:::\n:::\n\n\n\n## Vamos comparar com Regressão Multipla\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(leaps)\n## Cria uma função de predição para o leaps\npredict.regsubsets <- function(object,newdata,id,...){\n  form <- as.formula(object$call[[2]])\n  mat <- model.matrix(form,newdata)\n  coefi <- coef(object,id=id)\n  mat[,names(coefi)]%*%coefi\n}\nset.seed(21)\nenvelopes <- sample(rep(1:5,length=nrow(conj_treino)))\ntable(envelopes)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nenvelopes\n 1  2  3  4  5 \n77 76 76 76 76 \n```\n:::\n\n```{.r .cell-code}\nerro_cv <- matrix(NA,5,13)\nfor(k in 1:5){\n  melh_ajus <- regsubsets(medv ~ ., data=conj_treino[envelopes!=k,], \n                          nvmax=13,method=\"forward\")\n  for(i in 1:13){\n    prev <- predict(melh_ajus, conj_treino[envelopes==k,],id=i)\n    erro_cv[k,i] <- mean( (conj_treino$medv[envelopes==k]-prev)^2)\n  }\n}\nrmse_cv <- sqrt(apply(erro_cv,2,mean))  # Erro medio quadratico de cada modelo\nplot(rmse_cv,pch=19,type=\"b\")          \n```\n\n::: {.cell-output-display}\n![](Aula12B_files/figure-html/kfold-1.png){width=672}\n:::\n:::\n\n\n## Obtem a fórmula do modelo\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncoef(melh_ajus, 11)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  (Intercept)          crim            zn          chas           nox \n 37.051974686  -0.144753575   0.047778839   1.780110617 -14.931943579 \n           rm           dis           rad           tax       ptratio \n  3.815637016  -1.500993904   0.349882023  -0.015845438  -0.986230946 \n        black         lstat \n  0.009024705  -0.534163142 \n```\n:::\n:::\n\n\n## Teste com o conjunto de teste\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprevisao3 <- predict(melh_ajus, conj_teste, 11) \nRMSE(previsao3, conj_teste$medv)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 5.936577\n```\n:::\n:::\n",
    "supporting": [
      "Aula12B_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}