{
  "hash": "d89e11c89fab52267a4d49d3bd3f33b4",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: 'Arvores de Regressão - XGBoost'\nauthor: \"Ricardo Accioly\"\ndate: \"2025-11-11\"\nexecute: \n  echo: true\n  warning: false\n  message: false\n  freeze: auto\nformat:\n html:\n    code-link: true\n    fig-height: 10\n    fig-width: 10\n    fig-align: center\n    fig-dpi: 300\nknitr: \n  opts_chunk: \n    out.width: 90%\n    fig.showtext: true\n    collapese: true\n---\n\n## Bibliotecas\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(MASS)         # Dados Boston\nlibrary(xgboost)      # Modelo XGBoost\nlibrary(dplyr)        # Manipulação de dados\nlibrary(rsample)      # Separação treino/teste\nlibrary(Metrics)      # Cálculo de RMSE\nlibrary(ggplot2)      # Gráficos\n```\n:::\n\n\n## Avaliando, selecionando dados\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(\"Boston\")\nnames(Boston)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"crim\"    \"zn\"      \"indus\"   \"chas\"    \"nox\"     \"rm\"      \"age\"    \n [8] \"dis\"     \"rad\"     \"tax\"     \"ptratio\" \"black\"   \"lstat\"   \"medv\"   \n```\n\n\n:::\n\n```{.r .cell-code}\ndados <- Boston \n```\n:::\n\n\n## Treino e Teste com todas as variáveis\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Vamos criar os conjuntos de treino teste e desenvolver a arvore \n## com todas as variáveis.\nlibrary(caret)\nset.seed(2025)\nindice <- createDataPartition(dados$medv, times=1, p=0.75, list=FALSE)\nconj_treino <- dados[indice,]\nconj_teste <- dados[-indice,] \nstr(conj_treino)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'data.frame':\t381 obs. of  14 variables:\n $ crim   : num  0.0273 0.0299 0.0883 0.1446 0.17 ...\n $ zn     : num  0 0 12.5 12.5 12.5 12.5 12.5 12.5 0 0 ...\n $ indus  : num  7.07 2.18 7.87 7.87 7.87 7.87 7.87 7.87 8.14 8.14 ...\n $ chas   : int  0 0 0 0 0 0 0 0 0 0 ...\n $ nox    : num  0.469 0.458 0.524 0.524 0.524 0.524 0.524 0.524 0.538 0.538 ...\n $ rm     : num  6.42 6.43 6.01 6.17 6 ...\n $ age    : num  78.9 58.7 66.6 96.1 85.9 94.3 82.9 39 61.8 84.5 ...\n $ dis    : num  4.97 6.06 5.56 5.95 6.59 ...\n $ rad    : int  2 3 5 5 5 5 5 5 4 4 ...\n $ tax    : num  242 222 311 311 311 311 311 311 307 307 ...\n $ ptratio: num  17.8 18.7 15.2 15.2 15.2 15.2 15.2 15.2 21 21 ...\n $ black  : num  397 394 396 397 387 ...\n $ lstat  : num  9.14 5.21 12.43 19.15 17.1 ...\n $ medv   : num  21.6 28.7 22.9 27.1 18.9 15 18.9 21.7 20.4 18.2 ...\n```\n\n\n:::\n\n```{.r .cell-code}\nstr(conj_teste)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'data.frame':\t125 obs. of  14 variables:\n $ crim   : num  0.00632 0.02729 0.03237 0.06905 0.21124 ...\n $ zn     : num  18 0 0 0 12.5 0 0 0 0 75 ...\n $ indus  : num  2.31 7.07 2.18 2.18 7.87 8.14 8.14 8.14 8.14 2.95 ...\n $ chas   : int  0 0 0 0 0 0 0 0 0 0 ...\n $ nox    : num  0.538 0.469 0.458 0.458 0.524 0.538 0.538 0.538 0.538 0.428 ...\n $ rm     : num  6.58 7.18 7 7.15 5.63 ...\n $ age    : num  65.2 61.1 45.8 54.2 100 36.6 94.1 85.7 88.8 21.8 ...\n $ dis    : num  4.09 4.97 6.06 6.06 6.08 ...\n $ rad    : int  1 2 3 3 5 4 4 4 4 3 ...\n $ tax    : num  296 242 222 222 311 307 307 307 307 252 ...\n $ ptratio: num  15.3 17.8 18.7 18.7 15.2 21 21 21 21 18.3 ...\n $ black  : num  397 393 395 397 387 ...\n $ lstat  : num  4.98 4.03 2.94 5.33 29.93 ...\n $ medv   : num  24 34.7 33.4 36.2 16.5 20.2 15.6 13.9 14.8 30.8 ...\n```\n\n\n:::\n:::\n\n\n## Preparando os dados\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx_treino <- model.matrix(medv ~ . , data = conj_treino)[, -1]\ny_treino <- conj_treino$medv\n\nx_teste <- model.matrix(medv ~ . , data = conj_teste)[, -1]\ny_teste = conj_teste$medv\n\ndtrain <- xgb.DMatrix(data = x_treino, label = y_treino)\ndtest <- xgb.DMatrix(data = x_teste, label = y_teste)\n```\n:::\n\n\n## Treinamento com validação cruzada e grid de parâmetros\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2025)\n# Grid de hiperparâmetros\ngrid <- expand.grid(\n  eta = c(0.01, 0.1),\n  max_depth = c(3, 6),\n  nrounds = c(100, 200)\n)\n\n# Avaliar cada combinação com CV\nresultados_cv <- list()\n\nfor (i in 1:nrow(grid)) {\n  params <- list(\n    objective = \"reg:squarederror\",\n    eta = grid$eta[i],\n    max_depth = grid$max_depth[i],\n    verbosity = 0\n  )\n  \n  cv <- xgb.cv(\n    params = params,\n    data = dtrain,\n    nrounds = grid$nrounds[i],\n    nfold = 5,\n    metrics = \"rmse\",\n    early_stopping_rounds = 10,\n    verbose = 0\n  )\n  \n  resultados_cv[[i]] <- list(\n    rmse = min(cv$evaluation_log$test_rmse_mean),\n    best_nrounds = cv$best_iteration,\n    params = grid[i, ]\n  )\n}\n\n# Melhor modelo\nrmses <- sapply(resultados_cv, function(x) x$rmse)\nmelhor_indice <- which.min(rmses)\nmelhor_param <- resultados_cv[[melhor_indice]]$params\nmelhor_nrounds <- resultados_cv[[melhor_indice]]$best_nrounds\n\nmelhor_param\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  eta max_depth nrounds\n6 0.1         3     200\n```\n\n\n:::\n:::\n\n\n## Modelo Final\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Treino final com os melhores parâmetros\nfinal_model <- xgb.train(\n  params = list(\n    objective = \"reg:squarederror\",\n    eta = melhor_param$eta,\n    max_depth = melhor_param$max_depth\n  ),\n  data = dtrain,\n  nrounds = melhor_nrounds,\n  verbose = 0\n)\n```\n:::\n\n\n## Importancia das variáveis\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Importância das variáveis\nimportance_matrix <- xgb.importance(model = final_model)\n\n# Gráfico\nxgb.plot.importance(importance_matrix)\n```\n\n::: {.cell-output-display}\n![](Aula11.3_files/figure-html/unnamed-chunk-7-1.png){width=90%}\n:::\n:::\n\n\n## Previsões\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconj_teste$prev <- predict(final_model, dtest)\n\n\nggplot(conj_teste, aes(x = prev, y = medv)) + \n  geom_point() + \n  geom_abline()\n```\n\n::: {.cell-output-display}\n![](Aula11.3_files/figure-html/unnamed-chunk-8-1.png){width=90%}\n:::\n:::\n\n\n## Calculando o RMSE\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrmse_final <- rmse(y_teste, conj_teste$prev)\ncat(\"RMSE no conjunto de teste:\", rmse_final)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRMSE no conjunto de teste: 3.34077\n```\n\n\n:::\n\n```{.r .cell-code}\ncaret::postResample(conj_teste$prev, conj_teste$medv)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     RMSE  Rsquared       MAE \n3.3407695 0.8947614 2.1166538 \n```\n\n\n:::\n:::\n\n\n## Comparação com outro modelo (Regressão Linear)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nctrl <- trainControl(method = \"cv\", number = 5)\n\nmodel_lm <- train(\n  medv ~ ., data = conj_treino,\n  method = \"lm\",\n  trControl = ctrl\n)\n\npred_lm <- predict(model_lm, newdata = conj_teste)\npostResample(pred_lm, conj_teste$medv)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     RMSE  Rsquared       MAE \n5.5535662 0.7120235 3.8617314 \n```\n\n\n:::\n:::\n\n",
    "supporting": [
      "Aula11.3_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}