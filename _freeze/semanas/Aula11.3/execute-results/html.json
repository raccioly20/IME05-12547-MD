{
  "hash": "2ff1116e382140197d5ae5cbb35db7e3",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: 'Arvores de Regressão - XGBoost'\nauthor: \"Ricardo Accioly\"\ndate: \"2024-08-20\"\nexecute: \n  echo: true\n  warning: false\n  message: false\n  freeze: auto\nformat:\n html:\n    code-link: true\n    fig-height: 10\n    fig-width: 10\n    fig-align: center\n    fig-dpi: 300\nknitr: \n  opts_chunk: \n    out.width: 90%\n    fig.showtext: true\n    collapese: true\n---\n\n\n\n\n## Bibliotecas\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(MASS)\nlibrary(tidyverse)\n```\n:::\n\n\n\n\n## Avaliando, selecionando dados\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(\"Boston\")\nnames(Boston)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"crim\"    \"zn\"      \"indus\"   \"chas\"    \"nox\"     \"rm\"      \"age\"    \n [8] \"dis\"     \"rad\"     \"tax\"     \"ptratio\" \"black\"   \"lstat\"   \"medv\"   \n```\n\n\n:::\n\n```{.r .cell-code}\ndados <- Boston \n```\n:::\n\n\n\n\n## Treino e Teste com todas as variáveis\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Vamos criar os conjuntos de treino teste e desenvolver a arvore \n## com todas as variáveis.\nlibrary(caret)\nset.seed(21)\nindice <- createDataPartition(dados$medv, times=1, p=0.75, list=FALSE)\nconj_treino <- dados[indice,]\nconj_teste <- dados[-indice,] \nstr(conj_treino)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'data.frame':\t381 obs. of  14 variables:\n $ crim   : num  0.00632 0.02729 0.03237 0.06905 0.02985 ...\n $ zn     : num  18 0 0 0 0 12.5 12.5 12.5 12.5 12.5 ...\n $ indus  : num  2.31 7.07 2.18 2.18 2.18 7.87 7.87 7.87 7.87 7.87 ...\n $ chas   : int  0 0 0 0 0 0 0 0 0 0 ...\n $ nox    : num  0.538 0.469 0.458 0.458 0.458 0.524 0.524 0.524 0.524 0.524 ...\n $ rm     : num  6.58 7.18 7 7.15 6.43 ...\n $ age    : num  65.2 61.1 45.8 54.2 58.7 66.6 96.1 100 94.3 39 ...\n $ dis    : num  4.09 4.97 6.06 6.06 6.06 ...\n $ rad    : int  1 2 3 3 3 5 5 5 5 5 ...\n $ tax    : num  296 242 222 222 222 311 311 311 311 311 ...\n $ ptratio: num  15.3 17.8 18.7 18.7 18.7 15.2 15.2 15.2 15.2 15.2 ...\n $ black  : num  397 393 395 397 394 ...\n $ lstat  : num  4.98 4.03 2.94 5.33 5.21 ...\n $ medv   : num  24 34.7 33.4 36.2 28.7 22.9 27.1 16.5 15 21.7 ...\n```\n\n\n:::\n\n```{.r .cell-code}\nstr(conj_teste)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'data.frame':\t125 obs. of  14 variables:\n $ crim   : num  0.0273 0.17 0.1175 0.6274 0.8027 ...\n $ zn     : num  0 12.5 12.5 0 0 0 0 0 0 75 ...\n $ indus  : num  7.07 7.87 7.87 8.14 8.14 8.14 8.14 5.96 5.96 2.95 ...\n $ chas   : int  0 0 0 0 0 0 0 0 0 0 ...\n $ nox    : num  0.469 0.524 0.524 0.538 0.538 0.538 0.538 0.499 0.499 0.428 ...\n $ rm     : num  6.42 6 6.01 5.83 5.46 ...\n $ age    : num  78.9 85.9 82.9 56.5 36.6 91.7 82 61.4 30.2 21.8 ...\n $ dis    : num  4.97 6.59 6.23 4.5 3.8 ...\n $ rad    : int  2 5 5 4 4 4 4 5 5 3 ...\n $ tax    : num  242 311 311 307 307 307 307 279 279 252 ...\n $ ptratio: num  17.8 15.2 15.2 21 21 21 21 19.2 19.2 18.3 ...\n $ black  : num  397 387 397 396 289 ...\n $ lstat  : num  9.14 17.1 13.27 8.47 11.69 ...\n $ medv   : num  21.6 18.9 18.9 19.9 20.2 15.2 13.2 20 24.7 30.8 ...\n```\n\n\n:::\n:::\n\n\n\n\n## Preparando os dados\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx_treino <- model.matrix(medv ~ . , data = conj_treino)[, -1]\ny_treino <- conj_treino$medv\n\nx_teste <- model.matrix(medv ~ . , data = conj_teste)[, -1]\ny_teste = conj_teste$medv\n```\n:::\n\n\n\n\n## 1a tentativa Xgboost\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(xgboost)\nset.seed(21)\ncv <- xgb.cv(data = as.matrix(x_treino), label = as.matrix(y_treino),\n             objective = \"reg:squarederror\", nrounds = 100, nfold = 5, eta = 0.3, max_depth = 6,\n             verbose = FALSE)\n# cv\nelog <- as.data.frame(cv$evaluation_log)\nelog %>% \n   summarize(ntrees.train = which.min(train_rmse_mean),   # find the index of min(train_rmse_mean)\n             ntrees.test  = which.min(test_rmse_mean))   # find the index of min(test_rmse_mean)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  ntrees.train ntrees.test\n1          100          39\n```\n\n\n:::\n\n```{.r .cell-code}\n(nrounds <- which.min(elog$test_rmse_mean))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 39\n```\n\n\n:::\n:::\n\n\n\n\n## Modelo Final\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n modelo_xgb <- xgboost(data = as.matrix(x_treino), label = as.matrix(y_treino),\n             objective = \"reg:squarederror\", nrounds = nrounds, eta = 0.3, max_depth = 6,\n             verbose = FALSE)\n```\n:::\n\n\n\n\n## Previsões\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconj_teste$prev <- predict(modelo_xgb, as.matrix(x_teste))\n\n\nggplot(conj_teste, aes(x = prev, y = medv)) + \n  geom_point() + \n  geom_abline()\n```\n\n::: {.cell-output-display}\n![](Aula11.3_files/figure-html/unnamed-chunk-2-1.png){width=90%}\n:::\n:::\n\n\n\n\n## Calculando o RMSE\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconj_teste %>%\n  mutate(residuos = medv - prev) %>%\n  summarize(rmse = sqrt(mean(residuos^2)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      rmse\n1 3.874079\n```\n\n\n:::\n:::\n",
    "supporting": [
      "Aula11.3_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}