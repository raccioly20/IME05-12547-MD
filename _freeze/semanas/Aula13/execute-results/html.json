{
  "hash": "f197ec29f291137a78b09608f41f1ccb",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Análise de Clusters\"\nauthor: \"Ricardo Accioly\"\ndate: \"2024-12-04\"\nexecute: \n  echo: true\n  warning: false\n  message: false\n  freeze: auto\nformat:\n html:\n    code-link: true\n    fig-height: 10\n    fig-width: 10\n    fig-align: center\n    fig-dpi: 300\nknitr: \n  opts_chunk: \n    out.width: 90%\n    fig.showtext: true\n    collapese: true\n---\n\n\n\n\n## Bibliotecas\n\nEste conteúdo foi adaptado de: https://mhahsler.github.io/Introduction_to_Data_Mining_R_Examples/book/clustering-analysis.html\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(cluster)\n```\n:::\n\n\n\n\n## Dados\n\nO conjunto de dados Ruspini, que consiste em 75 pontos dividido em quatro grupos, ele é popular para ilustrar técnicas de agrupamento. É um conjunto de dados muito simples com clusters bem separados. O conjunto de dados original tem os pontos ordenados por grupo. Podemos embaralhar os dados (linhas) usando sample_frac.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(ruspini, package=\"cluster\")\n```\n:::\n\n\n\n\n## Manipulando os dados\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nruspini <- as_tibble(ruspini) %>% sample_frac()\nruspini\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 75 × 2\n       x     y\n   <int> <int>\n 1    41   150\n 2    76    27\n 3    31    60\n 4    66    18\n 5    27    72\n 6    70     4\n 7    60   136\n 8    28    60\n 9    61    25\n10    83    21\n# ℹ 65 more rows\n```\n\n\n:::\n:::\n\n\n\n\n## Explorando os dados\n\nNesta etapa os dados são avaliados, pois eventualmente temos situações de dados ausentes, pontos afastados.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(ruspini, aes(x = x, y = y)) + geom_point()\n```\n\n::: {.cell-output-display}\n![](Aula13_files/figure-html/unnamed-chunk-4-1.png){width=90%}\n:::\n\n```{.r .cell-code}\nsummary(ruspini)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       x                y         \n Min.   :  4.00   Min.   :  4.00  \n 1st Qu.: 31.50   1st Qu.: 56.50  \n Median : 52.00   Median : 96.00  \n Mean   : 54.88   Mean   : 92.03  \n 3rd Qu.: 76.50   3rd Qu.:141.50  \n Max.   :117.00   Max.   :156.00  \n```\n\n\n:::\n:::\n\n\n\n\n## Normalização\n\nComo os algoritmos usam medidas de distância é necessário usarmos a normalização para que os resultados naõ sejam afetados pela escala dos dados.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Aqui vamos essa função para fazer a normalização\nescala_numerica <- function(x) x %>% mutate_if(is.numeric, function(y) as.vector(scale(y)))\n\nruspini_norm <- ruspini %>% escala_numerica()\nsummary(ruspini_norm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       x                  y           \n Min.   :-1.66806   Min.   :-1.80743  \n 1st Qu.:-0.76649   1st Qu.:-0.72946  \n Median :-0.09442   Median : 0.08158  \n Mean   : 0.00000   Mean   : 0.00000  \n 3rd Qu.: 0.70879   3rd Qu.: 1.01582  \n Max.   : 2.03655   Max.   : 1.31355  \n```\n\n\n:::\n:::\n\n\n\n\n## Métodos para obtenção de Clusters\n\n### K-médias\n\nO algoritmo do k-médias usa a distância Eucliadiana quadrática. Aqui vamos usar k=4 e vamos rodar o algoritmo 10 vezes\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkm <- kmeans(ruspini_norm, centers = 4, nstart = 10)\nkm\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nK-means clustering with 4 clusters of sizes 23, 15, 20, 17\n\nCluster means:\n           x          y\n1 -0.3595425  1.1091151\n2  0.4607268 -1.4912271\n3 -1.1385941 -0.5559591\n4  1.4194387  0.4692907\n\nClustering vector:\n [1] 1 2 3 2 3 2 1 3 2 2 1 4 1 2 3 1 4 4 2 4 1 4 3 1 3 4 3 3 3 2 2 4 4 4 4 1 4 3\n[39] 1 1 1 4 4 3 4 2 1 3 1 4 3 4 1 1 2 1 3 1 1 1 2 2 1 4 3 3 3 1 1 3 1 2 3 2 3\n\nWithin cluster sum of squares by cluster:\n[1] 2.658679 1.082373 2.705477 3.641276\n (between_SS / total_SS =  93.2 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nruspini_clusters <- ruspini_norm %>% add_column(cluster = factor(km$cluster))\nruspini_clusters\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 75 × 3\n        x      y cluster\n    <dbl>  <dbl> <fct>  \n 1 -0.455  1.19  1      \n 2  0.692 -1.34  2      \n 3 -0.783 -0.658 3      \n 4  0.365 -1.52  2      \n 5 -0.914 -0.411 3      \n 6  0.496 -1.81  2      \n 7  0.168  0.903 1      \n 8 -0.881 -0.658 3      \n 9  0.201 -1.38  2      \n10  0.922 -1.46  2      \n# ℹ 65 more rows\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(ruspini_clusters, aes(x = x, y = y, color = cluster)) + geom_point()\n```\n\n::: {.cell-output-display}\n![](Aula13_files/figure-html/unnamed-chunk-8-1.png){width=90%}\n:::\n:::\n\n\n\n\nAdicionando os centroides aos gráficos\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncentroids <- as_tibble(km$centers, rownames = \"cluster\")\ncentroids\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 3\n  cluster      x      y\n  <chr>    <dbl>  <dbl>\n1 1       -0.360  1.11 \n2 2        0.461 -1.49 \n3 3       -1.14  -0.556\n4 4        1.42   0.469\n```\n\n\n:::\n\n```{.r .cell-code}\nggplot(ruspini_clusters, aes(x = x, y = y, color = cluster)) + geom_point() + geom_point(data = centroids, aes(x = x, y = y, color = cluster), shape = 3, size = 10)\n```\n\n::: {.cell-output-display}\n![](Aula13_files/figure-html/unnamed-chunk-9-1.png){width=90%}\n:::\n:::\n\n\n\n\nVamos usar a biblioteca factoextra para visualizarmos os clusters\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(factoextra)\nfviz_cluster(km, data = ruspini_norm, centroids = TRUE, repel = TRUE, ellipse.type = \"norm\")\n```\n\n::: {.cell-output-display}\n![](Aula13_files/figure-html/unnamed-chunk-10-1.png){width=90%}\n:::\n:::\n\n\n\n\n### k-medoides\n\nOs medoides pertencem ao proprio conjunto de dados. Podemos observar que o resultado é semelhante ao obtido no k-médias, mas o algoritmo é mais lento.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#library(cluster)\nkmed <- pam(ruspini_norm, k = 4)\nsummary(kmed)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMedoids:\n     ID          x          y\n[1,] 24 -0.3566917  1.1698207\n[2,] 19  0.4629124 -1.4583746\n[3,] 67 -1.1762959 -0.5549325\n[4,] 12  1.4464374  0.5538374\nClustering vector:\n [1] 1 2 3 2 3 2 1 3 2 2 1 4 1 2 3 1 4 4 2 4 1 4 3 1 3 4 3 3 3 2 2 4 4 4 4 1 4 3\n[39] 1 1 1 4 4 3 4 2 1 3 1 4 3 4 1 1 2 1 3 1 1 1 2 2 1 4 3 3 3 1 1 3 1 2 3 2 3\nObjective function:\n    build      swap \n0.4422977 0.3187056 \n\nNumerical information per cluster:\n     size  max_diss   av_diss  diameter separation\n[1,]   23 0.6558680 0.2993397 1.1591436   0.767612\n[2,]   15 0.4589783 0.2433250 0.8359025   1.157682\n[3,]   20 0.5755656 0.3401125 1.1192822   1.157682\n[4,]   17 0.9459253 0.3862345 1.4627043   0.767612\n\nIsolated clusters:\n L-clusters: character(0)\n L*-clusters: [1] 2 3\n\nSilhouette plot information:\n   cluster neighbor sil_width\n24       1        3 0.8368407\n1        1        3 0.8305019\n49       1        4 0.8222142\n69       1        3 0.8220686\n21       1        3 0.8158429\n39       1        4 0.8150398\n56       1        3 0.8134280\n11       1        3 0.8064757\n58       1        4 0.7984225\n53       1        3 0.7969057\n47       1        3 0.7841631\n59       1        4 0.7794889\n40       1        4 0.7605512\n36       1        3 0.7591035\n13       1        3 0.7473901\n16       1        4 0.7423529\n63       1        3 0.7402623\n68       1        3 0.7249133\n41       1        4 0.7007372\n71       1        3 0.6739284\n54       1        4 0.5661372\n7        1        4 0.5413082\n60       1        4 0.4673917\n19       2        3 0.8592059\n74       2        3 0.8553255\n4        2        3 0.8530741\n46       2        3 0.8449473\n72       2        3 0.8361633\n14       2        3 0.8187150\n62       2        3 0.8178795\n2        2        4 0.8087015\n31       2        3 0.8013799\n6        2        3 0.7983516\n55       2        3 0.7918724\n61       2        3 0.7768261\n9        2        3 0.7727269\n10       2        4 0.7425993\n30       2        3 0.7328306\n67       3        1 0.8094377\n57       3        2 0.8027447\n66       3        1 0.7782513\n44       3        2 0.7704646\n28       3        2 0.7700388\n27       3        1 0.7597906\n29       3        1 0.7530091\n5        3        1 0.7436695\n8        3        2 0.7412965\n23       3        1 0.7270442\n73       3        2 0.7255183\n70       3        2 0.7226938\n25       3        2 0.7042349\n38       3        1 0.7026960\n48       3        1 0.6966533\n3        3        2 0.6921822\n15       3        2 0.6756339\n65       3        2 0.6463656\n75       3        1 0.6005277\n51       3        2 0.6004543\n18       4        1 0.7898609\n12       4        1 0.7834341\n43       4        1 0.7822308\n42       4        1 0.7790446\n34       4        1 0.7780891\n45       4        1 0.7694930\n33       4        1 0.7624335\n50       4        1 0.7609359\n22       4        1 0.7400337\n35       4        1 0.7392052\n64       4        1 0.7390493\n26       4        1 0.7234199\n52       4        1 0.5894345\n17       4        2 0.5666610\n32       4        1 0.5114355\n37       4        1 0.4358476\n20       4        1 0.3312348\nAverage silhouette width per cluster:\n[1] 0.7454551 0.8073733 0.7211353 0.6812849\nAverage silhouette width of total data set:\n[1] 0.7368082\n\n2775 dissimilarities, summarized :\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n0.03868 1.16780 1.92500 1.78630 2.47760 3.91720 \nMetric :  euclidean \nNumber of objects : 75\n\nAvailable components:\n [1] \"medoids\"    \"id.med\"     \"clustering\" \"objective\"  \"isolation\" \n [6] \"clusinfo\"   \"silinfo\"    \"diss\"       \"call\"       \"data\"      \n```\n\n\n:::\n\n```{.r .cell-code}\nplot(kmed)\n```\n\n::: {.cell-output-display}\n![](Aula13_files/figure-html/unnamed-chunk-11-1.png){width=90%}\n:::\n\n::: {.cell-output-display}\n![](Aula13_files/figure-html/unnamed-chunk-11-2.png){width=90%}\n:::\n:::\n\n\n\n\nOutra forma de visualização\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfviz_cluster(kmed, ruspini_norm,\n             ellipse.type = \"convex\",\n             repel =TRUE,\n             ggtheme =theme_minimal())\n```\n\n::: {.cell-output-display}\n![](Aula13_files/figure-html/unnamed-chunk-12-1.png){width=90%}\n:::\n\n```{.r .cell-code}\nknitr::kable(kmed$medoids)\n```\n\n::: {.cell-output-display}\n\n\n|          x|          y|\n|----------:|----------:|\n| -0.3566917|  1.1698207|\n|  0.4629124| -1.4583746|\n| -1.1762959| -0.5549325|\n|  1.4464374|  0.5538374|\n\n\n:::\n\n```{.r .cell-code}\nlibrary(janitor)\ntabyl(kmed$clustering)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n kmed$clustering  n   percent\n               1 23 0.3066667\n               2 15 0.2000000\n               3 20 0.2666667\n               4 17 0.2266667\n```\n\n\n:::\n:::\n\n\n\n\n### Clusters Hierarquicos\n\nO agrupamento hierárquico começa com uma matriz de distância ´dist()´ e tem como padrão method=\"Euclidiano\". As matrizes de distância tornam-se muito grandes rapidamente (tamanho e complexidade de tempo é O(n2) onde n é o número se pontos de dados. Só é possível calcular e armazenar a matriz para pequenos conjuntos de dados.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd <- dist(ruspini_norm)\n```\n:::\n\n\n\n\nA função hclust() implementa o HCA, ou seja, o cluster hierarquico aglomerativo. Vamos começar usando o método da média.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhc <- hclust(d, method = \"average\")\n```\n:::\n\n\n\n\nO HCA retorna um dendrograma e não uma definição de clusters.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(hc)\n```\n\n::: {.cell-output-display}\n![](Aula13_files/figure-html/unnamed-chunk-15-1.png){width=90%}\n:::\n:::\n\n\n\n\nSe usarmos a biblioteca factoextra podemos definir o número de clusters que queremos visualizar.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfviz_dend(hc, k=4, horiz=TRUE)\n```\n\n::: {.cell-output-display}\n![](Aula13_files/figure-html/unnamed-chunk-16-1.png){width=90%}\n:::\n:::\n\n\n\n\nPodemos extrair as atribuições de cluster cortando o dendrograma em 4 partes e adicionando a identidade aos dados.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nclusters <- cutree(hc, k = 4)\ncluster_completo <- ruspini_norm %>%\n  add_column(cluster = factor(clusters))\ncluster_completo\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 75 × 3\n        x      y cluster\n    <dbl>  <dbl> <fct>  \n 1 -0.455  1.19  1      \n 2  0.692 -1.34  2      \n 3 -0.783 -0.658 3      \n 4  0.365 -1.52  2      \n 5 -0.914 -0.411 3      \n 6  0.496 -1.81  2      \n 7  0.168  0.903 1      \n 8 -0.881 -0.658 3      \n 9  0.201 -1.38  2      \n10  0.922 -1.46  2      \n# ℹ 65 more rows\n```\n\n\n:::\n:::\n\n\n\n\nPodemos usar o método de Ward para obter o cluster.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhc_w <- hclust(d, method = \"ward.D\")\n```\n:::\n\n\n\n\nO HCA retorna um dendrograma e não uma definição de clusters.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(hc_w)\n```\n\n::: {.cell-output-display}\n![](Aula13_files/figure-html/unnamed-chunk-19-1.png){width=90%}\n:::\n:::\n\n\n\n\nSe usarmos a biblioteca factoextra podemos definir o número de clusters que queremos visualizar.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfviz_dend(hc_w, k=4, horiz=TRUE)\n```\n\n::: {.cell-output-display}\n![](Aula13_files/figure-html/unnamed-chunk-20-1.png){width=90%}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfviz_cluster(list(data = ruspini_norm, cluster = cutree(hc_w, k = 4)), geom = \"point\")\n```\n\n::: {.cell-output-display}\n![](Aula13_files/figure-html/unnamed-chunk-21-1.png){width=90%}\n:::\n:::\n\n\n\n\n## Validação dos Clusters\n\n### Silhouette\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#library(cluster)\nplot(silhouette(kmed$clustering,d))\n```\n\n::: {.cell-output-display}\n![](Aula13_files/figure-html/unnamed-chunk-22-1.png){width=90%}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfviz_silhouette(silhouette(kmed$clustering, d))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  cluster size ave.sil.width\n1       1   23          0.75\n2       2   15          0.81\n3       3   20          0.72\n4       4   17          0.68\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](Aula13_files/figure-html/unnamed-chunk-23-1.png){width=90%}\n:::\n:::\n\n\n\n\n## Numero ótimo de clusters\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Usando o silhouette\nfviz_nbclust(ruspini_norm, pam, method =\"silhouette\", k.max = 8)\n```\n\n::: {.cell-output-display}\n![](Aula13_files/figure-html/unnamed-chunk-24-1.png){width=90%}\n:::\n\n```{.r .cell-code}\n## Metodo do cotovelo\nfviz_nbclust(ruspini_norm, kmeans, method =\"wss\", k.max = 8)\n```\n\n::: {.cell-output-display}\n![](Aula13_files/figure-html/unnamed-chunk-24-2.png){width=90%}\n:::\n:::\n\n\n\n\n## \n",
    "supporting": [
      "Aula13_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}