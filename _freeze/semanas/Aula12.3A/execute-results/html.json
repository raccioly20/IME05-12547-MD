{
  "hash": "ac638e1654c5ba264256e1e635e6f08f",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Arvores de Classificação - XGboost no Tidymodels\"\nauthor: \"Ricardo Accioly\"\ndate: \"2024-12-04\"\nexecute: \n  echo: true\n  warning: false\n  message: false\n  freeze: auto\nformat:\n html:\n    code-link: true\n    fig-height: 10\n    fig-width: 10\n    fig-align: center\n    fig-dpi: 300\nknitr: \n  opts_chunk: \n    out.width: 90%\n    fig.showtext: true\n    collapese: true\n---\n\n\n\n\n\n## Bibliotecas\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(ISLR)\n```\n:::\n\n\n\n\n\n## Dados\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(Default)\nsummary(Default)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n default    student       balance           income     \n No :9667   No :7056   Min.   :   0.0   Min.   :  772  \n Yes: 333   Yes:2944   1st Qu.: 481.7   1st Qu.:21340  \n                       Median : 823.6   Median :34553  \n                       Mean   : 835.4   Mean   :33517  \n                       3rd Qu.:1166.3   3rd Qu.:43808  \n                       Max.   :2654.3   Max.   :73554  \n```\n\n\n:::\n\n```{.r .cell-code}\nstr(Default)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'data.frame':\t10000 obs. of  4 variables:\n $ default: Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n $ student: Factor w/ 2 levels \"No\",\"Yes\": 1 2 1 1 1 2 1 2 1 1 ...\n $ balance: num  730 817 1074 529 786 ...\n $ income : num  44362 12106 31767 35704 38463 ...\n```\n\n\n:::\n\n```{.r .cell-code}\nhead(Default)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  default student   balance    income\n1      No      No  729.5265 44361.625\n2      No     Yes  817.1804 12106.135\n3      No      No 1073.5492 31767.139\n4      No      No  529.2506 35704.494\n5      No      No  785.6559 38463.496\n6      No     Yes  919.5885  7491.559\n```\n\n\n:::\n:::\n\n\n\n\n\n## Manipulando os dados\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncredito <- tibble(Default)\nsummary(credito)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n default    student       balance           income     \n No :9667   No :7056   Min.   :   0.0   Min.   :  772  \n Yes: 333   Yes:2944   1st Qu.: 481.7   1st Qu.:21340  \n                       Median : 823.6   Median :34553  \n                       Mean   : 835.4   Mean   :33517  \n                       3rd Qu.:1166.3   3rd Qu.:43808  \n                       Max.   :2654.3   Max.   :73554  \n```\n\n\n:::\n\n```{.r .cell-code}\n# renomeando colunas\ncredito <- credito %>% \n                rename( inadimplente = default, estudante = student, balanco = balance,\n                receita = income)\ncredito <- credito %>% mutate( inadimplente =  case_when(\n                           inadimplente == \"No\"  ~ \"Nao\",\n                           inadimplente == \"Yes\" ~ \"Sim\"\n                          )) %>% mutate(inadimplente = factor(inadimplente))\ncredito <- credito %>% mutate( estudante =  case_when(\n                           estudante == \"No\"  ~ 0,\n                           estudante == \"Yes\" ~ 1\n                          )) \n\nstr(credito)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ntibble [10,000 × 4] (S3: tbl_df/tbl/data.frame)\n $ inadimplente: Factor w/ 2 levels \"Nao\",\"Sim\": 1 1 1 1 1 1 1 1 1 1 ...\n $ estudante   : num [1:10000] 0 1 0 0 0 1 0 1 0 0 ...\n $ balanco     : num [1:10000] 730 817 1074 529 786 ...\n $ receita     : num [1:10000] 44362 12106 31767 35704 38463 ...\n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(credito)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n inadimplente   estudante         balanco          receita     \n Nao:9667     Min.   :0.0000   Min.   :   0.0   Min.   :  772  \n Sim: 333     1st Qu.:0.0000   1st Qu.: 481.7   1st Qu.:21340  \n              Median :0.0000   Median : 823.6   Median :34553  \n              Mean   :0.2944   Mean   : 835.4   Mean   :33517  \n              3rd Qu.:1.0000   3rd Qu.:1166.3   3rd Qu.:43808  \n              Max.   :1.0000   Max.   :2654.3   Max.   :73554  \n```\n\n\n:::\n:::\n\n\n\n\n\n## Treino e Teste\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\nset.seed(2024)\n\ndados_split <- initial_split(credito, strata = inadimplente)\nconj_treino <- training(dados_split)\nconj_teste <- testing(dados_split)\n```\n:::\n\n\n\n\n\n## Treinando\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## 1a tentativa Xgboost\nxgb_spec <- boost_tree(\n  trees = 1000,\n  tree_depth = tune(), min_n = tune(),\n  loss_reduction = tune(),         ## first three: model complexity\n  sample_size = tune(), mtry = tune(),         ## randomness\n  learn_rate = tune()                          ## step size\n) %>%\n  set_engine(\"xgboost\") %>%\n  set_mode(\"classification\")\n\nxgb_spec\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nBoosted Tree Model Specification (classification)\n\nMain Arguments:\n  mtry = tune()\n  trees = 1000\n  min_n = tune()\n  tree_depth = tune()\n  learn_rate = tune()\n  loss_reduction = tune()\n  sample_size = tune()\n\nComputational engine: xgboost \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nxgb_grid <- grid_latin_hypercube(\n  tree_depth(),\n  min_n(),\n  loss_reduction(),\n  sample_size = sample_prop(),\n  finalize(mtry(), conj_treino),\n  learn_rate(),\n  size = 30\n)\n\nxgb_grid\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 30 × 6\n   tree_depth min_n loss_reduction sample_size  mtry learn_rate\n        <int> <int>          <dbl>       <dbl> <int>      <dbl>\n 1          8    32       3.30e+ 0       0.324     2   1.61e- 5\n 2         14    11       1.14e- 7       0.132     1   8.04e-10\n 3         13     5       1.36e- 2       0.924     3   3.83e- 2\n 4          6    37       3.22e- 6       0.666     4   2.75e- 5\n 5          1    11       4.68e- 9       0.189     2   6.81e- 9\n 6         13    17       5.55e- 2       0.563     2   1.11e-10\n 7         15    16       1.74e-10       0.106     1   2.35e- 8\n 8          3     4       3.68e- 3       0.385     3   5.14e- 6\n 9          8    35       4.34e- 5       0.843     3   6.70e- 5\n10         13    20       3.07e- 4       0.969     1   4.09e-10\n# ℹ 20 more rows\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nxgb_wf <- workflow() %>%\n  add_formula(inadimplente ~ .) %>%\n  add_model(xgb_spec)\n\nxgb_wf\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Formula\nModel: boost_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\ninadimplente ~ .\n\n── Model ───────────────────────────────────────────────────────────────────────\nBoosted Tree Model Specification (classification)\n\nMain Arguments:\n  mtry = tune()\n  trees = 1000\n  min_n = tune()\n  tree_depth = tune()\n  learn_rate = tune()\n  loss_reduction = tune()\n  sample_size = tune()\n\nComputational engine: xgboost \n```\n\n\n:::\n:::\n\n\n\n\n\n## \n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2024)\nvb_folds <- vfold_cv(conj_treino, v = 5, strata = inadimplente)\nvb_folds\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#  5-fold cross-validation using stratification \n# A tibble: 5 × 2\n  splits              id   \n  <list>              <chr>\n1 <split [6000/1500]> Fold1\n2 <split [6000/1500]> Fold2\n3 <split [6000/1500]> Fold3\n4 <split [6000/1500]> Fold4\n5 <split [6000/1500]> Fold5\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndoParallel::registerDoParallel()\n\nset.seed(2024)\nxgb_res <- tune_grid(\n  xgb_wf,\n  resamples = vb_folds,\n  grid = xgb_grid,\n  control = control_grid(save_pred = TRUE)\n)\n\nxgb_res\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# Tuning results\n# 5-fold cross-validation using stratification \n# A tibble: 5 × 5\n  splits              id    .metrics           .notes           .predictions\n  <list>              <chr> <list>             <list>           <list>      \n1 <split [6000/1500]> Fold1 <tibble [90 × 10]> <tibble [0 × 3]> <tibble>    \n2 <split [6000/1500]> Fold2 <tibble [90 × 10]> <tibble [0 × 3]> <tibble>    \n3 <split [6000/1500]> Fold3 <tibble [90 × 10]> <tibble [0 × 3]> <tibble>    \n4 <split [6000/1500]> Fold4 <tibble [90 × 10]> <tibble [0 × 3]> <tibble>    \n5 <split [6000/1500]> Fold5 <tibble [90 × 10]> <tibble [0 × 3]> <tibble>    \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nxgb_res %>% collect_metrics()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 90 × 12\n    mtry min_n tree_depth learn_rate loss_reduction sample_size .metric    \n   <int> <int>      <int>      <dbl>          <dbl>       <dbl> <chr>      \n 1     2    32          8   1.61e- 5    3.30              0.324 accuracy   \n 2     2    32          8   1.61e- 5    3.30              0.324 brier_class\n 3     2    32          8   1.61e- 5    3.30              0.324 roc_auc    \n 4     1    11         14   8.04e-10    0.000000114       0.132 accuracy   \n 5     1    11         14   8.04e-10    0.000000114       0.132 brier_class\n 6     1    11         14   8.04e-10    0.000000114       0.132 roc_auc    \n 7     3     5         13   3.83e- 2    0.0136            0.924 accuracy   \n 8     3     5         13   3.83e- 2    0.0136            0.924 brier_class\n 9     3     5         13   3.83e- 2    0.0136            0.924 roc_auc    \n10     4    37          6   2.75e- 5    0.00000322        0.666 accuracy   \n# ℹ 80 more rows\n# ℹ 5 more variables: .estimator <chr>, mean <dbl>, n <int>, std_err <dbl>,\n#   .config <chr>\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nxgb_res %>%\n  collect_metrics() %>%\n  filter(.metric == \"roc_auc\") %>%\n  select(mean, mtry:sample_size) %>%\n  pivot_longer(mtry:sample_size,\n               values_to = \"value\",\n               names_to = \"parameter\"\n  ) %>%\n  ggplot(aes(value, mean, color = parameter)) +\n  geom_point(alpha = 0.8, show.legend = FALSE) +\n  facet_wrap(~parameter, scales = \"free_x\") +\n  labs(x = NULL, y = \"AUC\")\n```\n\n::: {.cell-output-display}\n![](Aula12.3A_files/figure-html/unnamed-chunk-10-1.png){width=90%}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmelhor_auc <- select_best(xgb_res)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_xgb <- finalize_workflow(\n  xgb_wf,\n  melhor_auc\n)\n\nfinal_xgb\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Formula\nModel: boost_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\ninadimplente ~ .\n\n── Model ───────────────────────────────────────────────────────────────────────\nBoosted Tree Model Specification (classification)\n\nMain Arguments:\n  mtry = 3\n  trees = 1000\n  min_n = 24\n  tree_depth = 4\n  learn_rate = 0.0139744688194663\n  loss_reduction = 1.15404948868465\n  sample_size = 0.407858160007745\n\nComputational engine: xgboost \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(vip)\n\nfinal_xgb %>%\n  fit(data = conj_treino) %>%\n  pull_workflow_fit() %>%\n  vip(geom = \"point\")\n```\n\n::: {.cell-output-display}\n![](Aula12.3A_files/figure-html/unnamed-chunk-13-1.png){width=90%}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_res <- last_fit(final_xgb, dados_split)\n\ncollect_metrics(final_res)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 4\n  .metric     .estimator .estimate .config             \n  <chr>       <chr>          <dbl> <chr>               \n1 accuracy    binary        0.966  Preprocessor1_Model1\n2 roc_auc     binary        0.948  Preprocessor1_Model1\n3 brier_class binary        0.0257 Preprocessor1_Model1\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_res %>%\n  collect_predictions() %>%\n  roc_curve(truth=inadimplente, .pred_Sim, event_level = \"second\") %>%\n  autoplot()\n```\n\n::: {.cell-output-display}\n![](Aula12.3A_files/figure-html/unnamed-chunk-15-1.png){width=90%}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmat_conf <- final_res %>%\n  collect_predictions() %>%\n  conf_mat(truth = inadimplente, estimate = .pred_class, event_level = \"second\")\nsummary(mat_conf)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 13 × 3\n   .metric              .estimator .estimate\n   <chr>                <chr>          <dbl>\n 1 accuracy             binary         0.966\n 2 kap                  binary         0.377\n 3 sens                 binary         0.994\n 4 spec                 binary         0.281\n 5 ppv                  binary         0.972\n 6 npv                  binary         0.643\n 7 mcc                  binary         0.411\n 8 j_index              binary         0.275\n 9 bal_accuracy         binary         0.638\n10 detection_prevalence binary         0.983\n11 precision            binary         0.972\n12 recall               binary         0.994\n13 f_meas               binary         0.983\n```\n\n\n:::\n:::\n",
    "supporting": [
      "Aula12.3A_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}