{
  "hash": "253f869d5e75be9609e00bcbb0807d93",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: 'Arvores de Regressão - Random Forest '\nauthor: \"Ricardo Accioly\"\ndate: \"2025-06-02\"\nexecute: \n  echo: true\n  warning: false\n  message: false\n  freeze: auto\nformat:\n html:\n    code-link: true\n    fig-height: 10\n    fig-width: 10\n    fig-align: center\n    fig-dpi: 300\nknitr: \n  opts_chunk: \n    out.width: 90%\n    fig.showtext: true\n    collapese: true\n---\n\n## Bibliotecas\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(MASS)\nlibrary(tidyverse)\nlibrary(ranger) # Random Forest\n```\n:::\n\n\n## Avaliando, selecionando dados\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(\"Boston\")\nnames(Boston)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"crim\"    \"zn\"      \"indus\"   \"chas\"    \"nox\"     \"rm\"      \"age\"    \n [8] \"dis\"     \"rad\"     \"tax\"     \"ptratio\" \"black\"   \"lstat\"   \"medv\"   \n```\n\n\n:::\n\n```{.r .cell-code}\ndados <- Boston \n```\n:::\n\n\n## Treino e Teste com todas as variáveis\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Vamos criar os conjuntos de treino teste e desenvolver a arvore \n## com todas as variáveis.\nlibrary(caret)\nset.seed(21)\nindice <- createDataPartition(dados$medv, times=1, p=0.75, list=FALSE)\nconj_treino <- dados[indice,]\nconj_teste <- dados[-indice,]\nhead(conj_treino)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     crim   zn indus chas   nox    rm  age    dis rad tax ptratio  black lstat\n1 0.00632 18.0  2.31    0 0.538 6.575 65.2 4.0900   1 296    15.3 396.90  4.98\n3 0.02729  0.0  7.07    0 0.469 7.185 61.1 4.9671   2 242    17.8 392.83  4.03\n4 0.03237  0.0  2.18    0 0.458 6.998 45.8 6.0622   3 222    18.7 394.63  2.94\n5 0.06905  0.0  2.18    0 0.458 7.147 54.2 6.0622   3 222    18.7 396.90  5.33\n6 0.02985  0.0  2.18    0 0.458 6.430 58.7 6.0622   3 222    18.7 394.12  5.21\n7 0.08829 12.5  7.87    0 0.524 6.012 66.6 5.5605   5 311    15.2 395.60 12.43\n  medv\n1 24.0\n3 34.7\n4 33.4\n5 36.2\n6 28.7\n7 22.9\n```\n\n\n:::\n\n```{.r .cell-code}\nhead(conj_teste)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      crim   zn indus chas   nox    rm  age    dis rad tax ptratio  black lstat\n2  0.02731  0.0  7.07    0 0.469 6.421 78.9 4.9671   2 242    17.8 396.90  9.14\n10 0.17004 12.5  7.87    0 0.524 6.004 85.9 6.5921   5 311    15.2 386.71 17.10\n12 0.11747 12.5  7.87    0 0.524 6.009 82.9 6.2267   5 311    15.2 396.90 13.27\n16 0.62739  0.0  8.14    0 0.538 5.834 56.5 4.4986   4 307    21.0 395.62  8.47\n19 0.80271  0.0  8.14    0 0.538 5.456 36.6 3.7965   4 307    21.0 288.99 11.69\n23 1.23247  0.0  8.14    0 0.538 6.142 91.7 3.9769   4 307    21.0 396.90 18.72\n   medv\n2  21.6\n10 18.9\n12 18.9\n16 19.9\n19 20.2\n23 15.2\n```\n\n\n:::\n:::\n\n\n## Ajuste do modelo Random Forest com `caret`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nctrl <- trainControl(method = \"cv\", number = 5)\n\nmodel_rf <- train(\n  medv ~ ., data = conj_treino,\n  method = \"ranger\", # Usando o pacote ranger para Random Forest\ntrControl = ctrl,\n  tuneLength = 5,\n  importance = \"permutation\"\n)\n```\n:::\n\n\n## Resultados do modelo\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(model_rf)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRandom Forest \n\n381 samples\n 13 predictor\n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 304, 303, 306, 305, 306 \nResampling results across tuning parameters:\n\n  mtry  splitrule   RMSE      Rsquared   MAE     \n   2    variance    3.738441  0.8478051  2.478749\n   2    extratrees  4.086867  0.8238616  2.699963\n   4    variance    3.407047  0.8674394  2.316360\n   4    extratrees  3.557420  0.8607147  2.350807\n   7    variance    3.426677  0.8607768  2.332572\n   7    extratrees  3.405111  0.8687576  2.282396\n  10    variance    3.490597  0.8541012  2.369265\n  10    extratrees  3.394757  0.8661821  2.263713\n  13    variance    3.537076  0.8489625  2.381604\n  13    extratrees  3.439379  0.8611419  2.305458\n\nTuning parameter 'min.node.size' was held constant at a value of 5\nRMSE was used to select the optimal model using the smallest value.\nThe final values used for the model were mtry = 10, splitrule = extratrees\n and min.node.size = 5.\n```\n\n\n:::\n\n```{.r .cell-code}\nplot(model_rf)\n```\n\n::: {.cell-output-display}\n![](Aula11.2A_files/figure-html/unnamed-chunk-2-1.png){width=90%}\n:::\n:::\n\n\n## Avaliação no conjunto de teste\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred <- predict(model_rf, newdata = conj_teste)\n\n# Métricas de desempenho\npostResample(pred, conj_teste$medv)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     RMSE  Rsquared       MAE \n4.0789597 0.8276481 2.3000856 \n```\n\n\n:::\n:::\n\n\n## Importância das variáveis\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvarimp <- varImp(model_rf)$importance\nvarimp$Variable <- rownames(varimp)\n\n# Gráfico de importância\nvarimp_plot <- ggplot(varimp, aes(x = reorder(Variable, Overall), y = Overall)) +\n  geom_col(fill = \"steelblue\") +\n  coord_flip() +\n  labs(title = \"Importância das Variáveis - Random Forest (ranger)\",\n       x = \"Variável\", y = \"Importância\") +\n  theme_minimal()\n\nvarimp_plot\n```\n\n::: {.cell-output-display}\n![](Aula11.2A_files/figure-html/unnamed-chunk-4-1.png){width=90%}\n:::\n:::\n\n\n## Comparação com outro modelo (Regressão Linear)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_lm <- train(\n  medv ~ ., data = conj_treino,\n  method = \"lm\",\n  trControl = ctrl\n)\n\npred_lm <- predict(model_lm, newdata = conj_teste)\npostResample(pred_lm, conj_teste$medv)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     RMSE  Rsquared       MAE \n5.9426142 0.6315418 3.9610503 \n```\n\n\n:::\n:::\n\n\n## Grafico de comparação\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Gráfico de comparação\ncomparison_plot <- ggplot() +\n  geom_point(aes(x = conj_teste$medv, y = pred), color = \"blue\", alpha = 0.5) +\n  geom_point(aes(x = conj_teste$medv, y = pred_lm), color = \"red\", alpha = 0.5) +\n  labs(title = \"Comparação de Previsões: Random Forest vs Regressão Linear\",\n       x = \"Valores Reais (medv)\", y = \"Previsões\") +\n  theme_minimal()\ncomparison_plot\n```\n\n::: {.cell-output-display}\n![](Aula11.2A_files/figure-html/unnamed-chunk-6-1.png){width=90%}\n:::\n:::\n\n",
    "supporting": [
      "Aula11.2A_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}