{
  "hash": "29f72f18149a7c610c79f0cfbcbd9a16",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: 'Arvores de Regressão - Random Forest '\nauthor: \"Ricardo Accioly\"\ndate: \"2025-06-03\"\nexecute: \n  echo: true\n  warning: false\n  message: false\n  freeze: auto\nformat:\n html:\n    code-link: true\n    fig-height: 10\n    fig-width: 10\n    fig-align: center\n    fig-dpi: 300\nknitr: \n  opts_chunk: \n    out.width: 90%\n    fig.showtext: true\n    collapese: true\n---\n\n## Bibliotecas\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ranger)\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(rsample)\nlibrary(yardstick)\nlibrary(tibble)\nlibrary(ggplot2)\nlibrary(MASS)\n```\n:::\n\n\n## Avaliando, selecionando dados\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(\"Boston\")\nnames(Boston)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"crim\"    \"zn\"      \"indus\"   \"chas\"    \"nox\"     \"rm\"      \"age\"    \n [8] \"dis\"     \"rad\"     \"tax\"     \"ptratio\" \"black\"   \"lstat\"   \"medv\"   \n```\n\n\n:::\n\n```{.r .cell-code}\ndados <- Boston \n```\n:::\n\n\n## Treino e Teste com todas as variáveis\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Vamos criar os conjuntos de treino teste e desenvolver a arvore \n## com todas as variáveis.\nlibrary(caret)\nset.seed(21)\nindice <- createDataPartition(dados$medv, times=1, p=0.75, list=FALSE)\nconj_treino <- dados[indice,]\nconj_teste <- dados[-indice,]\nhead(conj_treino)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     crim   zn indus chas   nox    rm  age    dis rad tax ptratio  black lstat\n1 0.00632 18.0  2.31    0 0.538 6.575 65.2 4.0900   1 296    15.3 396.90  4.98\n3 0.02729  0.0  7.07    0 0.469 7.185 61.1 4.9671   2 242    17.8 392.83  4.03\n4 0.03237  0.0  2.18    0 0.458 6.998 45.8 6.0622   3 222    18.7 394.63  2.94\n5 0.06905  0.0  2.18    0 0.458 7.147 54.2 6.0622   3 222    18.7 396.90  5.33\n6 0.02985  0.0  2.18    0 0.458 6.430 58.7 6.0622   3 222    18.7 394.12  5.21\n7 0.08829 12.5  7.87    0 0.524 6.012 66.6 5.5605   5 311    15.2 395.60 12.43\n  medv\n1 24.0\n3 34.7\n4 33.4\n5 36.2\n6 28.7\n7 22.9\n```\n\n\n:::\n\n```{.r .cell-code}\nhead(conj_teste)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      crim   zn indus chas   nox    rm  age    dis rad tax ptratio  black lstat\n2  0.02731  0.0  7.07    0 0.469 6.421 78.9 4.9671   2 242    17.8 396.90  9.14\n10 0.17004 12.5  7.87    0 0.524 6.004 85.9 6.5921   5 311    15.2 386.71 17.10\n12 0.11747 12.5  7.87    0 0.524 6.009 82.9 6.2267   5 311    15.2 396.90 13.27\n16 0.62739  0.0  8.14    0 0.538 5.834 56.5 4.4986   4 307    21.0 395.62  8.47\n19 0.80271  0.0  8.14    0 0.538 5.456 36.6 3.7965   4 307    21.0 288.99 11.69\n23 1.23247  0.0  8.14    0 0.538 6.142 91.7 3.9769   4 307    21.0 396.90 18.72\n   medv\n2  21.6\n10 18.9\n12 18.9\n16 19.9\n19 20.2\n23 15.2\n```\n\n\n:::\n:::\n\n\n## Criando um grid de parametros\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngrid_params <- expand.grid(\n  mtry = c(2, 4, 6),\n  min.node.size = c(1, 5),\n  splitrule = c(\"variance\", \"extratrees\"),\n  num.trees = c(100)\n)\nnrow(grid_params)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 12\n```\n\n\n:::\n\n```{.r .cell-code}\nhead(grid_params)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  mtry min.node.size splitrule num.trees\n1    2             1  variance       100\n2    4             1  variance       100\n3    6             1  variance       100\n4    2             5  variance       100\n5    4             5  variance       100\n6    6             5  variance       100\n```\n\n\n:::\n:::\n\n\n## Validação cruzada com `rsample`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2025)\nfolds <- vfold_cv(Boston, v = 5)\n```\n:::\n\n\n## Ajuste do modelo Random Forest com cada combinação\n\n\n::: {.cell}\n\n```{.r .cell-code}\navaliacoes <- grid_params %>%\n  mutate(\n    media_rmse = pmap_dbl(list(mtry, min.node.size, splitrule, num.trees), \n      function(mtry, min.node.size, splitrule, num.trees) {\n        rmse_fold <- map_dbl(folds$splits, function(split) {\n          treino <- analysis(split)\n          teste  <- assessment(split)\n          modelo <- ranger(\n            medv ~ ., \n            data = treino,\n            mtry = mtry,\n            min.node.size = min.node.size,\n            splitrule = splitrule,\n            num.trees = num.trees,\n            seed = 123\n          )\n          pred <- predict(modelo, data = teste)$predictions\n          metric <- yardstick::rmse_vec(truth = teste$medv, estimate = pred)\n          return(metric)\n        })\n        mean(rmse_fold)\n      }\n    )\n  )\navaliacoes %>% arrange(media_rmse) %>% head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  mtry min.node.size  splitrule num.trees media_rmse\n1    6             1   variance       100   3.138929\n2    6             5   variance       100   3.180183\n3    4             1   variance       100   3.182037\n4    6             1 extratrees       100   3.223310\n5    4             5   variance       100   3.223342\n6    6             5 extratrees       100   3.292367\n```\n\n\n:::\n:::\n\n\n## Melhor combinação de parâmetros\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmelhor_param <- avaliacoes %>% arrange(media_rmse) %>% slice(1)\nmelhor_param\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  mtry min.node.size splitrule num.trees media_rmse\n1    6             1  variance       100   3.138929\n```\n\n\n:::\n:::\n\n\n## Ajuste do modelo final\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodelo_final <- ranger(\n  medv ~ ., \n  data = conj_treino,\n  mtry = melhor_param$mtry,\n  min.node.size = melhor_param$min.node.size,\n  splitrule = melhor_param$splitrule,\n  num.trees = melhor_param$num.trees,\n  seed = 2025,\n  importance = \"permutation\"\n)\n\nmodelo_final\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRanger result\n\nCall:\n ranger(medv ~ ., data = conj_treino, mtry = melhor_param$mtry,      min.node.size = melhor_param$min.node.size, splitrule = melhor_param$splitrule,      num.trees = melhor_param$num.trees, seed = 2025, importance = \"permutation\") \n\nType:                             Regression \nNumber of trees:                  100 \nSample size:                      381 \nNumber of independent variables:  13 \nMtry:                             6 \nTarget node size:                 1 \nVariable importance mode:         permutation \nSplitrule:                        1 \nOOB prediction error (MSE):       12.75977 \nR squared (OOB):                  0.8445772 \n```\n\n\n:::\n:::\n\n\n## Avaliação no conjunto de teste\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred <- predict(modelo_final, data = conj_teste)$predictions\n\n# Métricas de desempenho\npostResample(pred, conj_teste$medv)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     RMSE  Rsquared       MAE \n4.0052763 0.8341371 2.2677500 \n```\n\n\n:::\n:::\n\n\n## Importância das variáveis\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Converter a importância em tibble ordenada\ndf_importancia <- importance(modelo_final) %>% \n  enframe(name = \"Variável\", value = \"Importância\") %>% \n  arrange(desc(Importância))\n\n# Gráfico de barras da importância das variáveis\nggplot(df_importancia, aes(x = reorder(Variável, Importância), y = Importância)) +\n  geom_col(fill = \"steelblue\") +\n  coord_flip() +\n  labs(title = \"Importância das Variáveis (Permutação)\",\n       x = \"Variável\",\n       y = \"Importância\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](Aula11.2A_files/figure-html/unnamed-chunk-10-1.png){width=90%}\n:::\n:::\n\n\n## Comparação com outro modelo (Regressão Linear)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nctrl <- trainControl(method = \"cv\", number = 5)\n\nmodel_lm <- train(\n  medv ~ ., data = conj_treino,\n  method = \"lm\",\n  trControl = ctrl\n)\n\npred_lm <- predict(model_lm, newdata = conj_teste)\npostResample(pred_lm, conj_teste$medv)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     RMSE  Rsquared       MAE \n5.9426142 0.6315418 3.9610503 \n```\n\n\n:::\n:::\n\n\n## Grafico de comparação\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Gráfico de comparação\ngraf_comparacao <- ggplot() +\n  geom_point(aes(x = conj_teste$medv, y = pred), color = \"blue\", alpha = 0.5) +\n  geom_point(aes(x = conj_teste$medv, y = pred_lm), color = \"red\", alpha = 0.5) +\n  labs(title = \"Comparação de Previsões: Random Forest vs Regressão Linear\",\n       x = \"Valores Reais (medv)\", y = \"Previsões\") +\n  theme_minimal()\ngraf_comparacao\n```\n\n::: {.cell-output-display}\n![](Aula11.2A_files/figure-html/unnamed-chunk-12-1.png){width=90%}\n:::\n:::\n\n\n## Analisando com o LIME\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Treinamento com ranger precisa ser refeito em formato compatível com lime\nlibrary(lime)\n\n# Treinando modelo com train() + ranger para compatibilidade com LIME\nset.seed(2025)\ntune_grid <- data.frame(\n  mtry = melhor_param$mtry,\n  splitrule = as.character(melhor_param$splitrule),\n  min.node.size = melhor_param$min.node.size\n)\nmodelo_caret <- train(\n  medv ~ ., \n  data = conj_treino,\n  method = \"ranger\",\n  trControl = trainControl(method = \"none\"),\n  tuneGrid = tune_grid\n)\n\n# Preparar explicador LIME\nexplainer <- lime(conj_treino, modelo_caret, bin_continuous = FALSE)\n\n# Aplicar LIME a 3 observações novas\nexplicacoes <- explain(\n  conj_teste[1:3, ],\n  explainer = explainer,\n  n_features = 5,\n  n_labels = 1\n)\n\n# Dados Analisados\nconj_teste[1:3, c(\"medv\", \"rm\", \"ptratio\", \"nox\", \"dis\", \"lstat\", \"tax\")]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   medv    rm ptratio   nox    dis lstat tax\n2  21.6 6.421    17.8 0.469 4.9671  9.14 242\n10 18.9 6.004    15.2 0.524 6.5921 17.10 311\n12 18.9 6.009    15.2 0.524 6.2267 13.27 311\n```\n\n\n:::\n\n```{.r .cell-code}\n# Visualizar explicações\nplot_features(explicacoes)\n```\n\n::: {.cell-output-display}\n![](Aula11.2A_files/figure-html/unnamed-chunk-13-1.png){width=90%}\n:::\n:::\n\n",
    "supporting": [
      "Aula11.2A_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}