{
  "hash": "803f15936201b6ef9e00d34818a39e57",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: 'Arvores de Regressão - Random Forest '\nauthor: \"Ricardo Accioly\"\ndate: \"2025-11-11\"\nexecute: \n  echo: true\n  warning: false\n  message: false\n  freeze: auto\nformat:\n html:\n    code-link: true\n    fig-height: 10\n    fig-width: 10\n    fig-align: center\n    fig-dpi: 300\nknitr: \n  opts_chunk: \n    out.width: 90%\n    fig.showtext: true\n    collapese: true\n---\n\n## Bibliotecas\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ranger)\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(rsample)\nlibrary(yardstick)\nlibrary(tibble)\nlibrary(ggplot2)\nlibrary(MASS)\n```\n:::\n\n\n## Avaliando, selecionando dados\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(\"Boston\")\nnames(Boston)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"crim\"    \"zn\"      \"indus\"   \"chas\"    \"nox\"     \"rm\"      \"age\"    \n [8] \"dis\"     \"rad\"     \"tax\"     \"ptratio\" \"black\"   \"lstat\"   \"medv\"   \n```\n\n\n:::\n\n```{.r .cell-code}\ndados <- Boston \n```\n:::\n\n\n## Treino e Teste com todas as variáveis\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Vamos criar os conjuntos de treino teste e desenvolver a arvore \n## com todas as variáveis.\nlibrary(caret)\nset.seed(2025)\nindice <- createDataPartition(dados$medv, times=1, p=0.75, list=FALSE)\nconj_treino <- dados[indice,]\nconj_teste <- dados[-indice,]\nhead(conj_treino)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      crim   zn indus chas   nox    rm  age    dis rad tax ptratio  black lstat\n2  0.02731  0.0  7.07    0 0.469 6.421 78.9 4.9671   2 242    17.8 396.90  9.14\n6  0.02985  0.0  2.18    0 0.458 6.430 58.7 6.0622   3 222    18.7 394.12  5.21\n7  0.08829 12.5  7.87    0 0.524 6.012 66.6 5.5605   5 311    15.2 395.60 12.43\n8  0.14455 12.5  7.87    0 0.524 6.172 96.1 5.9505   5 311    15.2 396.90 19.15\n10 0.17004 12.5  7.87    0 0.524 6.004 85.9 6.5921   5 311    15.2 386.71 17.10\n11 0.22489 12.5  7.87    0 0.524 6.377 94.3 6.3467   5 311    15.2 392.52 20.45\n   medv\n2  21.6\n6  28.7\n7  22.9\n8  27.1\n10 18.9\n11 15.0\n```\n\n\n:::\n\n```{.r .cell-code}\nhead(conj_teste)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      crim   zn indus chas   nox    rm   age    dis rad tax ptratio  black\n1  0.00632 18.0  2.31    0 0.538 6.575  65.2 4.0900   1 296    15.3 396.90\n3  0.02729  0.0  7.07    0 0.469 7.185  61.1 4.9671   2 242    17.8 392.83\n4  0.03237  0.0  2.18    0 0.458 6.998  45.8 6.0622   3 222    18.7 394.63\n5  0.06905  0.0  2.18    0 0.458 7.147  54.2 6.0622   3 222    18.7 396.90\n9  0.21124 12.5  7.87    0 0.524 5.631 100.0 6.0821   5 311    15.2 386.63\n19 0.80271  0.0  8.14    0 0.538 5.456  36.6 3.7965   4 307    21.0 288.99\n   lstat medv\n1   4.98 24.0\n3   4.03 34.7\n4   2.94 33.4\n5   5.33 36.2\n9  29.93 16.5\n19 11.69 20.2\n```\n\n\n:::\n:::\n\n\n## Criando um grid de parametros\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngrid_params <- expand.grid(\n  mtry = c(2, 4, 6),\n  min.node.size = c(1, 5),\n  splitrule = c(\"variance\", \"extratrees\"),\n  num.trees = c(100)\n)\nnrow(grid_params)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 12\n```\n\n\n:::\n\n```{.r .cell-code}\nhead(grid_params)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  mtry min.node.size splitrule num.trees\n1    2             1  variance       100\n2    4             1  variance       100\n3    6             1  variance       100\n4    2             5  variance       100\n5    4             5  variance       100\n6    6             5  variance       100\n```\n\n\n:::\n:::\n\n\n## Validação cruzada com `rsample`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2025)\nfolds <- vfold_cv(Boston, v = 5)\n```\n:::\n\n\n## Ajuste do modelo Random Forest com cada combinação\n\n\n::: {.cell}\n\n```{.r .cell-code}\navaliacoes <- grid_params %>%\n  mutate(\n    media_rmse = pmap_dbl(list(mtry, min.node.size, splitrule, num.trees), \n      function(mtry, min.node.size, splitrule, num.trees) {\n        rmse_fold <- map_dbl(folds$splits, function(split) {\n          treino <- analysis(split)\n          teste  <- assessment(split)\n          modelo <- ranger(\n            medv ~ ., \n            data = treino,\n            mtry = mtry,\n            min.node.size = min.node.size,\n            splitrule = splitrule,\n            num.trees = num.trees,\n            seed = 2025\n          )\n          pred <- predict(modelo, data = teste)$predictions\n          metric <- yardstick::rmse_vec(truth = teste$medv, estimate = pred)\n          return(metric)\n        })\n        mean(rmse_fold)\n      }\n    )\n  )\navaliacoes %>% arrange(media_rmse) %>% head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  mtry min.node.size  splitrule num.trees media_rmse\n1    6             1   variance       100   3.140727\n2    6             5   variance       100   3.174609\n3    4             1   variance       100   3.177201\n4    6             1 extratrees       100   3.192652\n5    4             5   variance       100   3.235676\n6    4             1 extratrees       100   3.249829\n```\n\n\n:::\n:::\n\n\n## Melhor combinação de parâmetros\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmelhor_param <- avaliacoes %>% arrange(media_rmse) %>% slice(1)\nmelhor_param\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  mtry min.node.size splitrule num.trees media_rmse\n1    6             1  variance       100   3.140727\n```\n\n\n:::\n:::\n\n\n## Ajuste do modelo final\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodelo_final <- ranger(\n  medv ~ ., \n  data = conj_treino,\n  mtry = melhor_param$mtry,\n  min.node.size = melhor_param$min.node.size,\n  splitrule = melhor_param$splitrule,\n  num.trees = melhor_param$num.trees,\n  seed = 2025,\n  importance = \"permutation\"\n)\n\nmodelo_final\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRanger result\n\nCall:\n ranger(medv ~ ., data = conj_treino, mtry = melhor_param$mtry,      min.node.size = melhor_param$min.node.size, splitrule = melhor_param$splitrule,      num.trees = melhor_param$num.trees, seed = 2025, importance = \"permutation\") \n\nType:                             Regression \nNumber of trees:                  100 \nSample size:                      381 \nNumber of independent variables:  13 \nMtry:                             6 \nTarget node size:                 1 \nVariable importance mode:         permutation \nSplitrule:                        1 \nOOB prediction error (MSE):       10.99587 \nR squared (OOB):                  0.8584414 \n```\n\n\n:::\n:::\n\n\n## Avaliação no conjunto de teste\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred <- predict(modelo_final, data = conj_teste)$predictions\n\n# Métricas de desempenho\npostResample(pred, conj_teste$medv)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     RMSE  Rsquared       MAE \n3.2494760 0.9077779 2.3676144 \n```\n\n\n:::\n:::\n\n\n## Importância das variáveis\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Converter a importância em tibble ordenada\ndf_importancia <- importance(modelo_final) %>% \n  enframe(name = \"Variável\", value = \"Importância\") %>% \n  arrange(desc(Importância))\n\n# Gráfico de barras da importância das variáveis\nggplot(df_importancia, aes(x = reorder(Variável, Importância), y = Importância)) +\n  geom_col(fill = \"steelblue\") +\n  coord_flip() +\n  labs(title = \"Importância das Variáveis (Permutação)\",\n       x = \"Variável\",\n       y = \"Importância\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](Aula11.2A_files/figure-html/unnamed-chunk-10-1.png){width=90%}\n:::\n:::\n\n\n## Comparação com outro modelo (Regressão Linear)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nctrl <- trainControl(method = \"cv\", number = 5)\n\nmodel_lm <- train(\n  medv ~ ., data = conj_treino,\n  method = \"lm\",\n  trControl = ctrl\n)\n\npred_lm <- predict(model_lm, newdata = conj_teste)\npostResample(pred_lm, conj_teste$medv)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     RMSE  Rsquared       MAE \n5.5535662 0.7120235 3.8617314 \n```\n\n\n:::\n:::\n\n\n## Grafico de comparação\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Gráfico de comparação\nggplot() +\n  geom_point(aes(x = conj_teste$medv, y = pred), color = \"blue\", alpha = 0.5) +\n  geom_point(aes(x = conj_teste$medv, y = pred_lm), color = \"red\", alpha = 0.5) +\n  labs(title = \"Comparação de Previsões: Random Forest (azul) vs Regressão Linear (vermelho)\", x = \"Valores Reais (medv)\", y = \"Previsões\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](Aula11.2A_files/figure-html/unnamed-chunk-12-1.png){width=90%}\n:::\n:::\n\n\n## Analisando com o LIME\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Treinamento com ranger precisa ser refeito em formato compatível com lime\nlibrary(lime)\n\n# Treinando modelo com train() + ranger para compatibilidade com LIME\nset.seed(2025)\ntune_grid <- data.frame(\n  mtry = melhor_param$mtry,\n  splitrule = as.character(melhor_param$splitrule),\n  min.node.size = melhor_param$min.node.size\n)\nmodelo_caret <- train(\n  medv ~ ., \n  data = conj_treino,\n  method = \"ranger\",\n  trControl = trainControl(method = \"none\"),\n  tuneGrid = tune_grid\n)\n\n# Preparar explicador LIME\nexplainer <- lime(conj_treino, modelo_caret, bin_continuous = FALSE)\n\n# Aplicar LIME a 3 observações novas\nexplicacoes <- explain(\n  conj_teste[1:3, ],\n  explainer = explainer,\n  n_features = 5,\n  n_labels = 1\n)\n\n# Dados Analisados\nconj_teste[1:3, c(\"medv\", \"rm\", \"ptratio\", \"nox\", \"dis\", \"lstat\", \"tax\")]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  medv    rm ptratio   nox    dis lstat tax\n1 24.0 6.575    15.3 0.538 4.0900  4.98 296\n3 34.7 7.185    17.8 0.469 4.9671  4.03 242\n4 33.4 6.998    18.7 0.458 6.0622  2.94 222\n```\n\n\n:::\n\n```{.r .cell-code}\n# Visualizar explicações\nplot_features(explicacoes)\n```\n\n::: {.cell-output-display}\n![](Aula11.2A_files/figure-html/unnamed-chunk-13-1.png){width=90%}\n:::\n:::\n\n",
    "supporting": [
      "Aula11.2A_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}