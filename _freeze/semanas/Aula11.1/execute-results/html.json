{
  "hash": "cced9b1fc88629d29e1a51d089e7cc6e",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Arvores de Regressão\"\nauthor: \"Ricardo Accioly\"\ndate: \"2025-11-11\"\nexecute: \n  echo: true\n  warning: false\n  message: false\n  freeze: auto\nformat:\n html:\n    code-link: true\n    fig-height: 10\n    fig-width: 10\n    fig-align: center\n    fig-dpi: 300\nknitr: \n  opts_chunk: \n    out.width: 90%\n    comment: \"#>\"\n---\n\n## Bibliotecas\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(MASS)\nlibrary(tidyverse)\nlibrary(rpart)\nlibrary(rpart.plot)\nlibrary(psych)\nlibrary(caret)\n```\n:::\n\n\n## Carregando os dados\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(\"Boston\")\nnames(Boston)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>  [1] \"crim\"    \"zn\"      \"indus\"   \"chas\"    \"nox\"     \"rm\"      \"age\"    \n#>  [8] \"dis\"     \"rad\"     \"tax\"     \"ptratio\" \"black\"   \"lstat\"   \"medv\"\n```\n\n\n:::\n\n```{.r .cell-code}\ndescribe(Boston)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>         vars   n   mean     sd median trimmed    mad    min    max  range  skew\n#> crim       1 506   3.61   8.60   0.26    1.68   0.33   0.01  88.98  88.97  5.19\n#> zn         2 506  11.36  23.32   0.00    5.08   0.00   0.00 100.00 100.00  2.21\n#> indus      3 506  11.14   6.86   9.69   10.93   9.37   0.46  27.74  27.28  0.29\n#> chas       4 506   0.07   0.25   0.00    0.00   0.00   0.00   1.00   1.00  3.39\n#> nox        5 506   0.55   0.12   0.54    0.55   0.13   0.38   0.87   0.49  0.72\n#> rm         6 506   6.28   0.70   6.21    6.25   0.51   3.56   8.78   5.22  0.40\n#> age        7 506  68.57  28.15  77.50   71.20  28.98   2.90 100.00  97.10 -0.60\n#> dis        8 506   3.80   2.11   3.21    3.54   1.91   1.13  12.13  11.00  1.01\n#> rad        9 506   9.55   8.71   5.00    8.73   2.97   1.00  24.00  23.00  1.00\n#> tax       10 506 408.24 168.54 330.00  400.04 108.23 187.00 711.00 524.00  0.67\n#> ptratio   11 506  18.46   2.16  19.05   18.66   1.70  12.60  22.00   9.40 -0.80\n#> black     12 506 356.67  91.29 391.44  383.17   8.09   0.32 396.90 396.58 -2.87\n#> lstat     13 506  12.65   7.14  11.36   11.90   7.11   1.73  37.97  36.24  0.90\n#> medv      14 506  22.53   9.20  21.20   21.56   5.93   5.00  50.00  45.00  1.10\n#>         kurtosis   se\n#> crim       36.60 0.38\n#> zn          3.95 1.04\n#> indus      -1.24 0.30\n#> chas        9.48 0.01\n#> nox        -0.09 0.01\n#> rm          1.84 0.03\n#> age        -0.98 1.25\n#> dis         0.46 0.09\n#> rad        -0.88 0.39\n#> tax        -1.15 7.49\n#> ptratio    -0.30 0.10\n#> black       7.10 4.06\n#> lstat       0.46 0.32\n#> medv        1.45 0.41\n```\n\n\n:::\n:::\n\n\n## Conhecendo as variáveis\n\n-   crim: taxa de criminalidade per capita por cidade.\n\n-   zn: proporção de terrenos residenciais zoneados para lotes acima de 25.000 pés quadrados.\n\n-   indus: proporção de acres de negócios não varejistas por cidade.\n\n-   chas: variável fictícia do rio Charles (= 1 se o trato limita o rio; 0 caso contrário). nox concentração de óxidos de nitrogênio (partes por 10 milhões).\n\n-   rm: número médio de cômodos por habitação.\n\n-   age: proporção de unidades ocupadas pelo proprietário construídas antes de 1940.\n\n-   dis: média ponderada das distâncias até cinco centros de emprego de Boston.\n\n-   rad: índice de acessibilidade a rodovias radiais. tax taxa de imposto sobre a propriedade do valor total por \\$ 10.000.\n\n-   ptratio: proporção aluno-professor por cidade.\n\n-   black: 1000(Bk−0,63)21000(Bk−0,63)2 onde BkBk é a proporção de negros por cidade.\n\n-   lstat: status inferior da população (por cento).\n\n-   medv: valor médio das casas ocupadas pelo proprietário em \\$ 1000s.\n\n## Selecionando os dados\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndados <- Boston \nextrato <- dados %>% select(medv, nox, rm)  \nsummary(extrato)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>       medv            nox               rm       \n#>  Min.   : 5.00   Min.   :0.3850   Min.   :3.561  \n#>  1st Qu.:17.02   1st Qu.:0.4490   1st Qu.:5.886  \n#>  Median :21.20   Median :0.5380   Median :6.208  \n#>  Mean   :22.53   Mean   :0.5547   Mean   :6.285  \n#>  3rd Qu.:25.00   3rd Qu.:0.6240   3rd Qu.:6.623  \n#>  Max.   :50.00   Max.   :0.8710   Max.   :8.780\n```\n\n\n:::\n\n```{.r .cell-code}\nboxplot(extrato$medv)\n```\n\n::: {.cell-output-display}\n![](Aula11.1_files/figure-html/unnamed-chunk-1-1.png){width=90%}\n:::\n:::\n\n\n## Visualizando os dados\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Distribuição de dados na maior parte simétrica com valores na cauda direta \n## maior do que o esperado para uam distribuição simétrica\nggplot(extrato, aes(x=medv)) +\n  geom_histogram(bins = round(1+3.322*log10(nrow(extrato)),0))\n```\n\n::: {.cell-output-display}\n![](Aula11.1_files/figure-html/ggplot-1.png){width=90%}\n:::\n\n```{.r .cell-code}\n## Grafico de dispersão nox vs rm \nggplot(extrato, aes(x=rm, y=nox)) + \n  geom_point()\n```\n\n::: {.cell-output-display}\n![](Aula11.1_files/figure-html/ggplot-2.png){width=90%}\n:::\n:::\n\n\n## Arvore de Regressão\n\nNa biblioteca rpart as arvores de regressão são obtidas usando o método anova. Existem alguns controles que podem ser feitos nos parametros da arvore.\n\nNeste exemplo só definimos o menor conjunto de dados numa partição (minsplit) e o parametro de complexidade cp. Qualquer partição/divisão que não melhore o ajuste por um fator de cp não é tentada. Por exemplo, com a partição pela anova, isso significa que o R-quadrado geral deve aumentar pelo valor de cp a cada etapa. O principal papel deste parâmetro é economizar tempo de computação podando divisões que obviamente não valem a pena. Essencialmente, o usuário informa ao programa que qualquer divisão que não melhore o ajuste pelo cp, provavelmente será podada por validação cruzada, e que, portanto, não é necessário persegui-lo.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n##Usando rpart para desenvolver a arvore  \narvreg <- rpart(medv ~ ., \n                data=extrato,\n                method=\"anova\", #para arvore de regressão\n                control=rpart.control(minsplit=30,cp=0.06))\narvreg\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> n= 506 \n#> \n#> node), split, n, deviance, yval\n#>       * denotes terminal node\n#> \n#> 1) root 506 42716.300 22.53281  \n#>   2) rm< 6.941 430 17317.320 19.93372  \n#>     4) nox>=0.6695 97  2214.391 13.73918 *\n#>     5) nox< 0.6695 333 10296.590 21.73814 *\n#>   3) rm>=6.941 76  6059.419 37.23816  \n#>     6) rm< 7.437 46  1899.612 32.11304 *\n#>     7) rm>=7.437 30  1098.850 45.09667 *\n```\n\n\n:::\n\n```{.r .cell-code}\nrpart.plot(arvreg)\n```\n\n::: {.cell-output-display}\n![](Aula11.1_files/figure-html/arvore-1.png){width=90%}\n:::\n:::\n\n\n## Segmentos\n\nA partir da árvore obtida no item anterior podemos fazer uma representação gráfica das partições obtidas.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(extrato, aes(x=rm, y=nox)) + \n  geom_point() +\n  geom_segment(aes(x = 0, y = 0.6695, xend = 6.941, yend = 0.6695), \n               linetype=\"dashed\", color=\"red\", size=1) +\n  geom_vline(xintercept = 6.941, linetype=\"dashed\", color=\"red\", size=1) +\n  geom_vline(xintercept = 7.437, linetype=\"dashed\", color=\"red\", size=1) \n```\n\n::: {.cell-output-display}\n![](Aula11.1_files/figure-html/segmentos-1.png){width=90%}\n:::\n\n```{.r .cell-code}\n  # scale_y_continuous(limits = c(0.3, 1)) +\n```\n:::\n\n\n## Treino e Teste com todas as variáveis\n\nAgora vamos trabalhar com o conjunto completo criando um conjunto de treino e teste.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Vamos criar os conjuntos de treino teste e desenvolver a arvore \n## com todas as variáveis.\nset.seed(2025)\nindice <- createDataPartition(dados$medv, times=1, p=0.75, list=FALSE)\nconj_treino <- dados[indice,]\nconj_teste <- dados[-indice,]\nhead(conj_treino)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>       crim   zn indus chas   nox    rm  age    dis rad tax ptratio  black lstat\n#> 2  0.02731  0.0  7.07    0 0.469 6.421 78.9 4.9671   2 242    17.8 396.90  9.14\n#> 6  0.02985  0.0  2.18    0 0.458 6.430 58.7 6.0622   3 222    18.7 394.12  5.21\n#> 7  0.08829 12.5  7.87    0 0.524 6.012 66.6 5.5605   5 311    15.2 395.60 12.43\n#> 8  0.14455 12.5  7.87    0 0.524 6.172 96.1 5.9505   5 311    15.2 396.90 19.15\n#> 10 0.17004 12.5  7.87    0 0.524 6.004 85.9 6.5921   5 311    15.2 386.71 17.10\n#> 11 0.22489 12.5  7.87    0 0.524 6.377 94.3 6.3467   5 311    15.2 392.52 20.45\n#>    medv\n#> 2  21.6\n#> 6  28.7\n#> 7  22.9\n#> 8  27.1\n#> 10 18.9\n#> 11 15.0\n```\n\n\n:::\n\n```{.r .cell-code}\nhead(conj_teste)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>       crim   zn indus chas   nox    rm   age    dis rad tax ptratio  black\n#> 1  0.00632 18.0  2.31    0 0.538 6.575  65.2 4.0900   1 296    15.3 396.90\n#> 3  0.02729  0.0  7.07    0 0.469 7.185  61.1 4.9671   2 242    17.8 392.83\n#> 4  0.03237  0.0  2.18    0 0.458 6.998  45.8 6.0622   3 222    18.7 394.63\n#> 5  0.06905  0.0  2.18    0 0.458 7.147  54.2 6.0622   3 222    18.7 396.90\n#> 9  0.21124 12.5  7.87    0 0.524 5.631 100.0 6.0821   5 311    15.2 386.63\n#> 19 0.80271  0.0  8.14    0 0.538 5.456  36.6 3.7965   4 307    21.0 288.99\n#>    lstat medv\n#> 1   4.98 24.0\n#> 3   4.03 34.7\n#> 4   2.94 33.4\n#> 5   5.33 36.2\n#> 9  29.93 16.5\n#> 19 11.69 20.2\n```\n\n\n:::\n:::\n\n\n## Arvore de Regressão com caret\n\nAqui vamos usar a biblioteca caret que tem umas facilidades para otimização do cp e apresentação dos resultados\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2025)\n## Otimizamos o valor de cp usando um 10-fold cv\n# O parametro tuneLength diz para o algoritmo escolher diferentes valores para cp\n# O parametro tuneGrid permite decidir que valores cp deve assumir enquanto que o\n# tuneLength somente limita o número default de parametros que se usa.\narvreg2 <- train(medv ~ . , data = conj_treino, method = \"rpart\",\n                 trControl = trainControl(\"cv\", number = 10),\n                 tuneGrid = data.frame(cp = seq(0.01,0.10, length.out=100)) \n                 )\n# Mostra a acurácia vs cp (parametro de complexidade)\nplot(arvreg2)\n```\n\n::: {.cell-output-display}\n![](Aula11.1_files/figure-html/arvore3-1.png){width=90%}\n:::\n\n```{.r .cell-code}\n## Indica o melhor valor de cp\narvreg2$bestTune\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>           cp\n#> 2 0.01090909\n```\n\n\n:::\n:::\n\n\n## Desenhando a Árvore\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Apresenta o modelo final de arvore ajustado\n## usando o rpart.plot\nrpart.plot(arvreg2$finalModel)\n```\n\n::: {.cell-output-display}\n![](Aula11.1_files/figure-html/arvore4-1.png){width=90%}\n:::\n:::\n\n\n## Gráfico de importancia das variáveis\n\nA importancia das variáveis é calculada com base nos resultados das melhores partições\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Gráfico de Importância de variável\nvar_imp <- varImp(arvreg2) \nvar_imp$importance %>%\n  rownames_to_column(var = \"Variável\") %>%\n  ggplot(aes(x = reorder(Variável, Overall), y = Overall)) +\n  geom_col(fill = \"steelblue\") +\n  coord_flip() +\n  labs(title = \"Importância das Variáveis na Árvore de Regressão\",\n       x = \"Variável\",\n       y = \"Importância (Overall)\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](Aula11.1_files/figure-html/unnamed-chunk-2-1.png){width=90%}\n:::\n:::\n\n\n## Previsões\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Regras de Decisão\narvreg2$finalModel\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> n= 381 \n#> \n#> node), split, n, deviance, yval\n#>       * denotes terminal node\n#> \n#>  1) root 381 29517.3100 22.24934  \n#>    2) rm< 7.0105 337 14500.6500 20.17745  \n#>      4) lstat>=14.4 133  2641.8500 14.88271  \n#>        8) nox>=0.607 84  1154.7830 12.88571  \n#>         16) lstat>=19.645 45   393.3764 10.63111 *\n#>         17) lstat< 19.645 39   268.7236 15.48718 *\n#>        9) nox< 0.607 49   577.8082 18.30612 *\n#>      5) lstat< 14.4 204  5699.3640 23.62941  \n#>       10) rm< 6.543 156  3036.0710 22.16090  \n#>         20) dis>=1.68515 149  1307.6110 21.81141 *\n#>         21) dis< 1.68515 7  1322.8800 29.60000 *\n#>       11) rm>=6.543 48  1233.5100 28.40208  \n#>         22) lstat>=5.195 29   213.9524 26.04828 *\n#>         23) lstat< 5.195 19   613.6495 31.99474 *\n#>    3) rm>=7.0105 44  2489.9250 38.11818  \n#>      6) rm< 7.435 26   382.4815 32.73846 *\n#>      7) rm>=7.435 18   268.0578 45.88889 *\n```\n\n\n:::\n\n```{.r .cell-code}\n# Fazendo Previsões\nprevisao2 <- arvreg2 %>% predict(conj_teste)\nhead(previsao2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>        1        3        4        5        9       19 \n#> 31.99474 32.73846 31.99474 32.73846 18.30612 21.81141\n```\n\n\n:::\n\n```{.r .cell-code}\n# Calcula os erros de previsão\nRMSE(previsao2, conj_teste$medv)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] 4.882214\n```\n\n\n:::\n:::\n\n\n## Vamos comparar com Regressão Multipla\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(leaps)\n## Cria uma função de predição para o leaps\npredict.regsubsets <- function(object,newdata,id,...){\n  form <- as.formula(object$call[[2]])\n  mat <- model.matrix(form,newdata)\n  coefi <- coef(object,id=id)\n  mat[,names(coefi)]%*%coefi\n}\nset.seed(2025)\nenvelopes <- sample(rep(1:5,length=nrow(conj_treino)))\ntable(envelopes)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> envelopes\n#>  1  2  3  4  5 \n#> 77 76 76 76 76\n```\n\n\n:::\n\n```{.r .cell-code}\nerro_cv <- matrix(NA,5,13)\nfor(k in 1:5){\n  melh_ajus <- regsubsets(medv ~ ., data=conj_treino[envelopes!=k,], \n                          nvmax=13,method=\"forward\")\n  for(i in 1:13){\n    prev <- predict(melh_ajus, conj_treino[envelopes==k,],id=i)\n    erro_cv[k,i] <- mean( (conj_treino$medv[envelopes==k]-prev)^2)\n  }\n}\nrmse_cv <- sqrt(apply(erro_cv,2,mean))  # Erro medio quadratico de cada modelo\nplot(rmse_cv,pch=19,type=\"b\")          \n```\n\n::: {.cell-output-display}\n![](Aula11.1_files/figure-html/kfold-1.png){width=90%}\n:::\n:::\n\n\n## Obtem a fórmula do modelo\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncoef(melh_ajus, 11)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>   (Intercept)          crim            zn          chas           nox \n#>  34.693940504  -0.124555425   0.050468728   4.160779593 -21.520063917 \n#>            rm           dis           rad           tax       ptratio \n#>   4.179728433  -1.594068072   0.357136658  -0.013259547  -0.837573459 \n#>         black         lstat \n#>   0.008077009  -0.500287649\n```\n\n\n:::\n:::\n\n\n## Teste com o conjunto de teste\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprevisao3 <- predict(melh_ajus, conj_teste, 11) \nRMSE(previsao3, conj_teste$medv)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] 5.519325\n```\n\n\n:::\n:::\n\n\n## E se usarmos outra semente?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Vamos criar os conjuntos de treino teste e desenvolver a arvore \n## com todas as variáveis.\nset.seed(23)\nindice <- createDataPartition(dados$medv, times=1, p=0.75, list=FALSE)\nconj_treino <- dados[indice,]\n\narvreg1s <- rpart(medv ~ ., \n                data=conj_treino,\n                method=\"anova\", #para arvore de regressão\n                control=rpart.control(minsplit=30,cp=0.01))\narvreg1s\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> n= 381 \n#> \n#> node), split, n, deviance, yval\n#>       * denotes terminal node\n#> \n#>  1) root 381 31257.8400 22.41496  \n#>    2) rm< 6.92 321 11786.7500 19.74361  \n#>      4) lstat>=14.4 133  2558.5280 14.88722  \n#>        8) crim>=6.99237 59   866.3051 11.96949 *\n#>        9) crim< 6.99237 74   789.4865 17.21351 *\n#>      5) lstat< 14.4 188  3872.3890 23.17926  \n#>       10) lstat>=5.51 165  2166.8920 22.28970  \n#>         20) lstat>=9.675 86   527.0414 20.76977 *\n#>         21) lstat< 9.675 79  1224.8950 23.94430 *\n#>       11) lstat< 5.51 23   638.2548 29.56087 *\n#>    3) rm>=6.92 60  4925.2370 36.70667  \n#>      6) rm< 7.437 39  1510.0630 31.43077  \n#>       12) ptratio>=18.55 10   750.6440 25.84000 *\n#>       13) ptratio< 18.55 29   339.0703 33.35862 *\n#>      7) rm>=7.437 21   313.5495 46.50476 *\n```\n\n\n:::\n\n```{.r .cell-code}\n## Colocar as duas arvores lado a lado\npar(mfrow=c(1,2))\nrpart.plot(arvreg1s)\nrpart.plot(arvreg2$finalModel)\n```\n\n::: {.cell-output-display}\n![](Aula11.1_files/figure-html/unnamed-chunk-3-1.png){width=90%}\n:::\n:::\n\n\nEsta é a principal fragilidade da árvore (única), qualquer mudança na amostra pode trazer uma configuração diferente. \\## Fim\n",
    "supporting": [
      "Aula11.1_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}